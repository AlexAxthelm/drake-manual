# General best practices for drake projects {#practices}

```{r bestpracticesstart, echo = F}
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(magrittr)))
suppressMessages(suppressWarnings(library(curl)))
suppressMessages(suppressWarnings(library(httr)))
suppressMessages(suppressWarnings(library(R.utils)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
pkgconfig::set_config("drake::strings_in_dots" = "literals")
tmp <- file.create("data.csv")
```

This chapter describes general best practices for creating, configuring, and running `drake` projects. It answers frequently asked questions and clears up common misconceptions, and it will continuously develop in response to community feedback.

## How to organize your files

### Examples

For examples of how to structure your code files, see the beginner oriented example projects:

- [mtcars](https://github.com/ropensci/drake/tree/master/inst/examples/mtcars)
- [gsp](https://github.com/ropensci/drake/tree/master/inst/examples/gsp)
- [packages](https://github.com/ropensci/drake/tree/master/inst/examples/packages)

Write the code directly with the `drake_example()` function.

```{r exampledrakewritingbestpractices, eval = FALSE}
drake_example("mtcars")
drake_example("gsp")
drake_example("packages")
``` 

In practice, you do not need to organize your files the way the examples do, but it does happen to be a reasonable way of doing things.

### Where do you put your code?

It is best to write your code as a bunch of functions. You can save those functions in R scripts and then `source()` them before doing anything else.

```{r sourcefunctions, eval = FALSE}
## Load functions get_data(), analyze_data, and summarize_results()
source("my_functions.R")
```

```{r sourcefunctionsbk, echo = FALSE}
get_data <- analyze_data <- summarize_results <- function(){}
```

Then, set up your workflow plan data frame.

```{r storecode1}
good_plan <- drake_plan(
  my_data = get_data(file_in("data.csv")), # External files need to be in commands explicitly. # nolint
  my_analysis = analyze_data(my_data),
  my_summaries = summarize_results(my_data, my_analysis)
)

good_plan
```

`Drake` knows that `my_analysis` depends on `my_data` because `my_data` is an argument to `analyze_data()`, which is part of the command for `my_analysis`.

```{r visgood}
config <- drake_config(good_plan)
vis_drake_graph(config)
```

Now, you can call `make()` to build the targets.

```{r makestorecode, eval = FALSE}
make(good_plan)
```

If your commands are really long, just put them in larger functions. `Drake` analyzes imported functions for non-file dependencies.

### Remember: your commands are code chunks, not R scripts

Some people are accustomed to dividing their work into R scripts and then calling `source()` to run each step of the analysis. For example you might have the following files.

- `get_data.R`
- `analyze_data.R`
- `summarize_results.R`

If you migrate to `drake`, you may be tempted to set up a workflow plan like this.

```{r badsource}
bad_plan <- drake_plan(
  my_data = source(file_in("get_data.R")),
  my_analysis = source(file_in("analyze_data.R")),
  my_summaries = source(file_in("summarize_data.R"))
)

bad_plan
```

But now, the dependency structure of your work is broken. Your R script files are dependencies, but since `my_data` is not mentioned in a function or command, `drake` does not know that `my_analysis` depends on it.

```{r scripts, echo = FALSE}
files <- c("get_data.R", "analyze_data.R", "summarize_data.R")
lapply(files, file.create)
```

```{r visbad}
config <- drake_config(bad_plan)
vis_drake_graph(config)
```

```{r scripts2, echo = FALSE}
lapply(files, file.remove)
```

Dangers:

1. In the first `make(bad_plan, jobs = 2)`, `drake` will try to build `my_data` and `my_analysis` at the same time even though `my_data` must finish before `my_analysis` begins.
2. `Drake` is oblivious to `data.csv` since it is not explicitly mentioned in a workflow plan command. So when `data.csv` changes, `make(bad_plan)` will not rebuild `my_data`.
3. `my_analysis` will not update when `my_data` changes.
4. The return value of `source()` is formatted counter-intuitively. If `source(file_in("get_data.R"))` is the command for `my_data`, then `my_data` will always be a list with elements `"value"` and `"visible"`. In other words, `source(file_in("get_data.R"))$value` is really what you would want.

In addition, this `source()`-based approach is simply inconvenient. `Drake` rebuilds `my_data` every time `get_data.R` changes, even when those changes are just extra comments or blank lines. On the other hand, in the previous plan that uses `my_data = get_data()`, `drake` does not trigger rebuilds when comments or whitespace in `get_data()` are modified. `Drake` is R-focused, not file-focused. If you embrace this viewpoint, your work will be easier.

### Workflows as R packages

The R package structure is a great way to organize the files of your project. Writing your own package to contain your data science workflow is a good idea, but you will need to

1. Use `expose_imports()` to properly account for all your nested function dependencies, and
2. If you load the package with `devtools::load_all()`, set the `prework` argument of `make()`: e.g. `make(prework = "devtools::load_all()")`.

Thanks to [Jasper Clarkberg](https://github.com/dapperjapper) for the workaround behind `expose_imports()`.

#### Advantages of putting workflows in R packages

- The file organization of R packages is a well-understood community standard. If you follow it, your work may be more readable and thus reproducible.
- R package installation is a standard process. The system makes it easier for others to obtain and run your code.
- You get development and quality control tools for free: [helpers for loading code and creating files](https://github.com/hadley/devtools), [unit testing](http://r-pkgs.had.co.nz/tests.html), [package checks](http://r-pkgs.had.co.nz/check.html), [code coverage](https://github.com/r-lib/covr), and [continuous integration](https://ipub.com/continuous-integration-for-r/).

#### The problem

For `drake`, there is one problem: nested functions. `Drake` always looks for imported functions nested in other imported functions, but only in your environment. When it sees a function from a package, it does not look in its body for other imports.

To see this, consider the `digest()` function from the [`digest` package](https://github.com/eddelbuettel/digest). [`Digest` package](https://github.com/eddelbuettel/digest) is a utility for computing hashes, not a data science workflow, but I will use it to demonstrate how `drake` treats imports from packages.


```{r nestingproblem}
library(digest)
g <- function(x){
  digest(x)
}
f <- function(x){
  g(x)
}
plan <- drake_plan(x = f(1))

## Here are the reproducibly tracked objects in the workflow.
config <- drake_config(plan)
tracked(config)

## But the `digest()` function has dependencies too.
## Because `drake` knows `digest()` is from a package,
## it ignores these dependencies by default.
head(deps_code(digest), 10)
```

#### The solution

To force `drake` to dive deeper into the nested functions in a package, you must use `expose_imports()`. Again, I demonstrate with the [`digest` package](https://github.com/eddelbuettel/digest) package, but you should really only do this with a package you write yourself to contain your workflow. For external packages, [packrat](https://rstudio.github.io/packrat/) is a much better solution for package reproducibility.

```{r nestingsolution}
expose_imports(digest)
config <- drake_config(plan)
new_objects <- tracked(config)
head(new_objects, 10)
length(new_objects)

## Now when you call `make()`, `drake` will dive into `digest`
## to import dependencies.

cache <- storr::storr_environment() # just for examples
make(plan, cache = cache)
head(cached(cache = cache), 10)
length(cached(cache = cache))
```

```{r rmfiles_caution, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
file.remove("report.Rmd")
unlink(
  c(
    "data.csv", "Makefile", "report.Rmd",
    "shell.sh", "STDIN.o*", "Thumbs.db"
  )
)
```

## Remote data sources

Some workflows rely on remote data from the internet, and the workflow needs to refresh when the datasets change. As an example, let us consider the download logs of [CRAN packages](https://cran.r-project.org/).

```{r logs1}
library(drake)
library(R.utils) # For unzipping the files we download.
library(curl)    # For downloading data.
library(httr)    # For querying websites.

url <- "http://cran-logs.rstudio.com/2018/2018-02-09-r.csv.gz"
```

How do we know when the data at the URL changed? We get the time that the file was last modified. (Alternatively, we could use an HTTP ETag.)

```{r logs2}
query <- HEAD(url)
timestamp <- query$headers[["last-modified"]]
timestamp
```

In our workflow plan, the timestamp is a target and a dependency. When the timestamp changes, so does everything downstream.

```{r logs3}
cranlogs_plan <- drake_plan(
  timestamp = HEAD(url)$headers[["last-modified"]],
  logs = get_logs(url, timestamp),
  strings_in_dots = "literals"
)
cranlogs_plan
```

To make sure we always have the latest timestamp, we use the `"always"` trigger. (For more on triggers, see the [guide to debugging and testing drake projects](#debug).)

```{r logs4}
cranlogs_plan$trigger <- c("always", "any")
cranlogs_plan
```

Lastly, we define the `get_logs()` function, which actually downloads the data.

```{r logs5}
## The ... is just so we can write dependencies as function arguments
## in the workflow plan.
get_logs <- function(url, ...){
  curl_download(url, "logs.csv.gz")       # Get a big file.
  gunzip("logs.csv.gz", overwrite = TRUE) # Unzip it.
  out <- read.csv("logs.csv", nrows = 4)  # Extract the data you need.
  unlink(c("logs.csv.gz", "logs.csv"))    # Remove the big files
  out                                     # Value of the target.
}
```

When we are ready, we run the workflow.

```{r logs6}
make(cranlogs_plan)

readd(logs)
```

```{r endofline_bestpractices, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db", "file.txt")
)
```
