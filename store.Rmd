# Storage {#store}

```{r store_setup, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

```{r store_setup2, message = FALSE, warning = FALSE, echo = FALSE}
library(drake)
library(tidyverse)
```

When you run `make()`, `drake` stores your targets in a cache.

```{r mtcars_storage}
library(drake)
load_mtcars_example() # from https://github.com/wlandau/drake-examples/tree/master/mtcars
make(my_plan, verbose = 0L)
```

The default cache is a hidden `.drake` folder. 

```{r getdefaultcache, eval = FALSE}
find_cache()
### [1] "/home/you/project/.drake"

find_project()
### [1] "/home/you/project"
```

`drake` uses the [storr](https://github.com/richfitz/storr) package to create and modify caches.

```{r oopcache}
library(storr)
cache <- storr_rds(".drake")

head(cache$list())

head(cache$get("small"))
```

`drake` has its own interface on top of [storr](https://github.com/richfitz/storr) to make it easier to work with the default  `.drake/` cache. The `loadd()`, `readd()`, and `cached()` functions explore saved targets.

```{r explore_mtcars}
head(cached())

head(readd(small))

loadd(large)

head(large)

rm(large) # Does not remove `large` from the cache.
```

Functions `get_cache()`, `storr::storr_rds()`, and `new_cache()` recover and create caches.

```{r recovercache}
cache <- get_cache(path = getwd()) # root or subdirectory of a drake project
cache$driver$path

cache <- storr::storr_rds(path = ".drake") # actual cache folder

cache2 <- new_cache("my_new_cache")
cache2$driver$path
```

You can supply your own cache to `make()` and similar functions.

```{r makecache}
cache2$list()

plan2 <- drake_plan(x = 1, y = sqrt(x))
make(plan2, cache = cache2)

cache2$list()

config <- drake_config(plan = plan2, cache = cache2)
vis_drake_graph(config)
```

Destroy caches to remove them from your file system.

```{r rm3}
cache$destroy()
cache2$destroy()
```

## Cache formats

### RDS caches

By default, `drake` uses `storr_rds()` caches because they allow `make(jobs = 4)` to safely store multiple targets in parallel. To achieve this thread safety, however, [`storr`](https://github.com/richfitz/storr) splits up the data into a pool of tiny cryptically-named files.

```{r rdsinternals}
make(my_plan, verbose = 0L)
head(list.files(".drake/data"))
head(list.files(".drake/keys/objects"))
```

This makes RDS caches difficult to share with collaborators and put under version control. For the sake of portability, you may wish to work with database cashes as the next section describes.

Alternatively, you can track changes in a cache log with fingerprints of all your targets.

```{r cachelogfile}
drake_cache_log()
make(my_plan, verbose = 0L, cache_log_file = TRUE)
read_csv("drake_cache.csv", col_types = cols())
```

Use the `cache_log_file` argument of `make()` to refresh the cache log file every time you run `make()`. Then, if you put this file under version control (e.g. with git/GitHub) then the commit history will tell you how your data objects change over time.

### Database caches

It is possible use a single SQLite database file as the cache.

```{r databaseinit}
mydb <- DBI::dbConnect(RSQLite::SQLite(), "database-file.sqlite")
cache <- storr::storr_dbi("datatable", "keystable", mydb)
make(my_plan, cache = cache, verbose = 0L)
loadd(small, cache = cache)
head(small)
```

```{r portabledisconnect2, echo = FALSE}
cache$destroy()
```

But be careful: for safe parallel computing (`jobs` > 1) there are additional requirements for `make()`:

1. Select either `parallelism = "clustermq"` or `parallelism = "future"`.
2. Select `caching = "master"` to ensure that only the master process touches the cache.

For a more complete demonstration, please see [this example code](https://github.com/wlandau/drake-examples/tree/master/dbi), which you can download with `drake_example("dbi")`.

### Environment caches

Environment caches live in computer memory, not your file system, so they are a nice way to run small and fast experiments. However, unless you save the cache manually, all your data will be lost when you quit your R session. And for large projects, you may not be able to fit all your data in memory anyway.

```{r envircache, eval = FALSE}
cache <- storr_environment()
make(my_plan, cache = cache)
```

## Hash algorithms

[storr](https://github.com/richfitz/storr) caches use [hash functions ](https://en.wikipedia.org/wiki/Hash_function) to keep track of stored objects. A hash function is just a way to fingerprint data. The idea is to represent an arbitrary chunks of data using (nearly) unique strings of fixed size.

```{r hashes}
library(digest) # package for hashing objects and files
smaller_data <- 12
larger_data <- rnorm(1000)

digest(smaller_data) # compute the hash

digest(larger_data)
```

The `digest` package (used by both `drake` and `storr`) supports a wide variety of hash functions. Some generate larger hash keys, some are slower to compute, and others are more prone to "collisions" (where two different data objects are given the same hash key). The `digest` package supports a variety of hash algorithms.

```{r compare_algo_lengths}
digest(larger_data, algo = "sha512")

digest(larger_data, algo = "md5")

digest(larger_data, algo = "xxhash64")

digest(larger_data, algo = "murmur32")
```

For `drake`, the default hash algorithm is `xxhash64`. You can choose different hash functions with the `hash_algorithm` argument to `new_cache()`, `storr_rds()`, and similar functions. (For `drake` version 6.2.1 and earlier, see the `long_hash_algo` and `short_hash_algo` arguments.)

```{r cachehashoptions, eval = FALSE}
cache3 <- new_cache("cache_3_path", hash_algorithm = "murmur32")
cache4 <- storr_rds("cache_4_path", hash_algorithm = "crc32")
```
