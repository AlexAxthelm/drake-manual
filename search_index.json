[
["index.html", "The drake R Package User Manual Chapter 1 Introduction 1.1 The drake R package 1.2 Installation 1.3 Why drake? 1.4 Documentation 1.5 Help and troubleshooting 1.6 Similar work 1.7 Acknowledgements", " The drake R Package User Manual Will Landau, Kirill Müller, Alex Axthelm, Jasper Clarkberg, Lorenz Walthert Copyright Eli Lilly and Company Chapter 1 Introduction 1.1 The drake R package Data analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. How much of that valuable output can you keep, and how much do you need to update? How much runtime must you endure all over again? For projects in R, the drake package can help. It analyzes your workflow, skips steps with up-to-date results, and orchestrates the rest with optional distributed computing. At the end, drake provides evidence that your results match the underlying code and data, which increases your ability to trust your research. 1.2 Installation You can choose among different versions of drake. The latest CRAN release may be more convenient to install, but this manual is kept up to date with the GitHub version, so some features described here may not yet be available on CRAN. # Install the latest stable release from CRAN. install.packages(&quot;drake&quot;) # Alternatively, install the development version from GitHub. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;ropensci/drake&quot;) 1.3 Why drake? 1.3.1 What gets done stays done. Too many data science projects follow a Sisyphean loop: Launch the code. Wait while it runs. Discover an issue. Restart from scratch. Ordinarily, it is hard to avoid restarting from scratch. But with drake, you can automatically Launch the parts that changed since last time. Skip the rest. 1.3.2 Reproducibility with confidence The R community emphasizes reproducibility. Traditional themes include scientific replicability, literate programming with knitr, and version control with git. But internal consistency is important too. Reproducibility carries the promise that your output matches the code and data you say you used. With the exception of non-default triggers and hasty mode, drake strives to keep this promise. 1.3.2.1 Evidence Suppose you are reviewing someone else’s data analysis project for reproducibility. You scrutinize it carefully, checking that the datasets are available and the documentation is thorough. But could you re-create the results without the help of the original author? With drake, it is quick and easy to find out. make(plan) config &lt;- drake_config(plan) outdated(config) With everything already up to date, you have tangible evidence of reproducibility. Even though you did not re-create the results, you know the results are re-creatable. They faithfully show what the code is producing. Given the right package environment and system configuration, you have everything you need to reproduce all the output by yourself. 1.3.2.2 Ease When it comes time to actually rerun the entire project, you have much more confidence. Starting over from scratch is trivially easy. clean() # Remove the original author&#39;s results. make(plan) # Independently re-create the results from the code and input data. 1.3.2.3 Independent replication With even more evidence and confidence, you can invest the time to independently replicate the original code base if necessary. Up until this point, you relied on basic drake functions such as make(), so you may not have needed to peek at any substantive author-defined code in advance. In that case, you can stay usefully ignorant as you reimplement the original author’s methodology. In other words, drake could potentially improve the integrity of independent replication. 1.3.2.4 Readability and transparency Ideally, independent observers should be able to read your code and understand it. drake helps in several ways. The drake plan explicitly outlines the steps of the analysis, and vis_drake_graph() visualizes how those steps depend on each other. drake takes care of the parallel scheduling and high-performance computing (HPC) for you. That means the HPC code is no longer tangled up with the code that actually expresses your ideas. You can generate large collections of targets without necessarily changing your code base of imported functions, another nice separation between the concepts and the execution of your workflow 1.3.3 Aggressively scale up. Not every project can complete in a single R session on your laptop. Some projects need more speed or computing power. Some require a few local processor cores, and some need large high-performance computing systems. But parallel computing is hard. Your tables and figures depend on your analysis results, and your analyses depend on your datasets, so some tasks must finish before others even begin. drake knows what to do. Parallelism is implicit and automatic. See the high-performance computing guide for all the details. # Use the spare cores on your local machine. make(plan, jobs = 4) # Or scale up to a supercomputer. drake_batchtools_tmpl_file(&quot;slurm&quot;) # https://slurm.schedmd.com/ library(future.batchtools) future::plan(batchtools_slurm, template = &quot;batchtools.slurm.tmpl&quot;, workers = 100) make(plan, parallelism = &quot;future_lapply&quot;) 1.4 Documentation The main resources to learn drake are The user manual, which contains a friendly introduction and several long-form tutorials. The documentation website, which serves as a quicker reference. Kirill Müller’s drake workshop from March 5, 2018. 1.4.1 Cheat sheet Thanks to Kirill for preparing a drake cheat sheet for the workshop. 1.4.2 Frequently asked questions The FAQ page is an index of links to appropriately-labeled issues on GitHub. To contribute, please submit a new issue and ask that it be labeled as a frequently asked question. 1.4.3 Function reference The reference section lists all the available functions. Here are the most important ones. drake_plan(): create a workflow data frame (like my_plan). make(): build your project. loadd(): load one or more built targets into your R session. readd(): read and return a built target. drake_config(): create a master configuration list for other user-side functions. vis_drake_graph(): show an interactive visual network representation of your workflow. outdated(): see which targets will be built in the next make(). deps(): check the dependencies of a command or function. failed(): list the targets that failed to build in the last make(). diagnose(): return the full context of a build, including errors, warnings, and messages. 1.4.4 Tutorials Thanks to Kirill for constructing two interactive learnr tutorials: one supporting drake itself, and a prerequisite walkthrough of the cooking package. 1.4.5 Examples The official rOpenSci use cases and associated discussion threads describe applications of drake in action. Here are some more real-world sightings of drake in the wild. efcaguab/demografia-del-voto efcaguab/great-white-shark-nsw IndianaCHE/Detailed-SSP-Reports pat-s/pathogen-modeling sol-eng/tensorflow-w-r tiernanmartin/home-and-hope There are also multiple drake-powered example projects available here, ranging from beginner-friendly stubs to demonstrations of high-performance computing. You can generate the files for a project with drake_example() (e.g. drake_example(&quot;gsp&quot;)), and you can list the available projects with drake_examples(). You can contribute your own example project with a fork and pull request. 1.4.6 Presentations Author Venue Date Materials Amanda Dobbyn R-Ladies NYC 2019-02-12 slides, source Will Landau Harvard DataFest 2019-01-22 slides, source Karthik Ram RStudio Conference 2019-01-18 video, slides, resources Sina Rüeger Geneva R User Group 2018-10-04 slides, example code Will Landau R in Pharma 2018-08-16 video, slides, source Christine Stawitz R-Ladies Seattle 2018-06-25 materials Kirill Müller Swiss Institute of Bioinformatics 2018-03-05 workshop, slides, source, exercises 1.4.7 Context and history For context and history, check out this post on the rOpenSci blog and episode 22 of the R Podcast. 1.5 Help and troubleshooting The following resources document many known issues and challenges. Frequently-asked questions. Cautionary notes and edge cases Debugging and testing drake projects Other known issues (please search both open and closed ones). If you are still having trouble, please submit a new issue with a bug report or feature request, along with a minimal reproducible example where appropriate. The GitHub issue tracker is mainly intended for bug reports and feature requests. While questions about usage etc. are also highly encouraged, you may alternatively wish to post to Stack Overflow and use the drake-r-package tag. 1.6 Similar work 1.6.1 GNU Make The original idea of a time-saving reproducible build system extends back at least as far as GNU Make, which still aids the work of data scientists as well as the original user base of complied language programmers. In fact, the name “drake” stands for “Data Frames in R for Make”. Make is used widely in reproducible research. Below are some examples from Karl Broman’s website. Bostock, Mike (2013). “A map of flowlines from NHDPlus.” https://github.com/mbostock/us-rivers. Powered by the Makefile at https://github.com/mbostock/us-rivers/blob/master/Makefile. Broman, Karl W (2012). “Halotype Probabilities in Advanced Intercross Populations.” G3 2(2), 199-202.Powered by the Makefile at https://github.com/kbroman/ailProbPaper/blob/master/Makefile. Broman, Karl W (2012). “Genotype Probabilities at Intermediate Generations in the Construction of Recombinant Inbred Lines.” *Genetics 190(2), 403-412. Powered by the Makefile at https://github.com/kbroman/preCCProbPaper/blob/master/Makefile. Broman, Karl W and Kim, Sungjin and Sen, Saunak and Ane, Cecile and Payseur, Bret A (2012). “Mapping Quantitative Trait Loci onto a Phylogenetic Tree.” Genetics 192(2), 267-279. Powered by the Makefile at https://github.com/kbroman/phyloQTLpaper/blob/master/Makefile. There are several reasons for R users to prefer drake instead. drake already has a Make-powered parallel backend. Just run make(..., parallelism = &quot;Makefile&quot;, jobs = 2) to enjoy most of the original benefits of Make itself. Improved scalability. With Make, you must write a potentially large and cumbersome Makefile by hand. But with drake, you can use wildcard templating to automatically generate massive collections of targets with minimal code. Lower overhead for light-weight tasks. For each Make target that uses R, a brand new R session must spawn. For projects with thousands of small targets, that means more time may be spent loading R sessions than doing the actual work. With make(..., parallelism = &quot;mclapply, jobs = 4&quot;), drake launches 4 persistent workers up front and efficiently processes the targets in R. Convenient organization of output. With Make, the user must save each target as a file. drake saves all the results for you automatically in a storr cache so you do not have to micromanage the results. 1.6.2 Remake drake overlaps with its direct predecessor, remake. In fact, drake owes its core ideas to remake and Rich Fitzjohn. Remake’s development repository lists several real-world applications. drake surpasses remake in several important ways, including but not limited to the following. High-performance computing. Remake has no native parallel computing support. drake, on the other hand, has a thorough selection of parallel computing technologies and scheduling algorithms. Thanks to future, future.batchtools, and batchtools, it is straightforward to configure a drake project for most popular job schedulers, such as SLURM, TORQUE, and the Grid Engine, as well as systems contained in Docker images. A friendly interface. In remake, the user must manually write a YAML configuration file to arrange the steps of a workflow, which leads to some of the same scalability problems as Make. drake’s data-frame-based interface and wildcard templating functionality easily generate workflows at scale. Thorough documentation. drake contains thorough user manual, a reference website, a comprehensive README, examples in the help files of user-side functions, and accessible example code that users can write with drake::example_drake(). Active maintenance. drake is actively developed and maintained, and issues are usually addressed promptly. Presence on CRAN. At the time of writing, drake is available on CRAN, but remake is not. 1.6.3 Memoise Memoization is the strategic caching of the return values of functions. Every time a memoized function is called with a new set of arguments, the return value is saved for future use. Later, whenever the same function is called with the same arguments, the previous return value is salvaged, and the function call is skipped to save time. The memoise package is an excellent implementation of memoization in R. However, memoization does not go far enough. In reality, the return value of a function depends not only on the function body and the arguments, but also on any nested functions and global variables, the dependencies of those dependencies, and so on upstream. drake surpasses memoise because it uses the entire dependency network graph of a project to decide which pieces need to be rebuilt and which ones can be skipped. 1.6.4 Knitr and R Markdown Much of the R community uses knitr and R Markdown for reproducible research. The idea is to intersperse code chunks in an R Markdown or *.Rnw file and then generate a dynamic report that weaves together code, output, and prose. Knitr is not designed to be a serious pipeline toolkit, and it should not be the primary computational engine for medium to large data analysis projects. Knitr scales far worse than Make or remake. The whole point is to consolidate output and prose, so it deliberately lacks the essential modularity. There is no obvious high-performance computing support. While there is a way to skip chunks that are already up to date (with code chunk options cache and autodep), this functionality is not the focus of knitr. It is deactivated by default, and remake and drake are more dependable ways to skip work that is already up to date. drake was designed to manage the entire workflow with knitr reports as targets. The strategy is analogous for knitr reports within remake projects. 1.6.5 Factual’s Drake Factual’s Drake is similar in concept, but the development effort is completely unrelated to the drake R package. 1.6.6 Other pipeline toolkits There are countless other successful pipeline toolkits. The drake package distinguishes itself with its R-focused approach, Tidyverse-friendly interface, and a thorough selection of parallel computing technologies and scheduling algorithms. 1.7 Acknowledgements Special thanks to Jarad Niemi, my advisor from graduate school, for first introducing me to the idea of Makefiles for research. He originally set me down the path that led to drake. Many thanks to Julia Lowndes, Ben Marwick, and Peter Slaughter for reviewing drake for rOpenSci, and to Maëlle Salmon for such active involvement as the editor. Thanks also to the following people for contributing early in development. Alex Axthelm Chan-Yub Park Daniel Falster Eric Nantz Henrik Bengtsson Ian Watson Jasper Clarkberg Kendon Bell Kirill Müller Credit for images is attributed here. "],
["walkthrough.html", "Chapter 2 Walkthrough 2.1 Set the stage. 2.2 Make your results. 2.3 Go back and fix things. 2.4 Try it yourself! 2.5 Thanks", " Chapter 2 Walkthrough A typical data analysis workflow is a sequence of data transformations. Raw data becomes tidy data, then turns into fitted models, summaries, and reports. Other analyses are usually variations of this pattern, and drake can easily accommodate them. 2.1 Set the stage. To set up a project, load your packages, library(drake) library(dplyr) library(ggplot2) load your custom functions, create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram() } check any supporting files (optional), ## Get the files with drake_example(&quot;main&quot;). file.exists(&quot;raw_data.xlsx&quot;) #&gt; [1] TRUE file.exists(&quot;report.Rmd&quot;) #&gt; [1] TRUE and plan what you are going to do. plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 hist create_plot(data) … #&gt; 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… Optionally, visualize your workflow to make sure you set it up correctly. The graph is interactive, so you can click, drag, hover, zoom, and explore. config &lt;- drake_config(plan) vis_drake_graph(config) 2.2 Make your results. So far, we have just been setting the stage. Use make() to do the real work. Targets are built in the correct order regardless of the row order of plan. make(plan) #&gt; target raw_data #&gt; target data #&gt; target fit #&gt; target hist #&gt; target report Except for output files like report.html, your output is stored in a hidden .drake/ folder. Reading it back is easy. readd(data) # See also loadd(). #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; # … with 140 more rows The graph shows everything up to date. vis_drake_graph(config) 2.3 Go back and fix things. You may look back on your work and see room for improvement, but it’s all good! The whole point of drake is to help you go back and change things quickly and painlessly. For example, we forgot to give our histogram a bin width. readd(hist) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. So let’s fix the plotting function. create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + theme_gray(20) } drake knows which results are affected. vis_drake_graph(config) The next make() just builds hist and report. No point in wasting time on the data or model. make(plan) #&gt; target hist #&gt; target report loadd(hist) hist 2.4 Try it yourself! Use drake_example(&quot;main&quot;) to download the code files for this example. 2.5 Thanks Thanks to Kirill Müller for originally providing this example. "],
["plans.html", "Chapter 3 drake plans 3.1 What is a drake plan? 3.2 Plans are similar to R scripts. 3.3 So why do we use plans? 3.4 Special custom columns in your plan 3.5 Large plans 3.6 Create large plans the old way", " Chapter 3 drake plans 3.1 What is a drake plan? A drake plan is a data frame with columns named target and command. Each target is an R object, and each command is an expression to produce it.1 The drake_plan() function is the best way to set up plans.2 Recall the plan from the walkthrough: plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 hist create_plot(data) … #&gt; 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… drake_plan() does not run the workflow, it only creates the plan. To build the actual targets, we need to run make(). Creating the plan is like writing an R script, and running make(your_plan) is like calling source(&quot;your_script.R&quot;). 3.2 Plans are similar to R scripts. Your drake plan is like a top-level R script that runs everything from end to end. In fact, you can convert back and forth between plans and scripts using functions plan_to_code() and code_to_plan() (with some caveats). plan_to_code(plan, &quot;new_script.R&quot;) #&gt; Loading required namespace: styler cat(readLines(&quot;new_script.R&quot;), sep = &quot;\\n&quot;) #&gt; raw_data &lt;- readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) #&gt; data &lt;- raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) #&gt; fit &lt;- lm(Sepal.Width ~ Petal.Width + Species, data) #&gt; hist &lt;- create_plot(data) #&gt; report &lt;- rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), #&gt; output_file = file_out(&quot;report.html&quot;), #&gt; quiet = TRUE #&gt; ) code_to_plan(&quot;new_script.R&quot;) #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 4 hist create_plot(data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… And plan_to_notebook() turns plans into R notebooks. plan_to_notebook(plan, &quot;new_notebook.Rmd&quot;) cat(readLines(&quot;new_notebook.Rmd&quot;), sep = &quot;\\n&quot;) #&gt; --- #&gt; title: &quot;My Notebook&quot; #&gt; output: html_notebook #&gt; --- #&gt; #&gt; ```{r my_code} #&gt; raw_data &lt;- readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) #&gt; data &lt;- raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) #&gt; fit &lt;- lm(Sepal.Width ~ Petal.Width + Species, data) #&gt; hist &lt;- create_plot(data) #&gt; report &lt;- rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), #&gt; output_file = file_out(&quot;report.html&quot;), #&gt; quiet = TRUE #&gt; ) #&gt; ``` code_to_plan(&quot;new_notebook.Rmd&quot;) #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 4 hist create_plot(data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… 3.3 So why do we use plans? If you have ever waited more than 10 minutes for an R script to finish, then you know the frustration of having to rerun the whole thing every time you make a change. Plans make life easier. 3.3.1 Plans chop up the work into pieces. Some targets may need an update while others may not. In the walkthrough, make() was smart enough to skip the data cleaning step and just rebuild the plot and report. drake and its plans compartmentalize the work, and this can save you from wasted effort in the long run. 3.3.2 drake uses plans to schedule you work. make() automatically learns the build order of your targets and how to run them in parallel. The underlying magic is static code analysis, which automatically detects the dependencies of each target without having to run its command. create_plot &lt;- function(data) { ggplot(data, aes_string(x = &quot;Petal.Width&quot;, fill = &quot;Species&quot;)) + geom_histogram(bins = 20) } deps_code(create_plot) #&gt; # A tibble: 3 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 geom_histogram globals #&gt; 2 ggplot globals #&gt; 3 aes_string globals deps_code(quote(create_plot(datasets::iris))) #&gt; # A tibble: 2 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 create_plot globals #&gt; 2 datasets::iris namespaced Because of the dependency relationships, row order does not matter once the plan is fully defined. The following plan declares file before plot. small_plan &lt;- drake_plan( file = ggsave(file_out(&quot;plot.png&quot;), plot, width = 7, height = 5), plot = create_plot(datasets::iris) ) But file actually depends on plot. small_config &lt;- drake_config(small_plan) vis_drake_graph(small_config) So make() builds plot first. library(ggplot2) make(small_plan) #&gt; target plot #&gt; target file 3.4 Special custom columns in your plan You can add other columns besides the required target and command. cbind(small_plan, cpu = c(1, 2)) #&gt; target command cpu #&gt; 1 file ggsave(file_out(&quot;plot.png&quot;), plot, width = 7, height = 5) 1 #&gt; 2 plot create_plot(datasets::iris) 2 Within drake_plan(), target() lets you create any custom column except target, command, and transform, the last of which has a special meaning. drake_plan( file = target( ggsave(file_out(&quot;plot.png&quot;), plot), elapsed = 10 ), create_plot(datasets::iris) ) #&gt; # A tibble: 2 x 3 #&gt; target command elapsed #&gt; &lt;chr&gt; &lt;expr&gt; &lt;dbl&gt; #&gt; 1 file ggsave(file_out(&quot;plot.png&quot;), plot) 10 #&gt; 2 drake_target_1 create_plot(datasets::iris) NA The following columns have special meanings for make(). elapsed and cpu: number of seconds to wait for the target to build before timing out (elapsed for elapsed time and cpu for CPU time). hpc: logical values (TRUE/FALSE/NA) whether to send each target to parallel workers. Click here to learn more. resources: target-specific lists of resources for a computing cluster. See the advanced options in the parallel computing chapter for details. retries: number of times to retry building a target in the event of an error. trigger: rule to decide whether a target needs to run. See the trigger chapter to learn more. 3.5 Large plans drake version 7.0.0 introduced new syntax to make it easier to create plans. To try it out before the next CRAN release, install the current development version from GitHub. install.packages(&quot;remotes&quot;) library(remotes) install_github(&quot;ropensci/drake&quot;) 3.5.1 How to create large plans Ordinarily, drake_plan() requires you to write out all the targets one-by-one. This is a literal pain. drake_plan( data = get_data(), analysis_1_1 = fit_model_x(data, mean = 1, sd = 1), analysis_2_1 = fit_model_x(data, mean = 2, sd = 1), analysis_5_1 = fit_model_x(data, mean = 5, sd = 1), analysis_10_1 = fit_model_x(data, mean = 10, sd = 1), analysis_100_1 = fit_model_x(data, mean = 100, sd = 1), analysis_1000_1 = fit_model_x(data, mean = 1000, sd = 1), analysis_1_2 = fit_model_x(data, mean = 1, sd = 2), analysis_2_2 = fit_model_x(data, mean = 2, sd = 2), analysis_5_2 = fit_model_x(data, mean = 5, sd = 2), analysis_10_2 = fit_model_x(data, mean = 10, sd = 2), analysis_100_2 = fit_model_x(data, mean = 100, sd = 2), analysis_1000_2 = fit_model_x(data, mean = 1000, sd = 2), # UUUGGGHH my wrists are cramping! :( ... ) Transformations reduce typing, especially when combined with tidy evaluation (!!). lots_of_sds &lt;- as.numeric(1:1e3) drake_plan( data = get_data(), analysis = target( fun(data, mean = mean_val, sd = sd_val), transform = cross(mean_val = c(2, 5, 10, 100, 1000), sd_val = !!lots_of_sds) ) ) #&gt; # A tibble: 5,001 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_data() #&gt; 2 analysis_2_1 fun(data, mean = 2, sd = 1) #&gt; 3 analysis_5_1 fun(data, mean = 5, sd = 1) #&gt; 4 analysis_10_1 fun(data, mean = 10, sd = 1) #&gt; 5 analysis_100_1 fun(data, mean = 100, sd = 1) #&gt; 6 analysis_1000_1 fun(data, mean = 1000, sd = 1) #&gt; 7 analysis_2_2 fun(data, mean = 2, sd = 2) #&gt; 8 analysis_5_2 fun(data, mean = 5, sd = 2) #&gt; 9 analysis_10_2 fun(data, mean = 10, sd = 2) #&gt; 10 analysis_100_2 fun(data, mean = 100, sd = 2) #&gt; # … with 4,991 more rows Behind the scenes during a transformation, drake_plan() creates new columns to track what is happening. You can see them with trace = TRUE. drake_plan( data = get_data(), analysis = target( analyze(data, mean, sd), transform = map(mean = c(3, 4), sd = c(1, 2)) ), trace = TRUE ) #&gt; # A tibble: 3 x 5 #&gt; target command mean sd analysis #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 data get_data() &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 analysis_3_1 analyze(data, 3, 1) 3 1 analysis_3_1 #&gt; 3 analysis_4_2 analyze(data, 4, 2) 4 2 analysis_4_2 Because of those columns, you can chain transformations together in complex pipelines. plan1 &lt;- drake_plan( small = get_small_data(), large = get_large_data(), analysis = target( # Analyze each dataset once with a different mean. analyze(data, mean), transform = map(data = c(small, large), mean = c(1, 2)) ), # Calculate 2 different performance metrics on every model fit. metric = target( metric_fun(analysis), # mse = mean squared error, mae = mean absolute error. # Assume these are functions you write. transform = cross(metric_fun = c(mse, mae), analysis) ), # Summarize the performance metrics for each dataset. summ_data = target( summary(metric), transform = combine(metric, .by = data) ), # Same, but for each metric type. summ_metric = target( summary(metric), transform = combine(metric, .by = metric_fun) ) ) plan1 #&gt; # A tibble: 12 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 small get_small_data() … #&gt; 2 large get_large_data() … #&gt; 3 analysis_small_1 analyze(small, 1) … #&gt; 4 analysis_large_2 analyze(large, 2) … #&gt; 5 metric_mse_analysis_s… mse(analysis_small_1) … #&gt; 6 metric_mae_analysis_s… mae(analysis_small_1) … #&gt; 7 metric_mse_analysis_l… mse(analysis_large_2) … #&gt; 8 metric_mae_analysis_l… mae(analysis_large_2) … #&gt; 9 summ_data_large summary(metric_mse_analysis_large_2, metric_mae_… #&gt; 10 summ_data_small summary(metric_mse_analysis_small_1, metric_mae_… #&gt; 11 summ_metric_mae summary(metric_mae_analysis_small_1, metric_mae_… #&gt; 12 summ_metric_mse summary(metric_mse_analysis_small_1, metric_mse_… config1 &lt;- drake_config(plan1) vis_drake_graph(config1) And you can write the transformations in any order. The following plan is equivalent to plan1 despite the rearranged rows. plan2 &lt;- drake_plan( # Calculate 2 different performance metrics on every model fit. summ_metric = target( summary(metric), transform = combine(metric, .by = metric_fun) ), metric = target( metric_fun(analysis), # mse = mean squared error, mae = mean absolute error. # Assume these are functions you write. transform = cross(metric_fun = c(mse, mae), analysis) ), small = get_small_data(), analysis = target( # Analyze each dataset once with a different mean. analyze(data, mean), transform = map(data = c(small, large), mean = c(1, 2)) ), # Summarize the performance metrics for each dataset. summ_data = target( summary(metric), transform = combine(metric, .by = data) ), large = get_large_data() # Same, but for each metric type. ) plan2 #&gt; # A tibble: 12 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 summ_metric_mae summary(metric_mae_analysis_small_1, metric_mae_… #&gt; 2 summ_metric_mse summary(metric_mse_analysis_small_1, metric_mse_… #&gt; 3 metric_mse_analysis_s… mse(analysis_small_1) … #&gt; 4 metric_mae_analysis_s… mae(analysis_small_1) … #&gt; 5 metric_mse_analysis_l… mse(analysis_large_2) … #&gt; 6 metric_mae_analysis_l… mae(analysis_large_2) … #&gt; 7 small get_small_data() … #&gt; 8 analysis_small_1 analyze(small, 1) … #&gt; 9 analysis_large_2 analyze(large, 2) … #&gt; 10 summ_data_large summary(metric_mse_analysis_large_2, metric_mae_… #&gt; 11 summ_data_small summary(metric_mse_analysis_small_1, metric_mae_… #&gt; 12 large get_large_data() … config2 &lt;- drake_config(plan2) vis_drake_graph(config2) 3.5.2 Start small To speed up initial testing and experimentation, you may want to limit the number of extra targets created by the map() and cross() transformations. Simply set max_expand in drake_plan(). plan &lt;- drake_plan( data = target( get_data(source), transform = map(source = !!seq_len(25)) ), analysis = target( fn(data, param), transform = cross( data, fn = !!letters, param = !!seq_len(25) ), ), result = target( bind_rows(analysis), transform = combine(analysis, .by = fn) ), max_expand = 3 ) plan #&gt; # A tibble: 33 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_1L get_data(1L) #&gt; 2 data_13L get_data(13L) #&gt; 3 data_25L get_data(25L) #&gt; 4 analysis_a_1L_data_1L a(data_1L, 1L) #&gt; 5 analysis_m_1L_data_1L m(data_1L, 1L) #&gt; 6 analysis_z_1L_data_1L z(data_1L, 1L) #&gt; 7 analysis_a_13L_data_1L a(data_1L, 13L) #&gt; 8 analysis_m_13L_data_1L m(data_1L, 13L) #&gt; 9 analysis_z_13L_data_1L z(data_1L, 13L) #&gt; 10 analysis_a_25L_data_1L a(data_1L, 25L) #&gt; # … with 23 more rows We can more easily inspect the graph to get an idea of the shape of our workflow. config &lt;- drake_config(plan) vis_drake_graph(config) We can even run make(plan) and verify that the subset of targets in the plan turn out okay. make(plan) # Run the small subset of targets we kept. loadd() # Load all those targets into memory. summary(analysis_z_25L_data_13L) # Look at the values of those targets # and decide if we are ready to scale up. Afterwards, we can scale up to the full collection of targets. plan &lt;- drake_plan( data = target( get_data(source), transform = map(source = !!seq_len(25)) ), analysis = target( fn(data, param), transform = cross( data, fn = !!letters, param = !!seq_len(25) ), ), result = target( bind_rows(analysis), transform = combine(analysis, .by = fn) ) ) nrow(plan) #&gt; [1] 16301 The full plan is enormous! Here, vis_drake_graph() would be prohibitively slow and eat up too much memory to show anything useful. Good thing we checked the scaled-down version first. # config &lt;- drake_config(plan) # Takes a long time. # vis_drake_graph() # Takes too much time and too much memory. The next make() will skip the targets we ran before in test mode if they are still up to date. In other words, experimenting on a downsized plan gave us a head start. make(plan) # Full scaled-up workflow. Takes much longer. 3.5.3 The types of transformations drake supports three types of transformations: map(), cross(), and combine(). These are not actual functions, but you can treat them as functions when you use them in drake_plan(). Each transformation takes after a function from the Tidyverse. drake Tidyverse analogue map() pmap() from purrr cross() crossing() from tidyr combine() summarize() from dplyr 3.5.3.1 map() map() creates a new target for each row in a grid. drake_plan( x = target( simulate_data(center, scale), transform = map(center = c(2, 1, 0), scale = c(3, 2, 1)) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_2_3 simulate_data(2, 3) #&gt; 2 x_1_2 simulate_data(1, 2) #&gt; 3 x_0_1 simulate_data(0, 1) You can supply your own custom grid using the .data argument. Note the use of !! below. my_grid &lt;- tibble( sim_function = c(&quot;rnrom&quot;, &quot;rt&quot;, &quot;rcauchy&quot;), title = c(&quot;Normal&quot;, &quot;Student t&quot;, &quot;Cauchy&quot;) ) my_grid$sim_function &lt;- rlang::syms(my_grid$sim_function) drake_plan( x = target( simulate_data(sim_function, title, center, scale), transform = map( center = c(2, 1, 0), scale = c(3, 2, 1), .data = !!my_grid, # In `.id`, you can select one or more grouping variables # for pretty target names. # Set to FALSE to use short numeric suffixes. .id = sim_function # Try `.id = c(sim_function, center)` yourself. ) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_rnrom simulate_data(rnrom, &quot;Normal&quot;, 2, 3) #&gt; 2 x_rt simulate_data(rt, &quot;Student t&quot;, 1, 2) #&gt; 3 x_rcauchy simulate_data(rcauchy, &quot;Cauchy&quot;, 0, 1) 3.5.3.2 Special considerations in map() map() column-binds variables together to create a grid. The lengths of those variables need to be conformable just as with data.frame(). drake_plan( x = target( simulate_data(center, scale), transform = map(center = c(2, 1, 0), scale = c(3, 2)) ) ) #&gt; Error: Failed to make a grid of grouping variables for map(). #&gt; Grouping variables in map() must have suitable lengths for coercion to a data frame. #&gt; Possibly uneven groupings detected in map(center = c(2, 1, 0), scale = c(3, 2)): #&gt; c(&quot;2&quot;, &quot;1&quot;, &quot;0&quot;) #&gt; c(&quot;3&quot;, &quot;2&quot;) Sometimes, the results are sensible when grouping variable lengths are multiples of each other, but be careful. drake_plan( x = target( simulate_data(center, scale), transform = map(center = c(2, 1, 0), scale = 4) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_2_4 simulate_data(2, 4) #&gt; 2 x_1_4 simulate_data(1, 4) #&gt; 3 x_0_4 simulate_data(0, 4) Things get tricker when drake reuses grouping variables from previous transformations. For example, below, each x_* target has an associated nrow value. So if you write transform = map(x), then nrow goes along for the ride. drake_plan( x = target( simulate_data(center), transform = map(center = c(1, 2)) ), y = target( process_data(x, center), transform = map(x) ), trace = TRUE # Adds extra columns for the grouping variables. ) #&gt; # A tibble: 4 x 5 #&gt; target command center x y #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x_1 simulate_data(1) 1 x_1 &lt;NA&gt; #&gt; 2 x_2 simulate_data(2) 2 x_2 &lt;NA&gt; #&gt; 3 y_x_1 process_data(x_1, 1) 1 x_1 y_x_1 #&gt; 4 y_x_2 process_data(x_2, 2) 2 x_2 y_x_2 But if other targets have centers’s of their own, drake_plan() may not know what to do with them. drake_plan( w = target( simulate_data(center), transform = map(center = c(3, 4)) ), x = target( simulate_data_2(center), transform = map(center = c(1, 2)) ), y = target( process_data(w, x, center), transform = map(w, x) ), trace = TRUE ) #&gt; # A tibble: 6 x 6 #&gt; target command center w x y #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 w_3 simulate_data(3) 3 w_3 &lt;NA&gt; &lt;NA&gt; #&gt; 2 w_4 simulate_data(4) 4 w_4 &lt;NA&gt; &lt;NA&gt; #&gt; 3 x_1 simulate_data_2(1) 1 &lt;NA&gt; x_1 &lt;NA&gt; #&gt; 4 x_2 simulate_data_2(2) 2 &lt;NA&gt; x_2 &lt;NA&gt; #&gt; 5 y_w_3_x_1 process_data(w_3, x_1, NA) &lt;NA&gt; w_3 x_1 y_w_3_x_1 #&gt; 6 y_w_4_x_2 process_data(w_4, x_2, NA) &lt;NA&gt; w_4 x_2 y_w_4_x_2 The problems is that there are 4 values of center and only two x_* targets (and two y_* targets). Even if you explicitly supply center to the transformation, map() can only takes the first two values. drake_plan( w = target( simulate_data(center), transform = map(center = c(3, 4)) ), x = target( simulate_data_2(center), transform = map(center = c(1, 2)) ), y = target( process_data(w, x, center), transform = map(w, x, center) ), trace = TRUE ) #&gt; # A tibble: 6 x 6 #&gt; target command center w x y #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 w_3 simulate_data(3) 3 w_3 &lt;NA&gt; &lt;NA&gt; #&gt; 2 w_4 simulate_data(4) 4 w_4 &lt;NA&gt; &lt;NA&gt; #&gt; 3 x_1 simulate_data_2(1) 1 &lt;NA&gt; x_1 &lt;NA&gt; #&gt; 4 x_2 simulate_data_2(2) 2 &lt;NA&gt; x_2 &lt;NA&gt; #&gt; 5 y_w_3_x_1_3 process_data(w_3, x_1, 3) 3 w_3 x_1 y_w_3_x_1_3 #&gt; 6 y_w_4_x_2_4 process_data(w_4, x_2, 4) 4 w_4 x_2 y_w_4_x_2_4 So please inspect the plan before you run it with make(). Once you have a drake_config() object, vis_drake_graph() and deps_target() can help. 3.5.3.3 cross() cross() creates a new target for each combination of argument values. drake_plan( x = target( simulate_data(nrow, ncol), transform = cross(nrow = c(1, 2, 3), ncol = c(4, 5)) ) ) #&gt; # A tibble: 6 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1_4 simulate_data(1, 4) #&gt; 2 x_2_4 simulate_data(2, 4) #&gt; 3 x_3_4 simulate_data(3, 4) #&gt; 4 x_1_5 simulate_data(1, 5) #&gt; 5 x_2_5 simulate_data(2, 5) #&gt; 6 x_3_5 simulate_data(3, 5) 3.5.3.4 combine() In combine(), you can insert multiple targets into individual commands. The closest comparison is the unquote-splice operator !!! from the Tidyverse. plan &lt;- drake_plan( data = target( sim_data(mean = x, sd = y), transform = map(x = c(1, 2), y = c(3, 4)) ), larger = target( bind_rows(data, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data) ) ) plan #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_1_3 sim_data(mean = 1, sd = 3) … #&gt; 2 data_2_4 sim_data(mean = 2, sd = 4) … #&gt; 3 larger bind_rows(data_1_3, data_2_4, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% … drake_plan_source(plan) #&gt; drake_plan( #&gt; data_1_3 = sim_data(mean = 1, sd = 3), #&gt; data_2_4 = sim_data(mean = 2, sd = 4), #&gt; larger = bind_rows(data_1_3, data_2_4, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) config &lt;- drake_config(plan) vis_drake_graph(config) You can different groups of targets in the same command. plan &lt;- drake_plan( data_group1 = target( sim_data(mean = x, sd = y), transform = map(x = c(1, 2), y = c(3, 4)) ), data_group2 = target( pull_data(url), transform = map(url = c(&quot;example1.com&quot;, &quot;example2.com&quot;)) ), larger = target( bind_rows(data_group1, data_group2, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data_group1, data_group2) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_group1_1_3 = sim_data(mean = 1, sd = 3), #&gt; data_group1_2_4 = sim_data(mean = 2, sd = 4), #&gt; data_group2_example1.com = pull_data(&quot;example1.com&quot;), #&gt; data_group2_example2.com = pull_data(&quot;example2.com&quot;), #&gt; larger = bind_rows(data_group1_1_3, data_group1_2_4, data_group2_example1.com, #&gt; data_group2_example2.com, #&gt; .id = &quot;id&quot; #&gt; ) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) And as with group_by() from dplyr, you can create a separate aggregate for each combination of levels of the arguments. Just pass a symbol or vector of symbols to the optional .by argument of combine(). plan &lt;- drake_plan( data = target( sim_data(mean = x, sd = y, skew = z), transform = cross(x = c(1, 2), y = c(3, 4), z = c(5, 6)) ), combined = target( bind_rows(data, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data, .by = c(x, y)) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_1_3_5 = sim_data(mean = 1, sd = 3, skew = 5), #&gt; data_2_3_5 = sim_data(mean = 2, sd = 3, skew = 5), #&gt; data_1_4_5 = sim_data(mean = 1, sd = 4, skew = 5), #&gt; data_2_4_5 = sim_data(mean = 2, sd = 4, skew = 5), #&gt; data_1_3_6 = sim_data(mean = 1, sd = 3, skew = 6), #&gt; data_2_3_6 = sim_data(mean = 2, sd = 3, skew = 6), #&gt; data_1_4_6 = sim_data(mean = 1, sd = 4, skew = 6), #&gt; data_2_4_6 = sim_data(mean = 2, sd = 4, skew = 6), #&gt; combined_1_3 = bind_rows(data_1_3_5, data_1_3_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_2_3 = bind_rows(data_2_3_5, data_2_3_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_1_4 = bind_rows(data_1_4_5, data_1_4_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_2_4 = bind_rows(data_2_4_5, data_2_4_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) In your post-processing, you may need the values of x and y that underly data_1_3 and data_2_4. Solution: get the trace and the target names. We define a new plan plan &lt;- drake_plan( data = target( sim_data(mean = x, sd = y), transform = map(x = c(1, 2), y = c(3, 4)) ), larger = target( post_process(data, plan = ignore(plan)) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data) ), trace = TRUE ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_1_3 = target( #&gt; command = sim_data(mean = 1, sd = 3), #&gt; x = &quot;1&quot;, #&gt; y = &quot;3&quot;, #&gt; data = &quot;data_1_3&quot; #&gt; ), #&gt; data_2_4 = target( #&gt; command = sim_data(mean = 2, sd = 4), #&gt; x = &quot;2&quot;, #&gt; y = &quot;4&quot;, #&gt; data = &quot;data_2_4&quot; #&gt; ), #&gt; larger = target( #&gt; command = post_process(data_1_3, data_2_4, plan = ignore(plan)) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; larger = &quot;larger&quot; #&gt; ) #&gt; ) and a new function post_process &lt;- function(..., plan) { args &lt;- list(...) names(args) &lt;- all.vars(substitute(list(...))) trace &lt;- filter(plan, target %in% names(args)) # Do post-processing with args and trace. } 3.5.4 Grouping variables A grouping variable is an argument to map(), cross(), or combine() that identifies a sub-collection of target names. Grouping variables can be either literals or symbols. Symbols can be scalars or vectors, and you can pass them to transformations with or without argument names. 3.5.4.1 Literal arguments When you pass a grouping variable of literals, you must use an explicit argument name. One does not simply write map(c(1, 2)). drake_plan(x = target(sqrt(y), transform = map(y = c(1, 2)))) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1 sqrt(1) #&gt; 2 x_2 sqrt(2) And if you supply integer sequences the usual way, you may notice some rows are missing. drake_plan(x = target(sqrt(y), transform = map(y = 1:3))) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1 sqrt(1) #&gt; 2 x_3 sqrt(3) Tidy evaluation and as.numeric() make sure all the data points show up. y_vals &lt;- as.numeric(1:3) drake_plan(x = target(sqrt(y), transform = map(y = !!y_vals))) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1 sqrt(1) #&gt; 2 x_2 sqrt(2) #&gt; 3 x_3 sqrt(3) Character vectors usually work without a hitch, and quotes are converted into dots to make valid target names. drake_plan(x = target(get_data(y), transform = map(y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)))) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_a get_data(&quot;a&quot;) #&gt; 2 x_b get_data(&quot;b&quot;) #&gt; 3 x_c get_data(&quot;c&quot;) y_vals &lt;- letters drake_plan(x = target(get_data(y), transform = map(y = !!y_vals))) #&gt; # A tibble: 26 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_a get_data(&quot;a&quot;) #&gt; 2 x_b get_data(&quot;b&quot;) #&gt; 3 x_c get_data(&quot;c&quot;) #&gt; 4 x_d get_data(&quot;d&quot;) #&gt; 5 x_e get_data(&quot;e&quot;) #&gt; 6 x_f get_data(&quot;f&quot;) #&gt; 7 x_g get_data(&quot;g&quot;) #&gt; 8 x_h get_data(&quot;h&quot;) #&gt; 9 x_i get_data(&quot;i&quot;) #&gt; 10 x_j get_data(&quot;j&quot;) #&gt; # … with 16 more rows 3.5.4.2 Named symbol arguments Symbols passed with explicit argument names define new groupings of existing targets on the fly, and only the map() and cross() transformations can accept them this ways. To generate long symbol lists, use the syms() function from the rlang package. Remember to use the tidy evaluation operator !! inside the transformation. vals &lt;- rlang::syms(letters) drake_plan(x = target(get_data(y), transform = map(y = !!vals))) #&gt; # A tibble: 26 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_a get_data(a) #&gt; 2 x_b get_data(b) #&gt; 3 x_c get_data(c) #&gt; 4 x_d get_data(d) #&gt; 5 x_e get_data(e) #&gt; 6 x_f get_data(f) #&gt; 7 x_g get_data(g) #&gt; 8 x_h get_data(h) #&gt; 9 x_i get_data(i) #&gt; 10 x_j get_data(j) #&gt; # … with 16 more rows The new groupings carry over to downstream targets by default, which you can see with trace = TRUE. Below, the rows for targets w_x and w_y have entries in the and z column. drake_plan( x = abs(mean(rnorm(10))), y = abs(mean(rnorm(100, 1))), z = target(sqrt(val), transform = map(val = c(x, y))), w = target(val + 1, transform = map(val)), trace = TRUE ) #&gt; # A tibble: 6 x 5 #&gt; target command val z w #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x abs(mean(rnorm(10))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 y abs(mean(rnorm(100, 1))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 z_x sqrt(x) x z_x &lt;NA&gt; #&gt; 4 z_y sqrt(y) y z_y &lt;NA&gt; #&gt; 5 w_x x + 1 x z_x w_x #&gt; 6 w_y y + 1 y z_y w_y However, this is incorrect because w does not depend on z_x or z_y. So for w, you should write map(val = c(x, y)) instead of map(val) to tell drake to clear the trace. Then, you will see NAs in the z column for w_x and w_y, which is right and proper. drake_plan( x = abs(mean(rnorm(10))), y = abs(mean(rnorm(100, 1))), z = target(sqrt(val), transform = map(val = c(x, y))), w = target(val + 1, transform = map(val = c(x, y))), trace = TRUE ) #&gt; # A tibble: 6 x 5 #&gt; target command val z w #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x abs(mean(rnorm(10))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 y abs(mean(rnorm(100, 1))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 z_x sqrt(x) x z_x &lt;NA&gt; #&gt; 4 z_y sqrt(y) y z_y &lt;NA&gt; #&gt; 5 w_x x + 1 x &lt;NA&gt; w_x #&gt; 6 w_y y + 1 y &lt;NA&gt; w_y 3.5.5 Tags Tags are special optional grouping variables. They are ignored while the transformation is happening and then added to the plan to help subsequent transformations. There are two types of tags: In-tags, which contain the target name you start with, and Out-tags, which contain the target names generated by the transformations. drake_plan( x = target( command, transform = map(y = c(1, 2), .tag_in = from, .tag_out = c(to, out)) ), trace = TRUE ) #&gt; # A tibble: 2 x 7 #&gt; target command y x from to out #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x_1 command 1 x_1 x x_1 x_1 #&gt; 2 x_2 command 2 x_2 x x_2 x_2 Subsequent transformations can use tags as grouping variables and add to existing tags. plan &lt;- drake_plan( prep_work = do_prep_work(), local = target( get_local_data(n, prep_work), transform = map(n = c(1, 2), .tag_in = data_source, .tag_out = data) ), online = target( get_online_data(n, prep_work, port = &quot;8080&quot;), transform = map(n = c(1, 2), .tag_in = data_source, .tag_out = data) ), summary = target( summarize(bind_rows(data, .id = &quot;data&quot;)), transform = combine(data, .by = data_source) ), munged = target( munge(bind_rows(data, .id = &quot;data&quot;)), transform = combine(data, .by = n) ) ) plan #&gt; # A tibble: 9 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 prep_work do_prep_work() #&gt; 2 local_1 get_local_data(1, prep_work) #&gt; 3 local_2 get_local_data(2, prep_work) #&gt; 4 online_1 get_online_data(1, prep_work, port = &quot;8080&quot;) #&gt; 5 online_2 get_online_data(2, prep_work, port = &quot;8080&quot;) #&gt; 6 summary_local summarize(bind_rows(local_1, local_2, .id = &quot;data&quot;)) #&gt; 7 summary_online summarize(bind_rows(online_1, online_2, .id = &quot;data&quot;)) #&gt; 8 munged_1 munge(bind_rows(local_1, online_1, .id = &quot;data&quot;)) #&gt; 9 munged_2 munge(bind_rows(local_2, online_2, .id = &quot;data&quot;)) config &lt;- drake_config(plan) vis_drake_graph(config) 3.5.6 Target names All transformations have an optional .id argument to control the names of targets. Use it to select the grouping variables that go into the names, as well as the order they appear in the suffixes. drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = param2 ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_7 get_data(123, 7) #&gt; 2 data_9 get_data(456, 9) drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = c(param2, param1) ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_7_123 get_data(123, 7) #&gt; 2 data_9_456 get_data(456, 9) drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = c(param1, param2) ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_123_7 get_data(123, 7) #&gt; 2 data_456_9 get_data(456, 9) Set .id to FALSE to ignore the grouping variables altogether. drake_plan( data = target( get_data(param1, param2), transform = map( param1 = c(123, 456), param2 = c(7, 9), param2 = c(&quot;abc&quot;, &quot;xyz&quot;), .id = FALSE ) ) ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_data(123, 7) #&gt; 2 data_2 get_data(456, 9) Finally, drake supports a special .id_chr symbol in commands to let you refer to the name of the current target as a character string. as_chr &lt;- function(x) { deparse(substitute(x)) } plan &lt;- drake_plan( data = target( get_data(param), transform = map(param = c(123, 456)) ), keras_model = target( save_model_hdf5(fit_model(data), file_out(!!sprintf(&quot;%s.h5&quot;, .id_chr))), transform = map(data, .id = param) ), result = target( predict(load_model_hdf5(file_in(!!sprintf(&quot;%s.h5&quot;, as_chr(keras_model))))), transform = map(keras_model, .id = param) ) ) plan #&gt; # A tibble: 6 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_123 get_data(123) … #&gt; 2 data_456 get_data(456) … #&gt; 3 keras_model_1… save_model_hdf5(fit_model(data_123), file_out(&quot;keras_mode… #&gt; 4 keras_model_4… save_model_hdf5(fit_model(data_456), file_out(&quot;keras_mode… #&gt; 5 result_123 predict(load_model_hdf5(file_in(&quot;keras_model_123.h5&quot;))) … #&gt; 6 result_456 predict(load_model_hdf5(file_in(&quot;keras_model_456.h5&quot;))) … drake_plan_source(plan) #&gt; drake_plan( #&gt; data_123 = get_data(123), #&gt; data_456 = get_data(456), #&gt; keras_model_123 = save_model_hdf5(fit_model(data_123), file_out(&quot;keras_model_123.h5&quot;)), #&gt; keras_model_456 = save_model_hdf5(fit_model(data_456), file_out(&quot;keras_model_456.h5&quot;)), #&gt; result_123 = predict(load_model_hdf5(file_in(&quot;keras_model_123.h5&quot;))), #&gt; result_456 = predict(load_model_hdf5(file_in(&quot;keras_model_456.h5&quot;))) #&gt; ) 3.6 Create large plans the old way drake provides several older utility that increase the flexibility of plan creation. They may go away at some point in the future, they are still officially supported at this point. map_plan() evaluate_plan() expand_plan() gather_by() reduce_by() gather_plan() reduce_plan() You can turn the command column of your plan into a character vector (e.g. plan$command &lt;- purrr::map_chr(plan$command, rlang::expr_text)) and drake will still understand you. However, the recommended format is a list of expressions. drake_plan() and friends always supply expression lists.↩ drake_plan() is the best way to create plans, but you can create plans any way you like. drake will understand plans you create directly using data.frame() or tibble().↩ "],
["projects.html", "Chapter 4 drake projects 4.1 Code files 4.2 Safer interactivity 4.3 Script file pitfalls 4.4 Workflows as R packages 4.5 Other tools", " Chapter 4 drake projects drake’s design philosophy is extremely R-focused. It embraces in-memory configuration, in-memory dependencies, interactivity, and flexibility. 4.1 Code files The names and locations of the files are entirely up to you, but this pattern is particularly useful to start with. make.R R/ ├── packages.R ├── functions.R └── plan.R Here, make.R is a master script that Loads your packages, functions, and other in-memory data. Creates the drake plan. Calls make(). Let’s consider the main example, which you can download with drake_example(&quot;main&quot;). Here, our master script is called make.R: source(&quot;R/packages.R&quot;) # loads packages source(&quot;R/functions.R&quot;) # defines the create_plot() function source(&quot;R/plan.R&quot;) # creates the drake plan # options(clustermq.scheduler = &quot;multicore&quot;) # optional parallel computing. Also needs parallelism = &quot;clustermq&quot; make( plan, # defined in R/plan.R verbose = 2 ) We have an R folder containing our supporting files, including packages.R: library(drake) require(dplyr) require(ggplot2) functions.R: create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + theme_gray(20) } and plan.R: plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) To run the example project above, Start a clean new R session. Run the make.R script. On Mac and Linux, you can do this by opening a terminal and entering R CMD BATCH --no-save make.R. On Windows, restart your R session and call source(&quot;make.R&quot;) in the R console. Note: this part of drake does not inherently focus on your script files. There is nothing magical about the names make.R, packages.R, functions.R, or plan.R. Different projects may require different file structures. drake has other functions to inspect your results and examine your workflow. Before invoking them interactively, it is best to start with a clean new R session. # Restart R. interactive() #&gt; [1] TRUE source(&quot;R/packages.R&quot;) source(&quot;R/functions.R&quot;) source(&quot;R/plan.R&quot;) config &lt;- drake_config(plan) vis_drake_graph(config) 4.2 Safer interactivity Caution: functions r_make() etc. described below are still experiemntal. 4.2.1 Motivation A serious drake workflow should be consistent and reliable, ideally with the help of a master R script. Before it builds your targets, this script should begin in a fresh R session and load your packages and functions in a dependable manner. Batch mode makes sure all this goes according to plan. If you use a single persistent interactive R session to repeatedly invoke make() while you develop the workflow, then over time, your session could grow stale and accidentally invalidate targets. For example, if you interactively tinker with a new version of create_plot(), targets hist and report will fall out of date without warning, and the next make() will build them again. Even worse, the outputs from hist and report will be wrong if they depend on a half-finished create_plot(). The quickest workaround is to restart R and source() your setup scripts all over again. However, a better solution is to use r_make() and friends. r_make() runs make() in a new transient R session so that accidental changes to your interactive environment do not break your workflow. 4.2.2 Usage To use r_make(), you need a configuration R script. Unless you supply a custom file path (e.g. r_make(source = &quot;your_file.R&quot;) or options(drake_source = &quot;your_file.R&quot;)) drake assumes this configuration script is called _drake.R. (So the file name really is magical in this case). The suggested file structure becomes: _drake.R R/ ├── packages.R ├── functions.R └── plan.R Like our previous make.R script, _drake.R runs all our pre-make() setup steps. But this time, rather than calling make(), it ends with a call to drake_config(). Example _drake.R: source(&quot;R/packages.R&quot;) source(&quot;R/functions.R&quot;) source(&quot;R/plan.R&quot;) # options(clustermq.scheduler = &quot;multicore&quot;) # optional parallel computing drake_config(plan, verbose = 2) Here is what happens when you call r_make(). drake launches a new transient R session using callr::r(). The remaining steps all happen within this transient session. Run the configuration script (e.g. _drake.R) to Load the packages, functions, global options, drake plan, etc. into the session’s environnment, and Run the call to drake_config()and store the results in a variable called config. Execute make(config = config) The purpose of drake_config() is to collect and sanitize all the parameters and settings that make() needs to do its job. In fact, if you do not set the config argument explicitly, then make() invokes drake_config() behind the scenes. make(plan, parallelism = &quot;clustermq&quot;, jobs = 2, verbose = 6) is equivalent to config &lt;- drake_config(plan, verbose = 2) make(config = config) There are many more r_*() functions besides r_make(), each of which launches a fresh session and runs an inner drake function on the config object from _drake.R. Outer function call Inner function call r_make() make(config = config) r_drake_build(...) drake_build(config, ...) r_outdated(...) outdated(config, ...) r_missed(...) missed(config, ...) r_vis_drake_graph(...) vis_drake_graph(config, ...) r_sankey_drake_graph(...) sankey_drake_graph(config, ...) r_drake_ggraph(...) drake_ggraph(config, ...) r_drake_graph_info(...) drake_graph_info(config, ...) r_predict_runtime(...) predict_runtime(config, ...) r_predict_workers(...) predict_workers(config, ...) clean() r_outdated(r_args = list(show = FALSE)) #&gt; [1] &quot;data&quot; &quot;fit&quot; &quot;hist&quot; &quot;raw_data&quot; &quot;report&quot; r_make() #&gt; Loading required package: dplyr #&gt; #&gt; Attaching package: ‘dplyr’ #&gt; #&gt; The following objects are masked from ‘package:stats’: #&gt; #&gt; filter, lag #&gt; #&gt; The following objects are masked from ‘package:base’: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; #&gt; Loading required package: ggplot2 #&gt; Registered S3 methods overwritten by &#39;ggplot2&#39;: #&gt; method from #&gt; [.quosures rlang #&gt; c.quosures rlang #&gt; print.quosures rlang #&gt; target raw_data #&gt; target data #&gt; target fit #&gt; target hist #&gt; target report r_outdated(r_args = list(show = FALSE)) #&gt; character(0) r_vis_drake_graph(targets_only = TRUE, r_args = list(show = FALSE)) Remarks: You can run r_make() in an interactive session, but the transient process it launches will not be interactive. Thus, any browser() statements in the commands in your drake plan will be ignored. You can select and configure the underlying callr function using arguments r_fn and r_args, respectively. For example code, you can download the updated main example (drake_example(&quot;main&quot;)) and experiment with files _drake.R and interactive.R. 4.3 Script file pitfalls Despite the above discussion of R scripts, drake plans rely more on in-memory functions. You might be tempted to write a plan like the following, but then drake cannot tell that my_analysis depends on my_data. bad_plan &lt;- drake_plan( my_data = source(file_in(&quot;get_data.R&quot;)), my_analysis = source(file_in(&quot;analyze_data.R&quot;)), my_summaries = source(file_in(&quot;summarize_data.R&quot;)) ) bad_config &lt;- drake_config(bad_plan) vis_drake_graph(bad_config, targets_only = TRUE) When it comes to plans, use functions instead. source(&quot;my_functions.R&quot;) # defines get_data(), analyze_data(), etc. good_plan &lt;- drake_plan( my_data = get_data(file_in(&quot;data.csv&quot;)), # External files need to be in commands explicitly. # nolint my_analysis = analyze_data(my_data), my_summaries = summarize_results(my_data, my_analysis) ) good_config &lt;- drake_config(good_plan) vis_drake_graph(good_config, targets_only = TRUE) 4.4 Workflows as R packages The R package structure is a great way to organize and quality-control a data analysis project. If you write a drake workflow as a package, you will need Use expose_imports() to properly account for all your nested function dependencies, and If you load the package with devtools::load_all(), set the prework argument of make(): e.g. make(prework = &quot;devtools::load_all()&quot;). For a minimal example, see Tiernan Martin’s drakepkg. 4.5 Other tools drake enhances reproducibility, but not in all respects. Local library managers, containerization, and session management tools offer more robust solutions in their respective domains. Reproducibility encompasses a wide variety of tools and techniques all working together. Comprehensive overviews: PLOS article by Wilson et al. RStudio Conference 2019 presentation by Karthik Ram. rrtools by Ben Marwick. "],
["churn.html", "Chapter 5 Customer churn and deep learning 5.1 Packages 5.2 Functions 5.3 Plan 5.4 Dependency graph 5.5 Run the models 5.6 Inspect the results 5.7 Add models 5.8 Inspect the results again 5.9 Increasing efficiency 5.10 Tips", " Chapter 5 Customer churn and deep learning drake is intended for projects with long runtimes, and a major use case is deep learning. This chapter demonstrates how to leverage drake to manage a deep learning workflow. The original example comes from a blog post by Matt Dancho, and the chapter’s content itself comes directly from this R notebook, part of an RStudio Solutions Engineering example demonstrating TensorFlow in R. The notebook is modified and redistributed under the terms of the Apache 2.0 license, copyright RStudio (details here). 5.1 Packages First, we load our packages into a fresh R session. library(drake) library(keras) library(tidyverse) library(rsample) library(recipes) library(yardstick) 5.2 Functions drake is R-focused and function-oriented. We create functions to preprocess the data, prepare_recipe &lt;- function(data) { data %&gt;% training() %&gt;% recipe(Churn ~ .) %&gt;% step_rm(customerID) %&gt;% step_naomit(all_outcomes(), all_predictors()) %&gt;% step_discretize(tenure, options = list(cuts = 6)) %&gt;% step_log(TotalCharges) %&gt;% step_mutate(Churn = ifelse(Churn == &quot;Yes&quot;, 1, 0)) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_center(all_predictors(), -all_outcomes()) %&gt;% step_scale(all_predictors(), -all_outcomes()) %&gt;% prep() } define a keras model, define_model &lt;- function(rec) { input_shape &lt;- ncol( juice(rec, all_predictors(), composition = &quot;matrix&quot;) ) keras_model_sequential() %&gt;% layer_dense( units = 16, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;, input_shape = input_shape ) %&gt;% layer_dropout(rate = 0.1) %&gt;% layer_dense( units = 16, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot; ) %&gt;% layer_dropout(rate = 0.1) %&gt;% layer_dense( units = 1, kernel_initializer = &quot;uniform&quot;, activation = &quot;sigmoid&quot; ) } train and serialize a model, train_model &lt;- function(data, rec, batch_size) { model &lt;- define_model(rec) compile( model, optimizer = &quot;adam&quot;, loss = &quot;binary_crossentropy&quot;, metrics = c(&quot;accuracy&quot;) ) x_train_tbl &lt;- juice( rec, all_predictors(), composition = &quot;matrix&quot; ) y_train_vec &lt;- juice(rec, all_outcomes()) %&gt;% pull() fit( object = model, x = x_train_tbl, y = y_train_vec, batch_size = batch_size, epochs = 35, validation_split = 0.30, verbose = 0 ) serialize_model(model) } compare the predictions of a serialized model against reality, confusion_matrix &lt;- function(data, rec, serialized_model) { model &lt;- unserialize_model(serialized_model) testing_data &lt;- bake(rec, testing(data)) x_test_tbl &lt;- testing_data %&gt;% select(-Churn) %&gt;% as.matrix() y_test_vec &lt;- testing_data %&gt;% select(Churn) %&gt;% pull() yhat_keras_class_vec &lt;- model %&gt;% predict_classes(x_test_tbl) %&gt;% as.factor() %&gt;% fct_recode(yes = &quot;1&quot;, no = &quot;0&quot;) yhat_keras_prob_vec &lt;- model %&gt;% predict_proba(x_test_tbl) %&gt;% as.vector() test_truth &lt;- y_test_vec %&gt;% as.factor() %&gt;% fct_recode(yes = &quot;1&quot;, no = &quot;0&quot;) estimates_keras_tbl &lt;- tibble( truth = test_truth, estimate = yhat_keras_class_vec, class_prob = yhat_keras_prob_vec ) estimates_keras_tbl %&gt;% conf_mat(truth, estimate) } and compare the performance of multiple models. compare_models &lt;- function(...) { batch_sizes &lt;- match.call()[-1] %&gt;% as.character() %&gt;% gsub(pattern = &quot;conf_&quot;, replacement = &quot;&quot;) df &lt;- map_df(list(...), summary) %&gt;% filter(.metric %in% c(&quot;accuracy&quot;, &quot;sens&quot;, &quot;spec&quot;)) %&gt;% mutate( batch_size = rep(batch_sizes, each = n() / length(batch_sizes)) ) %&gt;% rename(metric = .metric, estimate = .estimate) ggplot(df) + geom_line( aes(x = metric, y = estimate, color = batch_size, group = batch_size) ) + theme_gray(16) } 5.3 Plan Next, we define our workflow in a drake plan. We will prepare the data, train different models with different batch sizes, and compare the models in terms of performance. batch_sizes &lt;- c(16, 32) plan &lt;- drake_plan( data = read_csv(file_in(&quot;customer_churn.csv&quot;), col_types = cols()) %&gt;% initial_split(prop = 0.3), rec = prepare_recipe(data), model = target( train_model(data, rec, batch_size), transform = map(batch_size = !!batch_sizes) ), conf = target( confusion_matrix(data, rec, model), transform = map(model, .id = batch_size) ), comparison = target( compare_models(conf), transform = combine(conf) ) ) The plan is a data frame with the steps we are going to do. plan #&gt; # A tibble: 7 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data read_csv(file_in(&quot;customer_churn.csv&quot;), col_types = cols()) %… #&gt; 2 rec prepare_recipe(data) … #&gt; 3 model_16 train_model(data, rec, 16) … #&gt; 4 model_32 train_model(data, rec, 32) … #&gt; 5 conf_16 confusion_matrix(data, rec, model_16) … #&gt; 6 conf_32 confusion_matrix(data, rec, model_32) … #&gt; 7 comparison compare_models(conf_16, conf_32) … 5.4 Dependency graph The graph visualizes the dependency relationships among the steps of the workflow. config &lt;- drake_config(plan) vis_drake_graph(config) 5.5 Run the models Call make() to actually run the workflow. make(plan) #&gt; target data #&gt; target rec #&gt; target model_16 #&gt; target model_32 #&gt; target conf_16 #&gt; target conf_32 #&gt; target comparison 5.6 Inspect the results The two models performed about the same. readd(comparison) # see also loadd() 5.7 Add models Let’s try another batch size. batch_sizes &lt;- c(16, 32, 64) plan &lt;- drake_plan( data = read_csv(file_in(&quot;customer_churn.csv&quot;), col_types = cols()) %&gt;% initial_split(prop = 0.3), rec = prepare_recipe(data), model = target( train_model(data, rec, batch_size), transform = map(batch_size = !!batch_sizes) ), conf = target( confusion_matrix(data, rec, model), transform = map(model, .id = batch_size) ), comparison = target( compare_models(conf), transform = combine(conf) ) ) We already trained models with batch sizes 16 and 32, and their dependencies have not changed, so some of our work is already up to date. config &lt;- drake_config(plan) vis_drake_graph(config) # see also outdated() and predict_runtime() make() only trains the outdated or missing models and refreshes the post-processing. It skips the targets that are already up to date. make(plan) #&gt; target model_64 #&gt; target conf_64 #&gt; target comparison 5.8 Inspect the results again readd(comparison) # see also loadd() Going forward, we can turn our attention to different tuning parameters and try to improve specificity. 5.9 Increasing efficiency Due to the technical details of drake’s storage system, the above workflow serializes each Keras model twice, which could prove inefficient for large models. If it takes a long time to save your models, you may wish to store them in their own special HDF5 files. This approach adds some cumbersome bookkeeping, but make() should run faster. We need to rewrite our functions in terms of save_model_hdf5() and load_model_hdf5(). # We add a new model_file argument. train_model &lt;- function(data, rec, batch_size, model_file) { model &lt;- define_model(rec) compile( model, optimizer = &quot;adam&quot;, loss = &quot;binary_crossentropy&quot;, metrics = c(&quot;accuracy&quot;) ) x_train_tbl &lt;- juice( rec, all_predictors(), composition = &quot;matrix&quot; ) y_train_vec &lt;- juice(rec, all_outcomes()) %&gt;% pull() history &lt;- fit( object = model, x = x_train_tbl, y = y_train_vec, batch_size = batch_size, epochs = 35, validation_split = 0.30, verbose = 0 ) # Instead of calling serialize_model(), we save the model to a file. save_model_hdf5(model, model_file) # As an added bonus, we can now return the history from the function. history } # Again, we need a model_file argument. confusion_matrix &lt;- function(data, rec, model_file) { # Instead of calling unserialize_model(), # we load the model from the HDF5 file. model &lt;- load_model_hdf5(model_file) testing_data &lt;- bake(rec, testing(data)) x_test_tbl &lt;- testing_data %&gt;% select(-Churn) %&gt;% as.matrix() y_test_vec &lt;- testing_data %&gt;% select(Churn) %&gt;% pull() yhat_keras_class_vec &lt;- model %&gt;% predict_classes(x_test_tbl) %&gt;% as.factor() %&gt;% fct_recode(yes = &quot;1&quot;, no = &quot;0&quot;) yhat_keras_prob_vec &lt;- model %&gt;% predict_proba(x_test_tbl) %&gt;% as.vector() test_truth &lt;- y_test_vec %&gt;% as.factor() %&gt;% fct_recode(yes = &quot;1&quot;, no = &quot;0&quot;) estimates_keras_tbl &lt;- tibble( truth = test_truth, estimate = yhat_keras_class_vec, class_prob = yhat_keras_prob_vec ) estimates_keras_tbl %&gt;% conf_mat(truth, estimate) } And we need a new plan that tracks the model files using file_in() and file_out(). batch_sizes &lt;- c(16, 32, 64) model_files &lt;- paste0(&quot;model_&quot;, batch_sizes, &quot;.h5&quot;) plan &lt;- drake_plan( data = read_csv(file_in(&quot;customer_churn.csv&quot;), col_types = cols()) %&gt;% initial_split(prop = 0.3), rec = prepare_recipe(data), history = target( train_model(data, rec, batch_size, file_out(model_file)), transform = map( batch_size = !!batch_sizes, model_file = !!model_files, .id = batch_size ) ), conf = target( confusion_matrix(data, rec, file_in(model_file)), transform = map(model_file, .id = batch_size) ), comparison = target( compare_models(conf), transform = combine(conf) ) ) drake still resolves the correct dependency relationships, but because of the changes to our functions and plan, our models and downstream results are no longer up to date. config &lt;- drake_config(plan) vis_drake_graph(config) Another advantage of this more complicated approach is that we can now view model histories. make(plan) #&gt; target history_16 #&gt; target history_32 #&gt; target history_64 #&gt; target conf_16 #&gt; target conf_32 #&gt; target conf_64 #&gt; target comparison plot(readd(history_16)) 5.10 Tips To see this workflow organized as a collection of modular scripts, see the customer churn examples in this repository. You can download the code with drake_example(&quot;customer-churn-simple&quot;) and drake_example(&quot;customer-churn-fast&quot;). drake has built-in distributed computing support that lets you fit multiple models in parallel. "],
["packages.html", "Chapter 6 An analysis of R package download trends 6.1 Get the code. 6.2 Overview 6.3 Analysis 6.4 Other ways to trigger downloads", " Chapter 6 An analysis of R package download trends This chapter explores R package download trends using the cranlogs package, and it shows how drake’s custom triggers can help with workflows with remote data sources. 6.1 Get the code. Write the code files to your workspace. drake_example(&quot;packages&quot;) The new packages folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 6.2 Overview This small data analysis project explores some trends in R package downloads over time. The datasets are downloaded using the cranlogs package. library(cranlogs) cran_downloads(packages = &quot;dplyr&quot;, when = &quot;last-week&quot;) #&gt; date count package #&gt; 1 2019-04-29 36369 dplyr #&gt; 2 2019-04-30 36647 dplyr #&gt; 3 2019-05-01 29981 dplyr #&gt; 4 2019-05-02 35954 dplyr #&gt; 5 2019-05-03 33442 dplyr #&gt; 6 2019-05-04 20183 dplyr #&gt; 7 2019-05-05 19492 dplyr Above, each count is the number of times dplyr was downloaded from the RStudio CRAN mirror on the given day. To stay up to date with the latest download statistics, we need to refresh the data frequently. With drake, we can bring all our work up to date without restarting everything from scratch. 6.3 Analysis First, we load the required packages. drake detects the packages you install and load. library(cranlogs) library(drake) library(dplyr) library(ggplot2) library(knitr) library(rvest) We will want custom functions to summarize the CRAN logs we download. make_my_table &lt;- function(downloads){ group_by(downloads, package) %&gt;% summarize(mean_downloads = mean(count)) } make_my_plot &lt;- function(downloads){ ggplot(downloads) + geom_line(aes(x = date, y = count, group = package, color = package)) } Next, we generate the plan. We want to explore the daily downloads from the knitr, Rcpp, and ggplot2 packages. We will use the cranlogs package to get daily logs of package downloads from RStudio’s CRAN mirror. In our drake_plan(), we declare targets older and recent to contain snapshots of the logs. The following drake_plan() syntax is described here, which is supported in drake 7.0.0 and above (and the current development version). plan &lt;- drake_plan( older = cran_downloads( packages = c(&quot;knitr&quot;, &quot;Rcpp&quot;, &quot;ggplot2&quot;), from = &quot;2016-11-01&quot;, to = &quot;2016-12-01&quot;, ), recent = target( command = cran_downloads( packages = c(&quot;knitr&quot;, &quot;Rcpp&quot;, &quot;ggplot2&quot;), when = &quot;last-month&quot;), trigger = trigger(change = latest_log_date()) ), averages = target( make_my_table(data), transform = map(data = c(older, recent)) ), plot = target( make_my_plot(data), transform = map(data) ), report = knit( knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE ) ) Notice the custom trigger for the target recent. Here, we are telling drake to rebuild recent whenever a new day’s log is uploaded to http://cran-logs.rstudio.com. In other words, drake keeps track of the return value of latest_log_date() and recomputes recent (during make()) if that value changed since the last make(). Here, latest_log_date() is one of our custom imported functions. We use it to scrape http://cran-logs.rstudio.com using the rvest package. latest_log_date &lt;- function(){ read_html(&quot;http://cran-logs.rstudio.com/&quot;) %&gt;% html_nodes(&quot;li:last-of-type&quot;) %&gt;% html_nodes(&quot;a:last-of-type&quot;) %&gt;% html_text() %&gt;% max } Now, we run the project to download the data and analyze it. The results will be summarized in the knitted report, report.md, but you can also read the results directly from the cache. make(plan) #&gt; target older #&gt; target recent #&gt; target averages_older #&gt; target plot_older #&gt; target averages_recent #&gt; target plot_recent #&gt; target report readd(averages_recent) #&gt; # A tibble: 3 x 2 #&gt; package mean_downloads #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ggplot2 26714. #&gt; 2 knitr 15987. #&gt; 3 Rcpp 28264 readd(averages_older) #&gt; # A tibble: 3 x 2 #&gt; package mean_downloads #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ggplot2 14641. #&gt; 2 knitr 9069. #&gt; 3 Rcpp 14408. readd(plot_recent) readd(plot_older) If we run make() again right away, we see that everything is up to date. But if we wait until a new day’s log is uploaded, make() will update recent and everything that depends on it. make(plan) #&gt; All targets are already up to date. To visualize the build behavior, you can plot the dependency network. config &lt;- drake_config(plan) vis_drake_graph(config) 6.4 Other ways to trigger downloads Sometimes, our remote data sources get revised, and web scraping may not be the best way to detect changes. We may want to look at our remote dataset’s modification time or HTTP ETag. To see how this works, consider the CRAN log file from February 9, 2018. url &lt;- &quot;http://cran-logs.rstudio.com/2018/2018-02-09-r.csv.gz&quot; We can track the modification date using the httr package. library(httr) # For querying websites. HEAD(url)$headers[[&quot;last-modified&quot;]] #&gt; [1] &quot;Mon, 12 Feb 2018 16:34:48 GMT&quot; In our drake plan, we can track this timestamp and trigger a download whenever it changes. plan &lt;- drake_plan( logs = target( get_logs(url), trigger = trigger(change = HEAD(url)$headers[[&quot;last-modified&quot;]]) ) ) plan #&gt; # A tibble: 1 x 3 #&gt; target command trigger #&gt; &lt;chr&gt; &lt;expr&gt; &lt;expr&gt; #&gt; 1 logs get_logs(url) trigger(change = HEAD(url)$headers[[&quot;last-modified&quot;… where library(R.utils) # For unzipping the files we download. library(curl) # For downloading data. get_logs &lt;- function(url){ curl_download(url, &quot;logs.csv.gz&quot;) # Get a big file. gunzip(&quot;logs.csv.gz&quot;, overwrite = TRUE) # Unzip it. out &lt;- read.csv(&quot;logs.csv&quot;, nrows = 4) # Extract the data you need. unlink(c(&quot;logs.csv.gz&quot;, &quot;logs.csv&quot;)) # Remove the big files out # Value of the target. } When we are ready, we run the workflow. make(plan) #&gt; target logs readd(logs) #&gt; date time size version os country ip_id #&gt; 1 2018-02-09 13:01:13 82375220 3.4.3 win RO 1 #&gt; 2 2018-02-09 13:02:06 74286541 3.3.3 win US 2 #&gt; 3 2018-02-09 13:02:10 82375216 3.4.3 win US 3 #&gt; 4 2018-02-09 13:03:30 82375220 3.4.3 win IS 4 If the log file at the url ever changes, the timestamp will update remotely, and make() will download the file again. "],
["gsp.html", "Chapter 7 Finding the best model of gross state product 7.1 Get the code. 7.2 Objective and methods 7.3 Data 7.4 Analysis 7.5 Results 7.6 Comparison with GNU Make 7.7 References", " Chapter 7 Finding the best model of gross state product The following data analysis workflow shows off drake’s ability to generate lots of reproducibly-tracked tasks with ease. The same technique would be cumbersome, even intractable, with GNU Make. 7.1 Get the code. Write the code files to your workspace. drake_example(&quot;gsp&quot;) The new gsp folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 7.2 Objective and methods The goal is to search for factors closely associated with the productivity of states in the USA around the 1970s and 1980s. For the sake of simplicity, we use gross state product as a metric of productivity, and we restrict ourselves to multiple linear regression models with three variables. For each of the 84 possible models, we fit the data and then evaluate the root mean squared prediction error (RMSPE). \\[ \\begin{aligned} \\text{RMSPE} = \\sqrt{(\\text{y} - \\widehat{y})^T(y - \\widehat{y})} \\end{aligned} \\] Here, \\(y\\) is the vector of observed gross state products in the data, and \\(\\widehat{y}\\) is the vector of predicted gross state products under one of the models. We take the best variables to be the triplet in the model with the lowest RMSPE. 7.3 Data The Produc dataset from the Ecdat package contains data on the Gross State Product from 1970 to 1986. Each row is a single observation on a single state for a single year. The dataset has the following variables as columns. See the references later in this report for more details. gsp: gross state product. state: the state. year: the year. pcap: private capital stock. hwy: highway and streets. water: water and sewer facilities. util: other public buildings and structures. pc: public capital. emp: labor input measured by the employment in non-agricultural payrolls. unemp: state unemployment rate. library(Ecdat) data(Produc) head(Produc) #&gt; state year pcap hwy water util pc gsp emp #&gt; 1 ALABAMA 1970 15032.67 7325.80 1655.68 6051.20 35793.80 28418 1010.5 #&gt; 2 ALABAMA 1971 15501.94 7525.94 1721.02 6254.98 37299.91 29375 1021.9 #&gt; 3 ALABAMA 1972 15972.41 7765.42 1764.75 6442.23 38670.30 31303 1072.3 #&gt; 4 ALABAMA 1973 16406.26 7907.66 1742.41 6756.19 40084.01 33430 1135.5 #&gt; 5 ALABAMA 1974 16762.67 8025.52 1734.85 7002.29 42057.31 33749 1169.8 #&gt; 6 ALABAMA 1975 17316.26 8158.23 1752.27 7405.76 43971.71 33604 1155.4 #&gt; unemp #&gt; 1 4.7 #&gt; 2 5.2 #&gt; 3 4.7 #&gt; 4 3.9 #&gt; 5 5.5 #&gt; 6 7.7 7.4 Analysis First, we load the required packages. drake is aware of all the packages you load with library() or require(). library(biglm) # lightweight models, easier to store than with lm() library(drake) library(Ecdat) # econometrics datasets library(ggplot2) library(knitr) library(purrr) library(tidyverse) Next, we construct our plan. The following code uses drake’s special new language for generating plans (learn more here). predictors &lt;- setdiff(colnames(Produc), &quot;gsp&quot;) # We will try all combinations of three covariates. combos &lt;- combn(predictors, 3) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) %&gt;% setNames(c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)) head(combos) #&gt; x1 x2 x3 #&gt; 1 state year pcap #&gt; 2 state year hwy #&gt; 3 state year water #&gt; 4 state year util #&gt; 5 state year pc #&gt; 6 state year emp # We need to list each covariate as a symbol. for (col in colnames(combos)) { combos[[col]] &lt;- rlang::syms(combos[[col]]) } # Requires drake &gt;= 7.0.0 or the development version # at github.com/ropensci/drake. # Install with remotes::install_github(&quot;ropensci/drake&quot;). plan &lt;- drake_plan( model = target( biglm(gsp ~ x1 + x2 + x3, data = Ecdat::Produc), transform = map(.data = !!combos) # Remember the bang-bang!! ), rmspe_i = target( get_rmspe(model, Ecdat::Produc), transform = map(model) ), rmspe = target( bind_rows(rmspe_i, .id = &quot;model&quot;), transform = combine(rmspe_i) ), plot = ggsave( filename = file_out(&quot;rmspe.pdf&quot;), plot = plot_rmspe(rmspe), width = 8, height = 8 ), report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) ) plan #&gt; # A tibble: 171 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 model_state_year_pcap biglm(gsp ~ state + year + pcap, data = Ecdat::Pr… #&gt; 2 model_state_year_hwy biglm(gsp ~ state + year + hwy, data = Ecdat::Pro… #&gt; 3 model_state_year_wat… biglm(gsp ~ state + year + water, data = Ecdat::P… #&gt; 4 model_state_year_util biglm(gsp ~ state + year + util, data = Ecdat::Pr… #&gt; 5 model_state_year_pc biglm(gsp ~ state + year + pc, data = Ecdat::Prod… #&gt; 6 model_state_year_emp biglm(gsp ~ state + year + emp, data = Ecdat::Pro… #&gt; 7 model_state_year_une… biglm(gsp ~ state + year + unemp, data = Ecdat::P… #&gt; 8 model_state_pcap_hwy biglm(gsp ~ state + pcap + hwy, data = Ecdat::Pro… #&gt; 9 model_state_pcap_wat… biglm(gsp ~ state + pcap + water, data = Ecdat::P… #&gt; 10 model_state_pcap_util biglm(gsp ~ state + pcap + util, data = Ecdat::Pr… #&gt; # … with 161 more rows We also need to define functions for summaries and plots. get_rmspe &lt;- function(model_fit, data){ y &lt;- data$gsp yhat &lt;- as.numeric(predict(model_fit, newdata = data)) terms &lt;- attr(model_fit$terms, &quot;term.labels&quot;) tibble( rmspe = sqrt(mean((y - yhat)^2)), # nolint X1 = terms[1], X2 = terms[2], X3 = terms[3] ) } plot_rmspe &lt;- function(rmspe){ ggplot(rmspe) + geom_histogram(aes(x = rmspe), bins = 15) } We have a report.Rmd file to summarize our results at the end. drake_example(&quot;gsp&quot;) file.copy(from = &quot;gsp/report.Rmd&quot;, to = &quot;.&quot;, overwrite = TRUE) #&gt; [1] TRUE We can inspect the project before we run it. config &lt;- drake_config(plan) vis_drake_graph(config) Now, we can run the project. make(plan, verbose = 0L) 7.5 Results Here are the root mean squared prediction errors of all the models. results &lt;- readd(rmspe) library(ggplot2) plot_rmspe(rmspe = results) And here are the best models. The best variables are in the top row under X1, X2, and X3. head(results[order(results$rmspe, decreasing = FALSE), ]) #&gt; # A tibble: 6 x 5 #&gt; model rmspe X1 X2 X3 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 17 2614. state hwy emp #&gt; 2 21 2665. state water emp #&gt; 3 24 2666. state util emp #&gt; 4 26 2666. state pc emp #&gt; 5 12 2675. state pcap emp #&gt; 6 28 2693. state emp unemp 7.6 Comparison with GNU Make If we were using Make instead of drake with the same set of targets, the analogous Makefile would look something like this pseudo-code sketch. models = model_state_year_pcap.rds model_state_year_hwy.rds ... # 84 of these model_% Rscript -e 'saveRDS(lm(...), ...)' rmspe_%: model_% Rscript -e 'saveRDS(get_rmspe(...), ...)' rmspe.rds: rmspe_% Rscript -e 'saveRDS(dplyr::bind_rows(...), ...)' rmspe.pdf: rmspe.rds Rscript -e 'ggplot2::ggsave(plot_rmspe(readRDS(\"rmspe.rds\")), \"rmspe.pdf\")' report.md: report.Rmd Rscript -e 'knitr::knit(\"report.Rmd\")' There are three main disadvantages to this approach. Every target requires a new call to Rscript, which means that more time is spent initializing R sessions than doing the actual work. The user must micromanage nearly one hundred output files (in this case, *.rds files), which is cumbersome, messy, and inconvenient. drake, on the other hand, automatically manages storage using a storr cache. The user needs to write the names of the 84 models near the top of the Makefile, which is less convenient than maintaining a data frame in R. 7.7 References Baltagi, Badi H (2003). Econometric analysis of panel data, John Wiley and sons, http://www.wiley.com/legacy/wileychi/baltagi/. Baltagi, B. H. and N. Pinnoi (1995). “Public capital stock and state productivity growth: further evidence”, Empirical Economics, 20, 351-359. Munnell, A. (1990). “Why has productivity growth declined? Productivity and public investment”&quot;, New England Economic Review, 3-22. Yves Croissant (2016). Ecdat: Data Sets for Econometrics. R package version 0.3-1. https://CRAN.R-project.org/package=Ecdat. "],
["visuals.html", "Chapter 8 Visualization with drake 8.1 Underlying graph data: node and edge data frames 8.2 Visualizing target status 8.3 Subgraphs 8.4 Control the vis_drake_graph() legend. 8.5 Clusters 8.6 Output files", " Chapter 8 Visualization with drake Data analysis projects have complicated networks of dependencies, and drake can help you visualize them with vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() (note the two g’s). 8.0.1 vis_drake_graph() Powered by visNetwork. Colors represent target status, and shapes represent data type. These graphs are interactive, so you can click, drag, zoom, and and pan to adjust the size and position. Double-click on nodes to contract neighborhoods into clusters or expand them back out again. If you hover over a node, you will see text in a tooltip showing the first few lines of The command of a target, or The body of an imported function, or The content of an imported text file. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). config &lt;- drake_config(my_plan) vis_drake_graph(config) To save this interactive widget for later, just supply the name of an HTML file. vis_drake_graph(config, file = &quot;graph.html&quot;) To save a static image file, supply a file name that ends in &quot;.png&quot;, &quot;.pdf&quot;, &quot;.jpeg&quot;, or &quot;.jpg&quot;. vis_drake_graph(config, file = &quot;graph.png&quot;) 8.0.2 sankey_drake_graph() These interactive networkD3 Sankey diagrams have more nuance: the height of each node is proportional to its number of connections. Nodes with many incoming connnections tend to fall out of date more often, and nodes with many outgoing connections can invalidate bigger chunks of the downstream pipeline. sankey_drake_graph(config) Saving the graphs is the same as before. sankey_drake_graph(config, file = &quot;graph.html&quot;) # Interactive HTML widget sankey_drake_graph(config, file = &quot;graph.png&quot;) # Static image file Unfortunately, a legend is not yet available for Sankey diagrams, but drake exposes a separate legend for the colors and shapes. library(visNetwork) legend_nodes() #&gt; # A tibble: 10 x 6 #&gt; label color shape font.color font.size id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Up to date #228B22 dot black 20 1 #&gt; 2 Outdated #000000 dot black 20 2 #&gt; 3 Running #FF7221 dot black 20 3 #&gt; 4 Failed #AA0000 dot black 20 4 #&gt; 5 Imported #1874CD dot black 20 5 #&gt; 6 Missing #9A32CD dot black 20 6 #&gt; 7 Object #888888 dot black 20 7 #&gt; 8 Function #888888 triangle black 20 8 #&gt; 9 File #888888 square black 20 9 #&gt; 10 Cluster #888888 diamond black 20 10 visNetwork(nodes = legend_nodes()) 8.0.3 drake_ggraph() drake_ggraph() can handle larger workflows than the other graphing functions. If your project has thousands of targets and vis_drake_graph()/sankey_drake_graph() does not render properly, consider drake_ggraph(). Powered by ggraph, drake_ggraph()s are static ggplot2 objects, and you can save them with ggsave(). drake_ggraph(config) 8.0.4 text_drake_graph() If you are running R in a terminal without X Window support, the usual visualizations will show up interactively in your session. Here, you can use text_drake_graph() to see a text display in your terminal window. Terminal colors are deactivated in this manual, but you will see color in your console. # Use nchar = 0 or nchar = 1 for better results. # The color display is better in your own terminal. text_drake_graph(config, nchar = 3) #&gt; fil sum #&gt; reg coe #&gt; #&gt; reg sma coe #&gt; reg #&gt; sum #&gt; reg sim coe rep fil #&gt; reg #&gt; dat lar sum #&gt; #&gt; reg coe #&gt; ran sum 8.1 Underlying graph data: node and edge data frames drake_graph_info() is used behind the scenes in vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() to get the graph information ready for rendering. To save time, you can call drake_graph_info() to get these internals and then call render_drake_graph(), render_sankey_drake_graph(), or render_drake_ggraph(). str(drake_graph_info(config)) #&gt; List of 4 #&gt; $ nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 22 obs. of 12 variables: #&gt; ..$ id : chr [1:22] &quot;random_rows&quot; &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; &quot;reg1&quot; &quot;reg2&quot; ... #&gt; ..$ imported : logi [1:22] TRUE TRUE TRUE TRUE TRUE TRUE ... #&gt; ..$ label : chr [1:22] &quot;random_rows&quot; &quot;datasets::mtcars&quot; &quot;reg1&quot; &quot;reg2&quot; ... #&gt; ..$ status : chr [1:22] &quot;imported&quot; &quot;imported&quot; &quot;imported&quot; &quot;imported&quot; ... #&gt; ..$ type : chr [1:22] &quot;function&quot; &quot;object&quot; &quot;function&quot; &quot;function&quot; ... #&gt; ..$ font.size: num [1:22] 20 20 20 20 20 20 20 20 20 20 ... #&gt; ..$ color : chr [1:22] &quot;#1874CD&quot; &quot;#1874CD&quot; &quot;#1874CD&quot; &quot;#1874CD&quot; ... #&gt; ..$ shape : chr [1:22] &quot;triangle&quot; &quot;dot&quot; &quot;triangle&quot; &quot;triangle&quot; ... #&gt; ..$ level : num [1:22] 1 1 1 1 1 2 3 3 4 4 ... #&gt; ..$ title : chr [1:22] &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; ... #&gt; ..$ x : num [1:22] -1 -1 -1 -1 -1 ... #&gt; ..$ y : num [1:22] -8.57e-01 -4.29e-01 2.22e-16 4.29e-01 8.57e-01 ... #&gt; $ edges :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 25 obs. of 3 variables: #&gt; ..$ from : chr [1:25] &quot;random_rows&quot; &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; &quot;reg1&quot; &quot;reg1&quot; ... #&gt; ..$ to : chr [1:25] &quot;simulate&quot; &quot;simulate&quot; &quot;regression1_small&quot; &quot;regression1_large&quot; ... #&gt; ..$ arrows: chr [1:25] &quot;to&quot; &quot;to&quot; &quot;to&quot; &quot;to&quot; ... #&gt; $ legend_nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 5 obs. of 6 variables: #&gt; ..$ label : chr [1:5] &quot;Outdated&quot; &quot;Imported&quot; &quot;Object&quot; &quot;Function&quot; ... #&gt; ..$ color : chr [1:5] &quot;#000000&quot; &quot;#1874CD&quot; &quot;#888888&quot; &quot;#888888&quot; ... #&gt; ..$ shape : chr [1:5] &quot;dot&quot; &quot;dot&quot; &quot;dot&quot; &quot;triangle&quot; ... #&gt; ..$ font.color: chr [1:5] &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ... #&gt; ..$ font.size : num [1:5] 20 20 20 20 20 #&gt; ..$ id : int [1:5] 2 5 7 8 9 #&gt; $ default_title: chr &quot;Dependency graph&quot; 8.2 Visualizing target status drake’s visuals tell you which targets are up to date and which are outdated. make(my_plan, verbose = 0L) config &lt;- drake_config(my_plan, jobs = 2, verbose = 0L) #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 outdated(config) #&gt; character(0) sankey_drake_graph(config) When you change a dependency, some targets fall out of date (black nodes). reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } sankey_drake_graph(config) 8.3 Subgraphs Graphs can grow enormous for serious projects, so there are multiple ways to focus on a manageable subgraph. The most brute-force way is to just pick a manual subset of nodes. However, with the subset argument, the graphing functions can drop intermediate nodes and edges. vis_drake_graph( config, subset = c(&quot;regression2_small&quot;, &quot;large&quot;) ) The rest of the subgraph functionality preserves connectedness. Use targets_only to ignore the imports. vis_drake_graph(config, targets_only = TRUE) Similarly, you can just show downstream nodes. vis_drake_graph(config, from = c(&quot;regression2_small&quot;, &quot;regression2_large&quot;)) Or upstream ones. vis_drake_graph(config, from = &quot;small&quot;, mode = &quot;in&quot;) In fact, let us just take a small neighborhood around a target in both directions. For the graph below, given order is 1, but all the custom file_out() output files of the neighborhood’s targets appear as well. This ensures consistent behavior between show_output_files = TRUE and show_output_files = FALSE (more on that later). vis_drake_graph(config, from = &quot;small&quot;, mode = &quot;all&quot;, order = 1) 8.4 Control the vis_drake_graph() legend. Some arguments to vis_drake_graph() control the legend. vis_drake_graph(config, full_legend = TRUE, ncol_legend = 2) To remove the legend altogether, set the ncol_legend argument to 0. vis_drake_graph(config, ncol_legend = 0) 8.5 Clusters With the group and clusters arguments to the graphing functions, you can condense nodes into clusters. This is handy for workflows with lots of targets. Take the schools scenario from the drake plan guide. Our plan was generated with drake_plan(trace = TRUE), so it has wildcard columns that group nodes into natural clusters already. You can manually add such columns if you wish. # Visit https://ropenscilabs.github.io/drake-manual/plans.html#create-large-plans-the-easy-way # to learn about the syntax with target(transform = ...). plan &lt;- drake_plan( school = target( get_school_data(id), transform = map(id = c(1, 2, 3)) ), credits = target( fun(school), transform = cross( school, fun = c(check_credit_hours, check_students, check_graduations) ) ), public_funds_school = target( command = check_public_funding(school), transform = map(school = c(school_1, school_2)) ), trace = TRUE ) plan #&gt; # A tibble: 14 x 7 #&gt; target command id school public_funds_sch… fun credits #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 school_1 get_school… 1 school… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 school_2 get_school… 2 school… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 school_3 get_school… 3 school… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 credits_c… check_cred… 1 school… &lt;NA&gt; check… credits_c… #&gt; 5 credits_c… check_stud… 1 school… &lt;NA&gt; check… credits_c… #&gt; 6 credits_c… check_grad… 1 school… &lt;NA&gt; check… credits_c… #&gt; 7 credits_c… check_cred… 2 school… &lt;NA&gt; check… credits_c… #&gt; 8 credits_c… check_stud… 2 school… &lt;NA&gt; check… credits_c… #&gt; 9 credits_c… check_grad… 2 school… &lt;NA&gt; check… credits_c… #&gt; 10 credits_c… check_cred… 3 school… &lt;NA&gt; check… credits_c… #&gt; 11 credits_c… check_stud… 3 school… &lt;NA&gt; check… credits_c… #&gt; 12 credits_c… check_grad… 3 school… &lt;NA&gt; check… credits_c… #&gt; 13 public_fu… check_publ… &lt;NA&gt; school… public_funds_sch… &lt;NA&gt; &lt;NA&gt; #&gt; 14 public_fu… check_publ… &lt;NA&gt; school… public_funds_sch… &lt;NA&gt; &lt;NA&gt; Ordinarily, the workflow graph gives a separate node to each individual import object or target. config &lt;- drake_config(plan) vis_drake_graph(config) For large projects with hundreds of nodes, this can get quite cumbersome. But here, we can choose a wildcard column (or any other column in the plan, even custom columns) to condense nodes into natural clusters. For the group argument to the graphing functions, choose the name of a column in plan or a column you know will be in drake_graph_info(config)$nodes. Then for clusters, choose the values in your group column that correspond to nodes you want to bunch together. The new graph is not as cumbersome. config &lt;- drake_config(plan) vis_drake_graph(config, group = &quot;school&quot;, clusters = c(&quot;school_1&quot;, &quot;school_2&quot;, &quot;school_3&quot;) ) As I mentioned, you can group on any column in drake_graph_info(config)$nodes. Let’s return to the mtcars project for demonstration. config &lt;- drake_config(my_plan) vis_drake_graph(config) Let’s condense all the imports into one node and all the up-to-date targets into another. That way, the outdated targets stand out. vis_drake_graph( config, group = &quot;status&quot;, clusters = c(&quot;imported&quot;, &quot;up to date&quot;) ) 8.6 Output files drake can reproducibly track multiple output files per target and show them in the graph. plan &lt;- drake_plan( target1 = { file.copy(file_in(&quot;in1.txt&quot;), file_out(&quot;out1.txt&quot;)) file.copy(file_in(&quot;in2.txt&quot;), file_out(&quot;out2.txt&quot;)) }, target2 = { file.copy(file_in(&quot;out1.txt&quot;), file_out(&quot;out3.txt&quot;)) file.copy(file_in(&quot;out2.txt&quot;), file_out(&quot;out4.txt&quot;)) } ) writeLines(&quot;in1&quot;, &quot;in1.txt&quot;) writeLines(&quot;in2&quot;, &quot;in2.txt&quot;) make(plan) #&gt; target target1 #&gt; target target2 config &lt;- drake_config(plan) writeLines(&quot;abcdefg&quot;, &quot;out3.txt&quot;) vis_drake_graph(config, targets_only = TRUE) If your graph is too busy, you can hide the output files with show_output_files = FALSE. vis_drake_graph(config, show_output_files = FALSE, targets_only = TRUE) "],
["debug.html", "Chapter 9 Debugging and testing drake projects 9.1 Dependencies 9.2 Diagnose failures. 9.3 Timeouts and retries 9.4 More help", " Chapter 9 Debugging and testing drake projects This chapter is a guide to debugging and testing drake projects. 9.0.1 Start small If your workflow is large, consider running a downsized version to debug and test things first. That way, you can find the bugs early on without wasting as much time. Create a test plan with drake_plan(max_expand = SMALL_NUMBER) before scaling up to the full set of targets. See this section on plans for details. 9.1 Dependencies drake automatically detects dependency relationships among your targets and imports. While this is convenient most of the time, it can lead to some pitfalls. This section describes techniques to understand you project’s dependency structure and diagnose and debug issues. 9.1.1 Visualize your dependency graph. To avoid frustration early on, please use drake’s dependency graph visualizations to see how the steps of your workflow fit together. drake resolves the dependency relationships in the graph by analyzing the code in your commands and the functions in your environment. load_mtcars_example() config &lt;- drake_config(my_plan) # Hover, click, drag, zoom, and pan. See args &#39;from&#39; and &#39;to&#39;. vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) 9.1.2 Check specific dependency information. With the deps_code() function, you can see for yourself how drake detects first-order dependencies from code. print(simulate) #&gt; function (n) #&gt; { #&gt; data &lt;- random_rows(data = datasets::mtcars, n = n) #&gt; data.frame(x = data$wt, y = data$mpg) #&gt; } deps_code(simulate) #&gt; # A tibble: 5 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 data.frame globals #&gt; 2 mpg globals #&gt; 3 wt globals #&gt; 4 random_rows globals #&gt; 5 datasets::mtcars namespaced # knitr_in() makes sure your target depends on `report.Rmd` # and any dependencies loaded with loadd() and readd() # in the report&#39;s active code chunks. my_plan$command[[1]] #&gt; knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) deps_code(my_plan$command[[1]]) #&gt; # A tibble: 6 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 knit globals #&gt; 2 large loadd #&gt; 3 small readd #&gt; 4 coef_regression2_small readd #&gt; 5 report.md file_out #&gt; 6 report.Rmd knitr_in my_plan$command[[nrow(my_plan)]] #&gt; suppressWarnings(summary(regression2_large))$coefficients deps_code(my_plan$command[[nrow(my_plan)]]) #&gt; # A tibble: 4 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 summary globals #&gt; 2 suppressWarnings globals #&gt; 3 regression2_large globals #&gt; 4 coefficients globals With deps_target(), you can see the dependencies that drake has already detected for your targets and imports. deps_target(&quot;simulate&quot;, config) #&gt; # A tibble: 2 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 random_rows globals #&gt; 2 datasets::mtcars namespaced deps_target(&quot;small&quot;, config) #&gt; # A tibble: 1 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 simulate globals deps_target(&quot;report&quot;, config) #&gt; # A tibble: 5 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 large loadd #&gt; 2 small readd #&gt; 3 coef_regression2_small readd #&gt; 4 report.md file_out #&gt; 5 report.Rmd knitr_in And with tracked(), you can list all the reproducibly tracked objects and files. tracked(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;datasets::mtcars&quot; #&gt; [7] &quot;file report.Rmd&quot; &quot;file report.md&quot; #&gt; [9] &quot;random_rows&quot; &quot;reg1&quot; #&gt; [11] &quot;reg2&quot; &quot;regression1_large&quot; #&gt; [13] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [15] &quot;regression2_small&quot; &quot;report&quot; #&gt; [17] &quot;simulate&quot; &quot;small&quot; #&gt; [19] &quot;summ_regression1_large&quot; &quot;summ_regression1_small&quot; #&gt; [21] &quot;summ_regression2_large&quot; &quot;summ_regression2_small&quot; 9.1.3 Outdated targets and missing dependencies missed() shows any imports missing from your environment missed(config) # Nothing is missing right now. #&gt; character(0) outdated() reports any targets that are outdated. outdated(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; #&gt; [7] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [9] &quot;regression2_small&quot; &quot;report&quot; #&gt; [11] &quot;small&quot; &quot;summ_regression1_large&quot; #&gt; [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; #&gt; [15] &quot;summ_regression2_small&quot; make(my_plan) #&gt; target large #&gt; target small #&gt; target regression1_large #&gt; target regression2_large #&gt; target regression1_small #&gt; target regression2_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large #&gt; target summ_regression2_large #&gt; target coef_regression2_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target report outdated(config) #&gt; character(0) 9.1.4 But why are my targets out of date? drake has the option to produce a cache log with the fingerprint of every target and import. drake_cache_log() #&gt; # A tibble: 22 x 3 #&gt; hash type name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 ecb4739b594986b9 target coef_regression1_large #&gt; 2 599bea898e306775 target coef_regression1_small #&gt; 3 44bfee4c258a76d7 target coef_regression2_large #&gt; 4 b5502b9c62933484 target coef_regression2_small #&gt; 5 d87293dd4d012762 target large #&gt; 6 f4b89e63bc92af79 import datasets::mtcars #&gt; 7 7a456ee58df699be import file report.Rmd #&gt; 8 6153acbb3f1070de target file report.md #&gt; 9 db84c9f752635a13 import random_rows #&gt; 10 21935c86f12692e2 import reg1 #&gt; # … with 12 more rows filter(drake_cache_log(), type == &quot;import&quot;) #&gt; # A tibble: 6 x 3 #&gt; hash type name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 f4b89e63bc92af79 import datasets::mtcars #&gt; 2 7a456ee58df699be import file report.Rmd #&gt; 3 db84c9f752635a13 import random_rows #&gt; 4 21935c86f12692e2 import reg1 #&gt; 5 69ade4b78f15a3f9 import reg2 #&gt; 6 f4d57e5ba38b744b import simulate We highly recommend that you automatically produce a cache log file on every make() and put it under version control with the rest of your project. make(my_plan, cache_log_file = &quot;cache_log.csv&quot;) #&gt; All targets are already up to date. Suppose we go back and add input checking to one of our functions. print(random_rows) #&gt; function (data, n) #&gt; { #&gt; data[sample.int(n = nrow(data), size = n, replace = TRUE), #&gt; ] #&gt; } #&gt; &lt;bytecode: 0x160819d0&gt; random_rows &lt;- function(data, n){ stopifnot(n &gt; 0) data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } Then, we forget to run make() again, and we leave the the project for several months. When we come back, all our targets are suddenly out of date. outdated(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; #&gt; [7] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [9] &quot;regression2_small&quot; &quot;report&quot; #&gt; [11] &quot;small&quot; &quot;summ_regression1_large&quot; #&gt; [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; #&gt; [15] &quot;summ_regression2_small&quot; At first, we may not know why all our targets are outdated. But we can generate another cache log and check any hashes that changed. Our call to outdated() already re-cached the imports, so any changed imports will show up in the new cache log. read_csv(&quot;cache_log.csv&quot;, col_types = cols()) %&gt;% left_join(drake_cache_log(), by = &quot;name&quot;) %&gt;% filter(hash.x != hash.y) %&gt;% select(name, hash.x, hash.y, -type.x, -type.y) #&gt; # A tibble: 2 x 3 #&gt; name hash.x hash.y #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 random_rows db84c9f752635a13 089a5c1023c1c51a #&gt; 2 simulate f4d57e5ba38b744b 739a28336ceb4da2 Now, we see that random_rows() has changed since last time, and we have a new dependency stopifnot(). simulate() shows up in the changes too because random_rows() is nested in the body of simulate(). If we revert random_rows() to its original state, all our targets are up to date again. random_rows &lt;- function(data, n){ data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } print(outdated(config)) #&gt; character(0) read_csv(&quot;cache_log.csv&quot;, col_types = cols()) %&gt;% left_join(drake_cache_log(), by = &quot;name&quot;) %&gt;% filter(hash.x != hash.y) %&gt;% select(name, hash.x, hash.y, -type.x, -type.y) #&gt; # A tibble: 0 x 3 #&gt; # … with 3 variables: name &lt;chr&gt;, hash.x &lt;chr&gt;, hash.y &lt;chr&gt; 9.2 Diagnose failures. drake records diagnostic metadata on all your targets, including the latest errors, warnings, messages, and other bits of context. f &lt;- function(x){ if (x &lt; 0){ stop(&quot;`x` cannot be negative.&quot;) } x } bad_plan &lt;- drake_plan( a = 12, b = -a, my_target = f(b) ) bad_plan #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 a 12 #&gt; 2 b -a #&gt; 3 my_target f(b) withr::with_message_sink( new = stdout(), make(bad_plan) ) #&gt; target a #&gt; target b #&gt; target my_target #&gt; fail my_target #&gt; Error: Target `my_target` failed. Call `diagnose(my_target)` for details. Error message: #&gt; `x` cannot be negative. failed(verbose = 0L) # from the last make() only #&gt; [1] &quot;my_target&quot; # See also warnings and messages. error &lt;- diagnose(my_target, verbose = 0L)$error error$message #&gt; [1] &quot;`x` cannot be negative.&quot; error$call #&gt; f(b) str(error$calls) # View the traceback. #&gt; List of 8 #&gt; $ : language local({ f(b) ... #&gt; $ : language eval.parent(substitute(eval(quote(expr), envir))) #&gt; $ : language eval(expr, p) #&gt; $ : language eval(expr, p) #&gt; $ : language eval(quote({ f(b) ... #&gt; $ : language eval(quote({ f(b) ... #&gt; $ : language f(b) #&gt; $ : language stop(&quot;`x` cannot be negative.&quot;) #&gt; ..- attr(*, &quot;srcref&quot;)= &#39;srcref&#39; int [1:8] 3 5 3 35 5 35 3 3 #&gt; .. ..- attr(*, &quot;srcfile&quot;)=Classes &#39;srcfilecopy&#39;, &#39;srcfile&#39; &lt;environment: 0x85a2970&gt; To figure out what went wrong, you could try to build the failed target interactively. To do that, simply call drake_build() or drake_debug(). These functions first call loadd(deps = TRUE) to load any missing dependencies (see the replace argument here) and then build your target. drake_build() simply runs the command, and drake_debug() runs the command in debug mode using debugonce(). # Pretend we just opened a new R session. library(drake) # Unloads target `b`. config &lt;- drake_config(plan = bad_plan) # my_target depends on b. &quot;b&quot; %in% ls() #&gt; [1] FALSE # Try to build my_target until the error is fixed. # Skip all that pesky work checking dependencies. drake_build(my_target, config = config) # See also drake_debug(). #&gt; target my_target #&gt; fail my_target #&gt; Error: Target `my_target` failed. Call `diagnose(my_target)` for details. Error message: #&gt; `x` cannot be negative. # The target failed, but the dependency was loaded. &quot;b&quot; %in% ls() #&gt; [1] FALSE # What was `b` again? b #&gt; Error in eval(expr, envir, enclos): object &#39;b&#39; not found # How was `b` used? diagnose(my_target)$message #&gt; NULL diagnose(my_target)$call #&gt; NULL f #&gt; function(x){ #&gt; if (x &lt; 0){ #&gt; stop(&quot;`x` cannot be negative.&quot;) #&gt; } #&gt; x #&gt; } #&gt; &lt;bytecode: 0x2399c888&gt; # Aha! The error was in f(). Let&#39;s fix it and try again. f &lt;- function(x){ x &lt;- abs(x) if (x &lt; 0){ stop(&quot;`x` cannot be negative.&quot;) } x } # Now it works! # Since you called make() previously, `config` is read from the cache # if you do not supply it. drake_build(my_target, config) # See also drake_debug(). #&gt; target my_target readd(my_target) #&gt; [1] 12 9.3 Timeouts and retries See the elapsed, cpu, and retries argument to make(). clean(verbose = 0L) f &lt;- function(...){ Sys.sleep(1) } debug_plan &lt;- drake_plan(x = 1, y = f(x)) debug_plan #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x 1 #&gt; 2 y f(x) withr::with_message_sink( stdout(), make(debug_plan, elapsed = 1e-3, retries = 2) ) #&gt; target x #&gt; target y #&gt; retry y 1 of 2 To tailor these settings to each individual target, create new elapsed, cpu, or retries columns in your drake plan. These columns override the analogous arguments to make(). clean(verbose = 0L) debug_plan$elapsed &lt;- c(1e-3, 2e-3) debug_plan$retries &lt;- 1:2 debug_plan #&gt; # A tibble: 2 x 4 #&gt; target command elapsed retries #&gt; &lt;chr&gt; &lt;expr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 x 1 0.001 1 #&gt; 2 y f(x) 0.002 2 withr::with_message_sink( new = stdout(), make(debug_plan, elapsed = Inf, retries = 0) ) #&gt; target x #&gt; target y #&gt; retry y 1 of 2 #&gt; retry y 2 of 2 #&gt; fail y #&gt; Error: Target `y` failed. Call `diagnose(y)` for details. Error message: #&gt; reached elapsed time limit 9.4 More help Please also see the compendium of cautionary notes, which addresses drake’s known edge cases, pitfalls, and weaknesses that may or may not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the issue tracker, where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version. "],
["hpc.html", "Chapter 10 High-performance computing 10.1 Start small 10.2 Batch mode for long workflows 10.3 Let make() schedule your targets. 10.4 Parallel backends 10.5 The clustermq backend 10.6 The future backend 10.7 Advanced options", " Chapter 10 High-performance computing This chapter provides guidance on time-consuming drake workflows and high-level parallel computation. library(drake) load_mtcars_example() make(my_plan, jobs = 2) 10.1 Start small If your workflow is large, consider running a downsized version to debug and test things first. That way, you can avoid consuming lots of computing resources until you are reasonably sure everything works. Create a test plan with drake_plan(max_expand = SMALL_NUMBER) before scaling up to the full set of targets. See this section on plans for details. 10.2 Batch mode for long workflows If you expect make() to take a long time, create a master script for your project (say, make.R, as in the guidance on projects) and run it in a persistent background process. The following should work in the Mac/Linux terminal/shell. nohup nice -19 R CMD BATCH --no-save make.R & where: nohup: Keep the job running even if you log out of the machine. nice -19: This is a low-priority job that should not consume many resources. Other processes should take priority. R CMD BATCH: Run the R script in a fresh new R session. --no-save: do not save the workspace in a .RData file. &amp;: Run this job in the background so you can do other stuff in the terminal window. 10.3 Let make() schedule your targets. drake uses your project’s implicit dependency graph to figure out which targets can run in parallel and which ones need to wait for dependencies. load_mtcars_example() config &lt;- drake_config(my_plan) vis_drake_graph(config) You do not need to not micromanage the timing among targets, and you do not need to run parallel instances of make(). As the next sections describe, drake has built-in parallel and distributed computing support. 10.4 Parallel backends Choose the parallel backend with the parallelism argument and set the jobs argument to scale the work appropriately. make(my_plan, parallelism = &quot;future&quot;, jobs = 2) The two primary backends with long term support are clustermq and future. If you can install ZeroMQ, the best choice is usually clustermq. (It is faster than future.) However, future is more accessible: it does not require ZeroMQ, it supports parallel computing on Windows, it can work with more restrictive wall time limits on clusters, and it can deploy targets to Docker images (drake_example(&quot;Docker-psock&quot;)). 10.5 The clustermq backend 10.5.1 Persistent workers The make(parallelism = &quot;clustermq&quot;, jobs = 2) launches 2 parallel persistent workers. The master process assigns targets to workers, and the workers simultaneously traverse the dependency graph. 10.5.2 Installation Persistent workers require the clustermq R package, which in turn requires ZeroMQ. Please refer to the clustermq installation guide for specific instructions. 10.5.3 On your local machine To run your targets in parallel over the cores of your local machine, set the global option below and run make(). options(clustermq.scheduler = &quot;multicore&quot;) make(plan, parallelism = &quot;clustermq&quot;, jobs = 2) 10.5.4 On a cluster Set the clustermq global options to register your computing resources. For SLURM: options(clustermq.scheduler = &quot;slurm&quot;, clustermq.template = &quot;slurm_clustermq.tmpl&quot;) Here, slurm_clustermq.tmpl is a template file with configuration details. Use drake_hpc_template_file() to write one of the available examples. drake_hpc_template_file(&quot;slurm_clustermq.tmpl&quot;) # Write the file slurm_clustermq.tmpl. After modifying slurm_clustermq.tmpl by hand to meet your needs, call make() as usual. make(plan, parallelism = &quot;clustermq&quot;, jobs = 4) 10.6 The future backend 10.6.1 Transient workers make(parallelism = &quot;future&quot;, jobs = 2) launches transient workers to build your targets. When a target is ready to build, the master process creates a fresh worker to build it, and the worker terminates when the target is done. jobs = 2 means that at most 2 transient workers are allowed to run at a given time. 10.6.2 Installation Install the future package. install.packages(&quot;future&quot;) # CRAN release # Alternatively, install the GitHub development version. devtools::install_github(&quot;HenrikBengtsson/future&quot;, ref = &quot;develop&quot;) If you intend to use a cluster, be sure to install the future.batchtools package too. The future ecosystem contains even more packages that extend future’s parallel computing functionality, such as future.callr. 10.6.3 On your local machine First, select a future plan to tell future how to create the workers. See this table for descriptions of the core options. future::plan(future::multiprocess) Next, run make(). make(plan, parallelism = &quot;future&quot;, jobs = 2) 10.6.4 On a cluster Install the future.batchtools package and use this list to select a future plan that matches your resources. You will also need a compatible template file with configuration details. As with clustermq, drake can generate some examples: drake_hpc_template_file(&quot;slurm_batchtools.tmpl&quot;) # Edit by hand. Next, register the template file with a plan. library(future.batchtools) future::plan(batchtools_slurm, template = &quot;slurm_batchtools.tmpl&quot;) Finally, run make(). make(plan, parallelism = &quot;future&quot;, jobs = 2) 10.7 Advanced options 10.7.1 Selectivity Some targets build so quickly that it is not worth sending them to parallel workers. To run these targets locally in the master process, define a special hpc column of your drake plan. Below, NA and TRUE are treated the same, and make(plan, parallelism = &quot;clustermq&quot;) only sends model_1 and model_2 to parallel workers. drake_plan( model = target( crazy_long_computation(index), transform = map(index = c(1, 2)) ), accuracy = target( summarize_accuracy(model), transform = combine(model), hpc = FALSE ), specificity = target( summarize_specificity(model), transform = combine(model), hpc = FALSE ), report = target( render(knitr_in(&quot;results.Rmd&quot;), output_file = file_out(&quot;results.html&quot;)), hpc = FALSE ) ) #&gt; # A tibble: 5 x 3 #&gt; target command hpc #&gt; &lt;chr&gt; &lt;expr&gt; &lt;lgl&gt; #&gt; 1 model_1 crazy_long_computation(1) … NA #&gt; 2 model_2 crazy_long_computation(2) … NA #&gt; 3 accuracy summarize_accuracy(model_1, model_2) … FALSE #&gt; 4 specifici… summarize_specificity(model_1, model_2) … FALSE #&gt; 5 report render(knitr_in(&quot;results.Rmd&quot;), output_file = file_out(… FALSE 10.7.2 Memory By default, make() keeps targets in memory during runtime. Some targets are dependencies of other targets downstream, while others may be no longer actually need to be in memory. The memory_strategy argument to make() allows you to choose the tradeoff that best suits your project. Options: &quot;speed&quot;: Once a target is loaded in memory, just keep it there. This choice maximizes speed and hogs memory. &quot;memory&quot;: Just before building each new target, unload everything from memory except the target’s direct dependencies. This option conserves memory, but it sacrifices speed because each new target needs to reload any previously unloaded targets from storage. &quot;lookahead&quot;: Just before building each new target, search the dependency graph to find targets that will not be needed for the rest of the current make() session. In this mode, targets are only in memory if they need to be loaded, and we avoid superfluous reads from the cache. However, searching the graph takes time, and it could even double the computational overhead for large projects. 10.7.3 Storage In make(caching = &quot;master&quot;), the workers send the targets to the master process, and the master process stores them one by one in the cache. caching = &quot;master&quot; is compatible with all storr cache formats, including the more esoteric ones like storr_dbi() and storr_environment(). In make(caching = &quot;worker&quot;), the parallel workers are responsible for writing the targets to the cache. Some output-heavy projects can benefit from this form of parallelism. However, it can sometimes add slowness on clusters due to lag from network file systems. And there are additional restrictions: All the workers must have the same file system and the same working directory as the master process. Only the default storr_rds() cache may be used. Other formats like storr_dbi() and storr_environment() cannot accommodate parallel cache operations. See the storage chapter for details. 10.7.4 The template argument for persistent workers For more control and flexibility in the clustermq backend, you can parameterize your template file and use the template argument of make(). For example, suppose you want to programatically set the number of “slots” (basically cores) per job on an SGE system (clustermq guide to SGE setup here). Begin with a parameterized template file sge_clustermq.tmpl with a custom n_slots placeholder. # File: sge_clustermq.tmpl # Modified from https://github.com/mschubert/clustermq/wiki/SGE #$ -N {{ job_name }} # job name #$ -t 1-{{ n_jobs }} # submit jobs as array #$ -j y # combine stdout/error in one file #$ -o {{ log_file | /dev/null }} # output file #$ -cwd # use pwd as work dir #$ -V # use environment variable #$ -pe smp {{ n_slots | 1 }} # request n_slots cores per job module load R ulimit -v $(( 1024 * {{ memory | 4096 }} )) CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ master }}&quot;)&#39; Then when you run make(), use the template argument to set n_slots. options(clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge_clustermq.tmpl&quot;) library(drake) load_mtcars_example() make( my_plan, parallelism = &quot;clustermq&quot;, jobs = 16, template = list(n_slots = 4) # Request 4 cores per persistent worker. ) Custom placeholders like n_slots are processed with the infuser package. 10.7.5 The resources column for transient workers Different targets may need different resources. For example, plan &lt;- drake_plan( data = download_data(), model = big_machine_learning_model(data) ) The model needs a GPU and multiple CPU cores, and the data only needs the bare minimum resources. Declare these requirements in a new list column of the plan. Here, each element is a named list for the resources argument of future::future(). plan$resources &lt;- list( list(cores = 1, gpus = 0), list(cores = 4, gpus = 1) ) Next, plug your resources into the brew patterns of your batchtools template file. The following sge_batchtools.tmpl file shows how to do it, but the file itself probably requires modification before it will work with your own machine. #!/bin/bash #$ -cwd #$ -j y #$ -o &lt;%= log.file %&gt; #$ -V #$ -N &lt;%= job.name %&gt; #$ -pe smp &lt;%= resources[[&quot;cores&quot;]] %&gt; # CPU cores #$ -l gpu=&lt;%= resources[[&quot;gpus&quot;]] %&gt; # GPUs. Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; exit 0 Finally, register the template file and run your project. library(drake) library(future.batchtools) future::plan(batchtools_sge, template = &quot;sge_batchtools.tmpl&quot;) make(plan, parallelism = &quot;future&quot;, jobs = 2) 10.7.6 Parallel computing within targets To recruit parallel processes within individual targets, we recommend the future.callr and furrr packages. Usage details depend on the parallel backend you choose for make(). If you must write custom code with mclapply(), please read the subsection below on locked bindings/environments. 10.7.6.1 Locally Use future.callr and furrr normally. library(drake) # The targets just collect the process IDs of the callr processes. plan &lt;- drake_plan( x = furrr::future_map_int(1:2, function(x) Sys.getpid()), y = furrr::future_map_int(1:2, function(x) Sys.getpid()) ) # Tell the drake targets to fork up to 4 callr processes. future::plan(future.callr::callr) # Build the targets. make(plan) # Process IDs of the local workers of x: readd(x) 10.7.6.2 Persistent workers Each persistent worker needs its own future::plan(), which we set with the prework argument of make(). The following example uses SGE. To learn about templates for other clusters, please consult the clustermq documentation. library(drake) # The targets just collect the process IDs of the callr processes. plan &lt;- drake_plan( x = furrr::future_map_int(1:2, function(x) Sys.getpid()), y = furrr::future_map_int(1:2, function(x) Sys.getpid()) ) # Write a template file for clustermq. writeLines( c( &quot;#!/bin/bash&quot;, &quot;#$ -N {{ job_name }} # job name&quot;, &quot;#$ -t 1-{{ n_jobs }} # submit jobs as array&quot;, &quot;#$ -j y # combine stdout/error in one file&quot;, &quot;#$ -o {{ log_file | /dev/null }} # output file&quot;, &quot;#$ -cwd # use pwd as work dir&quot;, &quot;#$ -V # use environment variables&quot;, &quot;#$ -pe smp 4 # request 4 cores per job&quot;, &quot;module load R-qualified/3.5.2 # if loading R from an environment module&quot;, &quot;ulimit -v $(( 1024 * {{ memory | 4096 }} ))&quot;, &quot;CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(\\&quot;{{ master }}\\&quot;)&#39;&quot; ), &quot;sge_clustermq.tmpl&quot; ) # Register the scheduler and template file with clustermq. options( clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge_clustermq.tmpl&quot; ) # Build the targets. make( plan, parallelism = &quot;clustermq&quot;, jobs = 2, # Each of the two workers can spawn up to 4 local processes. prework = quote(future::plan(future.callr::callr)) ) # Process IDs of the local workers of x: readd(x) 10.7.6.3 Transient workers As explained in the future vignette, we can nest our future::plans(). Each target gets its own remote job, and each job can spawn up to 4 local callr processes. The following example uses SGE. To learn about templates for other clusters, please consult the future.batchtools documentation. library(drake) # The targets just collect the process IDs of the callr processes. plan &lt;- drake_plan( x = furrr::future_map_int(1:2, function(x) Sys.getpid()), y = furrr::future_map_int(1:2, function(x) Sys.getpid()) ) # Write a template file for future.batchtools. writeLines( c( &quot;#!/bin/bash&quot;, &quot;#$ -cwd # use pwd as work dir&quot;, &quot;#$ -j y # combine stdout/error in one file&quot;, &quot;#$ -o &lt;%= log.file %&gt; # output file&quot;, &quot;#$ -V # use environment variables&quot;, &quot;#$ -N &lt;%= job.name %&gt; # job name&quot;, &quot;#$ -pe smp 4 # 4 cores per job&quot;, &quot;module load R # if loading R from an environment module&quot;, &quot;Rscript -e &#39;batchtools::doJobCollection(\\&quot;&lt;%= uri %&gt;\\&quot;)&#39;&quot;, &quot;exit 0&quot; ), &quot;sge_batchtools.tmpl&quot; ) # In our nested plans, each target gets its own remote SGE job, # and each worker can spawn up to 4 `callr` processes. future::plan( list( future::tweak( future.batchtools::batchtools_sge, template = &quot;sge_batchtools.tmpl&quot; ), future.callr::callr ) ) # Build the targets. make(plan, parallelism = &quot;future&quot;, jobs = 2) # Process IDs of the local workers of x: readd(x) 10.7.6.4 Number of local workers per target By default, future::availableCores() determines the number of local callr workers. To better manage resources, you may wish to further restrict the number of callr workers for all targets in the plan, e.g. future::plan(future::callr, workers = 4L) or: future::plan( list( future::tweak( future.batchtools::batchtools_sge, template = &quot;sge_batchtools.tmpl&quot; ), future::tweak(future.callr::callr, workers = 4L) ) ) Alternatively, you can use chunking to prevent individual targets from using too many workers, e.g. furrr::future_map(.options = furrr::future_options(scheduling = 4)). Here, the scheduling argument sets the average number of futures per worker. 10.7.6.5 Locked binding/environment errors Some workflows unavoidably use mclapply(), which is known to modify the global environment against drake’s will. If you are stuck, there are two workarounds. Use make(lock_envir = FALSE). Use the envir argument of make(). That way, drake locks your special custom environment instead of the global environment. # Load the main example: https://github.com/wlandau/drake-examples library(drake) drake_example(&quot;main&quot;) setwd(&quot;main&quot;) # Define and populate a special custom environment. envir &lt;- new.env(parent = globalenv()) source(&quot;R/packages.R&quot;, local = envir) source(&quot;R/functions.R&quot;, local = envir) source(&quot;R/plan.R&quot;, local = envir) # Check the contents of your environments. ls(envir) # Should have your functions and plan ls() # The global environment should only have what you started with. # Build the targets using your custom environment make(envir$plan, envir = envir) 10.7.7 Custom job schedulers It is possible to supply a custom job scheduler function to the parallelism argument of make(). The backend_future_lapply_staged() function from the drake.future.lapply.staged package is an example. You might consider writing your own such function if you wish to Experiment with a more efficient job scheduler before proposing a patch to core drake, or Aggressively optimize drake for your specialized computing resources. This feature is very advanced, and you should only attempt it in production if you really know what you are doing. Use at your own risk. 10.7.8 Hasty mode The drake.hasty package is a bare-bones experimental spin-off of drake. It sacrifices reproducibility to aggressively boost speed when scheduling and executing your targets. It is not recommended for most serious production use cases, but it can useful for experimentation. "],
["triggers.html", "Chapter 11 Triggers: decision rules for building targets 11.1 What are triggers? 11.2 Customization 11.3 Alternative trigger modes 11.4 Consider hasty mode 11.5 A more practical example", " Chapter 11 Triggers: decision rules for building targets When you call make(), drake tries to skip as many targets as possible. If it thinks a command will return the same value as last time, it does not bother running it. In other words, drake is lazy, and laziness saves you time. 11.1 What are triggers? To figure out whether it can skip a target, drake goes through an intricate checklist of triggers: The missing trigger: Do we lack a return value from a previous make()? Maybe you are building the target for the first time or you removed it from the cache with clean(). The command trigger: did the command in the drake plan change nontrivially since the last make()? Changes to spacing, formatting, and comments are ignored. The depend trigger: did any non-file dependencies change since the last make()? These could be: Other targets. Imported objects. Imported functions (ignoring changes to spacing, formatting, and comments). Any dependencies of imported functions. Any dependencies of dependencies of imported functions, and so on. The file trigger: did any file inputs or file outputs change since the last make()? These files are the ones explicitly declared in the command with file_in(), knitr_in(), and file_out(). The condition trigger: an optional user-defined piece of code that evaluates to a TRUE/FALSE value. The target builds if the value is TRUE. The change trigger: an optional user-defined piece of code that evaluates to any value (preferably small and quick to compute). The target builds if the value changed since the last make(). If any trigger detects something wrong or different with the target or its dependencies, the next make() will run the command and (re)build the target. 11.2 Customization With the trigger() function, you can create your own customized checklist of triggers. Let’s run a simple workflow with just the missing trigger. We deactivate the command, depend, and file triggers by setting the respective command, depend, and file arguments to FALSE. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2, psi_2 = (sqrt(5) - 1) / 2 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_1 #&gt; target psi_2 Now, even if you wreck all the commands, nothing rebuilds. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = (sqrt(5) - 1) / 2 - 9999999999999 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; All targets are already up to date. You can also give different targets to different triggers. Triggers in the drake plan override the trigger argument to make(). Below, psi_2 always builds, but psi_1 only builds if it has never been built before. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = target( command = (sqrt(5) - 1) / 2 - 9999999999999, trigger = trigger(condition = psi_1 &gt; 0) ) ) plan #&gt; # A tibble: 2 x 3 #&gt; target command trigger #&gt; &lt;chr&gt; &lt;expr&gt; &lt;expr&gt; #&gt; 1 psi_1 (sqrt(5) + 1)/2 + 9999999999999 NA #&gt; 2 psi_2 (sqrt(5) - 1)/2 - 9999999999999 trigger(condition = psi_1 &gt; 0) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_2 make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_2 Interestingly, psi_2 now depends on psi_1. Since psi_1 is part of the because of the condition trigger, it needs to be up to date before we attempt psi_2. However, since psi_1 is not part of the command, changing it will not trip the other triggers such as depend. vis_drake_graph(drake_config(plan)) In the next toy example below, drake reads from a file to decide whether to build x. Try it out. plan &lt;- drake_plan( x = target( 1 + 1, trigger = trigger(condition = file_in(readRDS(&quot;file.rds&quot;))) ) ) saveRDS(TRUE, &quot;file.rds&quot;) make(plan) #&gt; target x make(plan) #&gt; target x make(plan) #&gt; target x saveRDS(FALSE, &quot;file.rds&quot;) make(plan) #&gt; All targets are already up to date. make(plan) #&gt; All targets are already up to date. make(plan) #&gt; All targets are already up to date. In a real project with remote data sources, you may want to use the condition trigger to limit your builds to times when enough bandwidth is available for a large download. For example, drake_plan( x = target( command = download_large_dataset(), trigger = trigger(condition = is_enough_bandwidth()) ) ) Since the change trigger can return any value, it is often easier to use than the condition trigger. clean(destroy = TRUE) plan &lt;- drake_plan( x = target( command = 1 + 1, trigger = trigger(change = sqrt(y)) ) ) y &lt;- 1 make(plan) #&gt; target x make(plan) #&gt; All targets are already up to date. y &lt;- 2 make(plan) #&gt; target x In practice, you may want to use the change trigger to check a large remote before downloading it. drake_plan( x = target( command = download_large_dataset(), trigger = trigger( condition = is_enough_bandwidth(), change = date_last_modified() ) ) ) A word of caution: every non-NULL change trigger is always evaluated, and its value is carried around in memory throughout make(). So if you are not careful, heavy use of the change trigger could slow down your workflow and consume extra resources. The change trigger should return small values (and should ideally be quick to evaluate). To reduce memory consumption, you may want to return a fingerprint of your trigger value rather than the value itself. See the digest package for more information on computing hashes/fingerprints. library(digest) drake_plan( x = target( command = download_large_dataset(), trigger = trigger( change = digest(download_medium_dataset()) ) ) ) 11.3 Alternative trigger modes Sometimes, you may want to suppress a target without having to worry about turning off every single trigger. That is why the trigger() function has a mode argument, which controls the role of the condition trigger in the decision to build or skip a target. The available trigger modes are &quot;whitelist&quot; (default), &quot;blacklist&quot;, and &quot;condition&quot;. trigger(mode = &quot;whitelist&quot;): we rebuild the target whenever condition evaluates to TRUE. Otherwise, we defer to the other triggers. This is the default behaviro described above in this chapter. trigger(mode = &quot;blacklist&quot;): we skip the target whenever condition evaluates to FALSE. Otherwise, we defer to the other triggers. trigger(mode = &quot;condition&quot;): here, the condition trigger is the only decider, and we ignore all the other triggers. We rebuild target whenever condition evaluates to TRUE and skip it whenever condition evaluates to FALSE. 11.4 Consider hasty mode In hasty mode, drake acts as a job scheduler without watching dependencies. In other words, make(parallelism = &quot;hasty&quot;) always runs all the targets, and computational overhead is dramatically reduced. Read more here. 11.5 A more practical example See the “packages” example for a more practical demonstration of triggers and their usefulness. "],
["time.html", "Chapter 12 Time: logging, prediction, and strategy 12.1 Predict total runtime 12.2 Strategize your high-performance computing", " Chapter 12 Time: logging, prediction, and strategy Thanks to Jasper Clarkberg, drake records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan) #&gt; target large #&gt; target small #&gt; target regression1_large #&gt; target regression2_large #&gt; target regression1_small #&gt; target regression2_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large #&gt; target summ_regression2_large #&gt; target coef_regression2_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target report build_times(digits = 8) # From the cache. #&gt; # A tibble: 15 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; #&gt; 1 coef_regression1_large 0.003s 0.004s 0s #&gt; 2 coef_regression1_small 0.003s 0.004s 0s #&gt; 3 coef_regression2_large 0.003s 0.004s 0s #&gt; 4 coef_regression2_small 0.004s 0.004s 0s #&gt; 5 large 0.003s 0.004s 0s #&gt; 6 regression1_large 0.004s 0.004s 0s #&gt; 7 regression1_small 0.004s 0s 0.004s #&gt; 8 regression2_large 0.005s 0.004s 0s #&gt; 9 regression2_small 0.004s 0.004s 0s #&gt; 10 report 0.049s 0.048s 0.004s #&gt; 11 small 0.003s 0.004s 0s #&gt; 12 summ_regression1_large 0.003s 0.004s 0s #&gt; 13 summ_regression1_small 0.003s 0.004s 0s #&gt; 14 summ_regression2_large 0.003s 0.004s 0s #&gt; 15 summ_regression2_small 0.003s 0.004s 0s ## `dplyr`-style `tidyselect` commands build_times(starts_with(&quot;coef&quot;), digits = 8) #&gt; # A tibble: 4 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; #&gt; 1 coef_regression1_large 0.003s 0.004s 0s #&gt; 2 coef_regression1_small 0.003s 0.004s 0s #&gt; 3 coef_regression2_large 0.003s 0.004s 0s #&gt; 4 coef_regression2_small 0.004s 0.004s 0s 12.1 Predict total runtime drake uses these times to predict the runtime of the next make(). At this moment, everything is up to date in the current example, so the next make() should ideally take no time at all (except for preprocessing overhead). config &lt;- drake_config(my_plan, verbose = 0L) predict_runtime(config) #&gt; [1] &quot;0s&quot; Suppose we change a dependency to make some targets out of date. Now, the next make() should take longer since some targets are out of date. reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } predict_runtime(config) #&gt; [1] &quot;0.071s&quot; And what if you plan to delete the cache and build all the targets from scratch? predict_runtime(config, from_scratch = TRUE) #&gt; [1] &quot;0.097s&quot; 12.2 Strategize your high-performance computing Let’s say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run make(), your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let’s write down these known times in seconds. known_times &lt;- rep(7200, nrow(my_plan)) names(known_times) &lt;- my_plan$target known_times[&quot;report&quot;] &lt;- 5 known_times #&gt; report small large #&gt; 5 7200 7200 #&gt; regression1_small regression1_large regression2_small #&gt; 7200 7200 7200 #&gt; regression2_large summ_regression1_small summ_regression1_large #&gt; 7200 7200 7200 #&gt; summ_regression2_small summ_regression2_large coef_regression1_small #&gt; 7200 7200 7200 #&gt; coef_regression1_large coef_regression2_small coef_regression2_large #&gt; 7200 7200 7200 How many parallel jobs should you use in the next make()? The predict_runtime() function can help you decide. predict_runtime(jobs = n) simulates persistent parallel workers and reports the estimated total runtime of make(jobs = n). (See also predict_workers().) time &lt;- c() for (jobs in 1:12){ time[jobs] &lt;- predict_runtime( config, jobs = jobs, from_scratch = TRUE, known_times = known_times ) } library(ggplot2) ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) + geom_line(aes(x = jobs, y = time, group = group)) + scale_y_continuous(breaks = 0:10 * 4, limits = c(0, 29)) + theme_gray(16) + xlab(&quot;jobs argument of make()&quot;) + ylab(&quot;Predicted runtime of make() (hours)&quot;) We see serious potential speed gains up to 4 jobs, but beyond that point, we have to double the jobs to shave off another 2 hours. Your choice of jobs for make() ultimately depends on the runtime you can tolerate and the computing resources at your disposal. A final note on predicting runtime: the output of predict_runtime() and predict_workers() also depends the optional workers column of your drake_plan(). If you micromanage which workers are allowed to build which targets, you may minimize reads from disk, but you could also slow down your workflow if you are not careful. See the high-performance computing guide for more. "],
["store.html", "Chapter 13 Storage 13.1 Cache formats 13.2 Hash algorithms", " Chapter 13 Storage When you run make(), drake stores your targets in a cache. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan, verbose = 0L) The default cache is a hidden .drake folder. find_cache() ### [1] &quot;/home/you/project/.drake&quot; find_project() ### [1] &quot;/home/you/project&quot; drake uses the storr package to create and modify caches. library(storr) cache &lt;- storr_rds(&quot;.drake&quot;) head(cache$list()) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; head(cache$get(&quot;small&quot;)) #&gt; x y #&gt; 1 3.780 15.2 #&gt; 2 3.440 17.8 #&gt; 3 3.215 21.4 #&gt; 4 3.440 19.2 #&gt; 5 2.770 19.7 #&gt; 6 2.465 21.5 drake has its own interface on top of storr to make it easier to work with the default .drake/ cache. The loadd(), readd(), and cached() functions explore saved targets. head(cached()) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; head(readd(small)) #&gt; x y #&gt; 1 3.780 15.2 #&gt; 2 3.440 17.8 #&gt; 3 3.215 21.4 #&gt; 4 3.440 19.2 #&gt; 5 2.770 19.7 #&gt; 6 2.465 21.5 loadd(large) head(large) #&gt; x y #&gt; 1 2.140 26.0 #&gt; 2 3.730 17.3 #&gt; 3 1.935 27.3 #&gt; 4 2.780 21.4 #&gt; 5 2.320 22.8 #&gt; 6 2.780 21.4 rm(large) # Does not remove `large` from the cache. Functions get_cache(), storr::storr_rds(), and new_cache() recover and create caches. cache &lt;- get_cache(path = getwd()) # root or subdirectory of a drake project cache$driver$path #&gt; [1] &quot;/tmp/RtmpVn0V8f/file4928e6274ff/.drake&quot; cache &lt;- storr::storr_rds(path = &quot;.drake&quot;) # actual cache folder cache2 &lt;- new_cache(&quot;my_new_cache&quot;) cache2$driver$path #&gt; [1] &quot;/tmp/RtmpVn0V8f/file4928e6274ff/my_new_cache&quot; You can supply your own cache to make() and similar functions. cache2$list() #&gt; character(0) plan2 &lt;- drake_plan(x = 1, y = sqrt(x)) make(plan2, cache = cache2) #&gt; Unloading targets from environment: #&gt; y #&gt; target x #&gt; target y cache2$list() #&gt; [1] &quot;x&quot; &quot;y&quot; config &lt;- drake_config(plan = plan2, cache = cache2) vis_drake_graph(config) Destroy caches to remove them from your file system. cache$destroy() cache2$destroy() 13.1 Cache formats 13.1.1 RDS caches By default, drake uses storr_rds() caches because they allow make(jobs = 4) to safely store multiple targets in parallel. To achieve this thread safety, however, storr splits up the data into a pool of tiny cryptically-named files. make(my_plan, verbose = 0L) head(list.files(&quot;.drake/data&quot;)) #&gt; [1] &quot;0ef06daf1ea3a4ff.rds&quot; &quot;106a19570cda1c17.rds&quot; &quot;1f3305a3d1c3d5ea.rds&quot; #&gt; [4] &quot;216027252a7b462f.rds&quot; &quot;21935c86f12692e2.rds&quot; &quot;24f1b86566e3eed1.rds&quot; head(list.files(&quot;.drake/keys/objects&quot;)) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; This makes RDS caches difficult to share with collaborators and put under version control. For the sake of portability, you may wish to work with database cashes as the next section describes. Alternatively, you can track changes in a cache log with fingerprints of all your targets. drake_cache_log() #&gt; # A tibble: 22 x 3 #&gt; hash type name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 ecb4739b594986b9 target coef_regression1_large #&gt; 2 599bea898e306775 target coef_regression1_small #&gt; 3 44bfee4c258a76d7 target coef_regression2_large #&gt; 4 b5502b9c62933484 target coef_regression2_small #&gt; 5 d87293dd4d012762 target large #&gt; 6 f4b89e63bc92af79 import datasets::mtcars #&gt; 7 7a456ee58df699be import file report.Rmd #&gt; 8 6153acbb3f1070de target file report.md #&gt; 9 db84c9f752635a13 import random_rows #&gt; 10 21935c86f12692e2 import reg1 #&gt; # … with 12 more rows make(my_plan, verbose = 0L, cache_log_file = TRUE) read_csv(&quot;drake_cache.csv&quot;, col_types = cols()) #&gt; # A tibble: 22 x 3 #&gt; hash type name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 ecb4739b594986b9 target coef_regression1_large #&gt; 2 599bea898e306775 target coef_regression1_small #&gt; 3 44bfee4c258a76d7 target coef_regression2_large #&gt; 4 b5502b9c62933484 target coef_regression2_small #&gt; 5 d87293dd4d012762 target large #&gt; 6 f4b89e63bc92af79 import datasets::mtcars #&gt; 7 7a456ee58df699be import file report.Rmd #&gt; 8 6153acbb3f1070de target file report.md #&gt; 9 db84c9f752635a13 import random_rows #&gt; 10 21935c86f12692e2 import reg1 #&gt; # … with 12 more rows Use the cache_log_file argument of make() to refresh the cache log file every time you run make(). Then, if you put this file under version control (e.g. with git/GitHub) then the commit history will tell you how your data objects change over time. 13.1.2 Database caches It is possible use a single SQLite database file as the cache. mydb &lt;- DBI::dbConnect(RSQLite::SQLite(), &quot;database-file.sqlite&quot;) cache &lt;- storr::storr_dbi(&quot;datatable&quot;, &quot;keystable&quot;, mydb) make(my_plan, cache = cache, verbose = 0L) loadd(small, cache = cache) head(small) #&gt; x y #&gt; 1 3.780 15.2 #&gt; 2 3.440 17.8 #&gt; 3 3.215 21.4 #&gt; 4 3.440 19.2 #&gt; 5 2.770 19.7 #&gt; 6 2.465 21.5 But be careful: for safe parallel computing (jobs &gt; 1) there are additional requirements for make(): Select either parallelism = &quot;clustermq&quot; or parallelism = &quot;future&quot;. Select caching = &quot;master&quot; to ensure that only the master process touches the cache. For a more complete demonstration, please see this example code, which you can download with drake_example(&quot;dbi&quot;). 13.1.3 Environment caches Environment caches live in computer memory, not your file system, so they are a nice way to run small and fast experiments. However, unless you save the cache manually, all your data will be lost when you quit your R session. And for large projects, you may not be able to fit all your data in memory anyway. cache &lt;- storr_environment() make(my_plan, cache = cache) 13.2 Hash algorithms storr caches use hash functions to keep track of stored objects. A hash function is just a way to fingerprint data. The idea is to represent an arbitrary chunks of data using (nearly) unique strings of fixed size. library(digest) # package for hashing objects and files smaller_data &lt;- 12 larger_data &lt;- rnorm(1000) digest(smaller_data) # compute the hash #&gt; [1] &quot;23c80a31c0713176016e6e18d76a5f31&quot; digest(larger_data) #&gt; [1] &quot;b03c1a142bc71a1b8afed7d23ce60f9f&quot; The digest package (used by both drake and storr) supports a wide variety of hash functions. Some generate larger hash keys, some are slower to compute, and others are more prone to “collisions” (where two different data objects are given the same hash key). The digest package supports a variety of hash algorithms. digest(larger_data, algo = &quot;sha512&quot;) #&gt; [1] &quot;cca91831466b966d82ae4440dea3be3a1b5555f6189563eb3d8b1a83c8c70d68daae9a35f62a00a96fefa241a26ce5c183a7f8a8d584bbbe023acb4b254080e9&quot; digest(larger_data, algo = &quot;md5&quot;) #&gt; [1] &quot;b03c1a142bc71a1b8afed7d23ce60f9f&quot; digest(larger_data, algo = &quot;xxhash64&quot;) #&gt; [1] &quot;ec927d1d9050ae96&quot; digest(larger_data, algo = &quot;murmur32&quot;) #&gt; [1] &quot;6faecd4e&quot; For drake, the default hash algorithm is xxhash64. You can choose different hash functions with the hash_algorithm argument to new_cache(), storr_rds(), and similar functions. (For drake version 6.2.1 and earlier, see the long_hash_algo and short_hash_algo arguments.) cache3 &lt;- new_cache(&quot;cache_3_path&quot;, hash_algorithm = &quot;murmur32&quot;) cache4 &lt;- storr_rds(&quot;cache_4_path&quot;, hash_algorithm = &quot;crc32&quot;) "],
["faq.html", "A Frequently-asked questions", " A Frequently-asked questions This FAQ is a compendium of pedagogically useful issues tagged on GitHub. To contribute, please submit a new issue and ask that it be labeled a frequently asked question. new transform function split to chunk a data.frame Plan should be out-of-date but isn’t map using initial parameter only follows through for one stage Dynamically scale clustermq workers knitr file paths List columns don’t work in map(.data) map() back to original variables after after combine() FAQ: Functions as data Avoid re-running targets if supplied args are the same as default args How to create a jagged cross() transform How to combine() while keeping track of the sources of targets target invalidated when referenced from another plan Within-target parallelism fails cannot remove bindings from a locked environment Functions that depend on targets Erroneous circular workflow error when using NSE in function function dependencies are missing: drake_config() in a magrittr pipe Best practices for including a drake workflow in a package Can you have multiple drake plans? evaluate file.path and variables in file_out and friends Working with HPC time limits Reproducibility with random numbers How should I mix non-R code (e.g. Python and shell scripts) in a large drake workflow? Reproducible remote data sources Trouble with caches sent through Dropbox How to add .R files to drake_plan() "],
["caution.html", "B Cautionary notes B.1 Workflow plans B.2 Execution B.3 Dependencies B.4 High-performance computing B.5 Storage", " B Cautionary notes This chapter addresses drake’s known edge cases, pitfalls, and weaknesses that might not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the issue tracker, where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version of drake. For a guide to debugging and testing drake projects, please refer to the separate guide to debugging and testing drake projects. B.1 Workflow plans B.1.1 Externalizing commands in R script files It is common practice to divide the work of a project into multiple R files, but if you do this, you will not get the most out of drake. Please see the chapter on organizing your files for more details. B.1.2 Commands are NOT perfectly flexible. In your drake plan (produced by drake_plan() and accepted by make()), your commands can usually be flexible R expressions. drake_plan( target1 = 1 + 1 - sqrt(sqrt(3)), target2 = my_function(web_scraped_data) %&gt;% my_tidy ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 target1 1 + 1 - sqrt(sqrt(3)) #&gt; 2 target2 my_function(web_scraped_data) %&gt;% my_tidy However, please try to avoid formulas and function definitions in your commands. You may be able to get away with drake_plan(f = function(x){x + 1}) or drake_plan(f = y ~ x) in some use cases, but be careful. It is generally to define functions and formulas in your workspace and then let make() import them. (Alternatively, use the envir argument to make() to tightly control which imported functions are available.) Use the check_plan() function to help screen and quality-control your drake plan, use tracked() to see the items that are reproducibly tracked, and use vis_drake_graph() and build_drake_graph() to see the dependency structure of your project. B.2 Execution B.2.1 Install drake properly. You must properly install drake using install.packages(), devtools::install_github(), or a similar approach. Functions like devtools::load_all() are insufficient, particularly for parallel computing functionality in which separate new R sessions try to require(drake). B.2.2 Install all your packages. Your workflow may depend on external packages such as ggplot2, dplyr, and MASS. Such packages must be formally installed with install.packages(), devtools::install_github(), devtools::install_local(), or a similar command. If you load uninstalled packages with devtools::load_all(), results may be unpredictable and incorrect. B.2.3 A note on tidy evaluation Running commands in your R console is not always exactly like running them with make(). That’s because make() uses tidy evaluation as implemented in the rlang package. ## This `drake` plan uses rlang&#39;s quasiquotation operator `!!`. my_plan &lt;- drake_plan(list = c( little_b = &quot;\\&quot;b\\&quot;&quot;, letter = &quot;!!little_b&quot; )) #&gt; Warning in drake_plan(list = c(little_b = &quot;\\&quot;b\\&quot;&quot;, letter = &quot;!!little_b&quot;)): #&gt; The `list` argument of `drake_plan()` is deprecated. Use the interface #&gt; described at https://ropenscilabs.github.io/drake-manual/plans.html#large- #&gt; plans. #&gt; Error in enexpr(expr): object &#39;little_b&#39; not found my_plan #&gt; # A tibble: 15 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;),… #&gt; 2 small simulate(48) … #&gt; 3 large simulate(64) … #&gt; 4 regression1_small reg1(small) … #&gt; 5 regression1_large reg1(large) … #&gt; 6 regression2_small reg2(small) … #&gt; 7 regression2_large reg2(large) … #&gt; 8 summ_regression1_s… suppressWarnings(summary(regression1_small$residual… #&gt; 9 summ_regression1_l… suppressWarnings(summary(regression1_large$residual… #&gt; 10 summ_regression2_s… suppressWarnings(summary(regression2_small$residual… #&gt; 11 summ_regression2_l… suppressWarnings(summary(regression2_large$residual… #&gt; 12 coef_regression1_s… suppressWarnings(summary(regression1_small))$coeffi… #&gt; 13 coef_regression1_l… suppressWarnings(summary(regression1_large))$coeffi… #&gt; 14 coef_regression2_s… suppressWarnings(summary(regression2_small))$coeffi… #&gt; 15 coef_regression2_l… suppressWarnings(summary(regression2_large))$coeffi… make(my_plan) #&gt; Unloading targets from environment: #&gt; small #&gt; Warning: knitr/rmarkdown report &#39;report.Rmd&#39; does not exist and cannot be #&gt; inspected for dependencies. #&gt; Warning: missing input files: #&gt; report.Rmd #&gt; target report #&gt; Warning: Missing files for target report: #&gt; report.md #&gt; Warning: target report warnings: #&gt; cannot open file &#39;report.Rmd&#39;: No such file or directory #&gt; fail report #&gt; Error: Target `report` failed. Call `diagnose(report)` for details. Error message: #&gt; cannot open the connection readd(letter) #&gt; Error: key &#39;letter&#39; (&#39;objects&#39;) not found For the commands you specify the free-form ... argument, drake_plan() also supports tidy evaluation. For example, it supports quasiquotation with the !! argument. Use tidy_evaluation = FALSE or the list argument to suppress this behavior. my_variable &lt;- 5 drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;) ) #&gt; Warning in drake_plan(a = !!my_variable, b = !!my_variable + 1, list = c(d #&gt; = &quot;!!my_variable&quot;)): The `list` argument of `drake_plan()` is deprecated. #&gt; Use the interface described at https://ropenscilabs.github.io/drake-manual/ #&gt; plans.html#large-plans. #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 a 5 #&gt; 2 b 5 + 1 #&gt; 3 d 5 drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;), tidy_evaluation = FALSE ) #&gt; Warning in drake_plan(a = !!my_variable, b = !!my_variable + 1, list = c(d #&gt; = &quot;!!my_variable&quot;), : The `list` argument of `drake_plan()` is deprecated. #&gt; Use the interface described at https://ropenscilabs.github.io/drake-manual/ #&gt; plans.html#large-plans. #&gt; Warning in drake_plan(a = !!my_variable, b = !!my_variable + 1, list = c(d #&gt; = &quot;!!my_variable&quot;), : The `tidy_evaluation` argument of `drake_plan()` is #&gt; deprecated. Use the `tidy_eval` argument instead. #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 a !!my_variable #&gt; 2 b !!my_variable + 1 #&gt; 3 d expression(!!my_variable) For instances of !! that remain in the drake plan, make() will run these commands in tidy fashion, evaluating the !! operator using the environment you provided. B.2.4 Find and diagnose your errors. When make() fails, use failed() and diagnose() to debug. Try the following out yourself. ## Targets with available diagnostic metadata, incluing errors, warnings, etc. diagnose() #&gt; [1] &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; &quot;p-OJSXA33SOQXFE3LE&quot; #&gt; [3] &quot;random_rows&quot; &quot;reg1&quot; #&gt; [5] &quot;reg2&quot; &quot;report&quot; #&gt; [7] &quot;simulate&quot; f &lt;- function(){ stop(&quot;unusual error&quot;) } bad_plan &lt;- drake_plan(target = f()) withr::with_message_sink( stdout(), make(bad_plan) ) #&gt; target target #&gt; fail target #&gt; Error: Target `target` failed. Call `diagnose(target)` for details. Error message: #&gt; unusual error failed() # From the last make() only #&gt; [1] &quot;target&quot; error &lt;- diagnose(target)$error # See also warnings and messages. error$message #&gt; [1] &quot;unusual error&quot; error$call #&gt; f() error$calls # View the traceback. #&gt; [[1]] #&gt; local({ #&gt; f() #&gt; }) #&gt; #&gt; [[2]] #&gt; eval.parent(substitute(eval(quote(expr), envir))) #&gt; #&gt; [[3]] #&gt; eval(expr, p) #&gt; #&gt; [[4]] #&gt; eval(expr, p) #&gt; #&gt; [[5]] #&gt; eval(quote({ #&gt; f() #&gt; }), new.env()) #&gt; #&gt; [[6]] #&gt; eval(quote({ #&gt; f() #&gt; }), new.env()) #&gt; #&gt; [[7]] #&gt; f() #&gt; #&gt; [[8]] #&gt; stop(&quot;unusual error&quot;) B.2.5 Refresh the drake_config() list early and often. The master configuration list returned by drake_config() is important to drake’s internals, and you will need it for functions like outdated() and vis_drake_graph(). The config list corresponds to a single call to make(), and you should not modify it by hand afterwards. For example, modifying the targets element post-hoc will have no effect because the graph element will remain the same. It is best to just call drake_config() again. B.2.6 Workflows as R packages. The R package structure is a great way to organize the files of your project. Writing your own package to contain your data science workflow is a good idea, but you will need to Use expose_imports() to properly account for all your nested function dependencies, and If you load the package with devtools::load_all(), set the prework argument of make(): e.g. make(prework = &quot;devtools::load_all()&quot;). See the file organization chapter and ?expose_imports for detailed explanations. Thanks to Jasper Clarkberg for the workaround. B.2.7 The lazy_load flag does not work with &quot;parLapply&quot; parallelism. Ordinarily, drake prunes the execution environment at every parallelizable stage. In other words, it loads all the dependencies and unloads anything superfluous for entire batches of targets. This approach may require too much memory for some use cases, so there is an option to delay the loading of dependencies using the lazy_load argument to make() (powered by delayedAssign()). There are two major risks. make(..., lazy_load = TRUE, parallelism = &quot;parLapply&quot;, jobs = 2) does not work. If you want to use local multisession parallelism with multiple jobs and lazy loading, try &quot;future_lapply&quot; parallelism instead. library(future) future::plan(multisession) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan, lazy_load = TRUE, parallelism = &quot;future_lapply&quot;) Delayed evaluation may cause the same dependencies to be loaded multiple times, and these duplicated loads could be slow. B.2.8 Timeouts may be unreliable. You can call make(..., timeout = 10) to time out all each target after 10 seconds. However, timeouts rely on R.utils::withTimeout(), which in turn relies on setTimeLimit(). These functions are the best that R can offer right now, but they have known issues, and timeouts may fail to take effect for certain environments. B.3 Dependencies B.3.1 Objects that contain functions may rebuild too often For example, an R6 class changes whenever a new R6 object of that class is created. library(digest) library(R6) circle_class &lt;- R6Class( &quot;circle_class&quot;, private = list(radius = NULL), public = list( initialize = function(radius){ private$radius &lt;- radius }, area = function(){ pi * private$radius ^ 2 } ) ) digest(circle_class) #&gt; [1] &quot;5a1c53e067dce43a71862882e4e7d9b4&quot; circle &lt;- circle_class$new(radius = 5) digest(circle_class) # example_class changed #&gt; [1] &quot;dd6d383cf76624c46feb4855fd91e60c&quot; rm(circle) Ordinarily, drake overreacts to this change and builds targets repeatedly. clean() plan &lt;- drake_plan( circle = circle_class$new(radius = 10), area = circle$area() ) make(plan) # `circle_class` changes because it is referenced. #&gt; target circle #&gt; target area make(plan) # Builds `circle` again because `circle_class` changed. #&gt; target circle The solution is to define your R6 class inside a function. drake does the right thing when it comes to tracking changes to functions. clean() new_circle &lt;- function(radius){ circle_class &lt;- R6Class( &quot;circle_class&quot;, private = list(radius = NULL), public = list( initialize = function(radius){ private$radius &lt;- radius }, area = function(){ pi * private$radius ^ 2 } ) ) circle_class$new(radius = radius) } plan &lt;- drake_plan( circle = new_circle(radius = 10), area = circle$area() ) make(plan) #&gt; target circle #&gt; target area make(plan) #&gt; All targets are already up to date. B.3.2 Dependencies are not tracked in some edge cases. You should explicitly learn the items in your workflow and the dependencies of your targets. ?deps ?tracked ?vis_drake_graph drake can be fooled into skipping objects that should be treated as dependencies. For example: f &lt;- function(){ b &lt;- get(&quot;x&quot;, envir = globalenv()) # x is incorrectly ignored digest::digest(file_dependency) } deps_code(f) #&gt; # A tibble: 4 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 globalenv globals #&gt; 2 file_dependency globals #&gt; 3 get globals #&gt; 4 digest::digest namespaced command &lt;- &quot;x &lt;- digest::digest(file_in(\\&quot;input_file.rds\\&quot;)); assign(\\&quot;x\\&quot;, 1); x&quot; # nolint deps_code(command) #&gt; # A tibble: 3 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 assign globals #&gt; 2 digest::digest namespaced #&gt; 3 input_file.rds file_in drake takes special precautions so that a target/import does not depend on itself. For example, deps_code(f) might return &quot;f&quot; if f() is a recursive function, but make() just ignores this conflict and runs as expected. In other words, make() automatically removes all self-referential loops in the dependency network. B.3.3 Dependencies of knitr reports If you have knitr reports, you can use knitr_report() in your commands so that your reports are refreshed every time one of their dependencies changes. See drake_example(&quot;mtcars&quot;) and the ?knitr_in() help file examples for demonstrations. Dependencies are detected if you call loadd() or readd() in your code chunks. But beware: an empty call to loadd() does not account for any dependencies even though it loads all the available targets into your R session. B.3.4 S3 and generic methods If you reference S3 methods, only the generic method is tracked as a dependency. plan &lt;- drake_plan(x = base::print(data.frame(y = 1))) cache &lt;- storr::storr_environment() make(plan, cache = cache) #&gt; target x #&gt; y #&gt; 1 1 readd(&quot;base::print&quot;, character_only = TRUE, cache = cache) #&gt; [1] &quot;function (x, ...) \\nUseMethod(\\&quot;print\\&quot;)&quot; #&gt; [2] &quot;&quot; But print() itself is not actually very helpful. Because of S3, print.data.frame() is actually doing the work. If you were to write your own S3 system and change a specific method like print.data.frame(), changes would not be reproducibly tracked because drake only finds the generic function. This is unavoidable because drake uses static code analysis to detect dependencies. It finds generics like print(), but it has no way of knowing in advance what method will actually be called. B.3.5 File outputs in imported functions. Do not call file_out() inside imported functions that you write. Only targets in your drake plan should have file outputs. ## toally_fine() will depend on the imported data.csv file. ## But make sure data.csv is an imported file and not a file target. totally_okay &lt;- function(x, y, z){ read.csv(file_in(&quot;data.csv&quot;)) } ## file_out() is for file targets, ## so `drake` will ignore it in imported functions. avoid_this &lt;- function(x, y, z){ read.csv(file_out(&quot;data.csv&quot;)) } B.3.6 Functions produced by Vectorize() With functions produced by Vectorize(), detecting dependencies is especially hard because the body of every such function is args &lt;- lapply(as.list(match.call())[-1L], eval, parent.frame()) names &lt;- if (is.null(names(args))) character(length(args)) else names(args) dovec &lt;- names %in% vectorize.args do.call(&quot;mapply&quot;, c(FUN = FUN, args[dovec], MoreArgs = list(args[!dovec]), SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES)) Thus, if f is constructed with Vectorize(g, ...), drake searches g() for dependencies, not f(). In fact, if drake sees that environment(f)[[&quot;FUN&quot;]] exists and is a function, then environment(f)[[&quot;FUN&quot;]] will be analyzed instead of f(). Furthermore, if f() is the output of Vectorize(), then drake reproducibly tracks environment(f)[[&quot;FUN&quot;]] rather than f() itself. Thus, if the configuration settings of vectorization change (such as which arguments are vectorized), but the core element-wise functionality remains the same, then make() will not react. Also, if you hover over the f node in vis_drake_graph(hover = TRUE), then you will see the body of environment(f)[[&quot;FUN&quot;]], not the body of f(). B.3.7 Compiled code is not reproducibly tracked. Some R functions use .Call() to run compiled code in the backend. The R code in these functions is tracked, but not the compiled object called with .Call(), nor its C/C++/Fortran source. B.3.8 Directories (folders) are not reproducibly tracked. In your drake plan, you can use file_in(), file_out(), and knitr_in() to assert that some targets/imports are external files. However, entire directories (i.e. folders) cannot be reproducibly tracked this way. Please see issue 12 for a discussion. B.3.9 Packages are not tracked as dependencies. drake may import functions from packages, but the packages themselves are not tracked as dependencies. For this, you will need other tools that support reproducibility beyond the scope of drake. Packrat creates a tightly-controlled local library of packages to extend the shelf life of your project. And with Docker, you can execute your project on a virtual machine to ensure platform independence. Together, packrat and Docker can help others reproduce your work even if they have different software and hardware. B.4 High-performance computing B.4.1 Calling mclapply() within targets The following workflow fails because make() locks your environment and mclapply() tries to add new variables to it. plan &lt;- drake_plan(parallel::mclapply(1:8, sqrt, mc.cores = 2)) make(plan) But there are plenty of workarounds, including make(plan, lock_envir = FALSE) and other parallel computing functions like parLapply() or furrr::future_map(). See this comment and the ensuing discussion. B.4.2 Zombie processes Some parallel backends, particularly make(parallelism = &quot;future&quot;) with future::multicore, may create zombie processes. Zombie children are not usually harmful, but you may wish to kill them yourself. The following function by Carl Boneri should work on Unix-like systems. For a discussion, see drake issue 116. fork_kill_zombies &lt;- function(){ require(inline) includes &lt;- &quot;#include &lt;sys/wait.h&gt;&quot; code &lt;- &quot;int wstat; while (waitpid(-1, &amp;wstat, WNOHANG) &gt; 0) {};&quot; wait &lt;- inline::cfunction( body = code, includes = includes, convention = &quot;.C&quot; ) invisible(wait()) } B.5 Storage B.5.1 Projects hosted on Dropbox and similar platforms If download a drake project from Dropbox, you may get an error like the one in issue 198: cache pathto/.drake connect 61 imports: ... connect 200 targets: ... Error in rawToChar(as.raw(x)) : embedded nul in string: 'initial_drake_version\\0\\0\\x9a\\x9d\\xdc\\0J\\xe9\\0\\0\\0(\\x9d\\xf9brם\\0\\xca)\\0\\0\\xb4\\xd7\\0\\0\\0\\0\\xb9' In addition: Warning message: In rawToChar(as.raw(x)) : out-of-range values treated as 0 in coercion to raw This is probably because Dropbox generates a bunch of “conflicted copy” files when file transfers do not go smoothly. This confuses storr, drake’s caching backend. keys/config/aG9vaw (Sandy Sum's conflicted copy 2018-01-31) keys/config/am9icw (Sandy Sum's conflicted copy 2018-01-31) keys/config/c2VlZA (Sandy Sum's conflicted copy 2018-01-31) Just remove these files using drake_gc() and proceed with your work. cache &lt;- get_cache() drake_gc(cache) B.5.2 Cache customization is limited The storage guide describes how storage works in drake. As explained near the end of that chapter, you can plug custom storr caches into make(). However, non-RDS caches such as storr_dbi() may not work with most forms of parallel computing. The storr::storr_dbi() cache and many others are not thread-safe. Either Set parallelism = &quot;clustermq_staged&quot; in make(), or Set parallelism = &quot;future&quot; with caching = &quot;master&quot; in make(), or Use no parallel computing at all. B.5.3 Runtime predictions In predict_runtime() and rate_limiting_times(), drake only accounts for the targets with logged build times. If some targets have not been timed, drake throws a warning and prints the untimed targets. "]
]
