[
["index.html", "The drake R Package User Manual Chapter 1 Introduction 1.1 The drake R package 1.2 Installation 1.3 Why drake? 1.4 Documentation 1.5 Help and troubleshooting 1.6 Similar work 1.7 Acknowledgements", " The drake R Package User Manual Will Landau, Kirill Müller, Alex Axthelm, Jasper Clarkberg, Lorenz Walthert Copyright Eli Lilly and Company Chapter 1 Introduction 1.1 The drake R package Data analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. How much of that valuable output can you keep, and how much do you need to update? How much runtime must you endure all over again? For projects in R, the drake package can help. It analyzes your workflow, skips steps with up-to-date results, and orchestrates the rest with optional distributed computing. At the end, drake provides evidence that your results match the underlying code and data, which increases your ability to trust your research. 1.2 Installation You can choose among different versions of drake. The latest CRAN release may be more convenient to install, but this manual is kept up to date with the GitHub version, so some features described here may not yet be available on CRAN. # Install the latest stable release from CRAN. install.packages(&quot;drake&quot;) # Alternatively, install the development version from GitHub. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;ropensci/drake&quot;) 1.3 Why drake? 1.3.1 What gets done stays done. Too many data science projects follow a Sisyphean loop: Launch the code. Wait while it runs. Discover an issue. Restart from scratch. Ordinarily, it is hard to avoid restarting from scratch. But with drake, you can automatically Launch the parts that changed since last time. Skip the rest. 1.3.2 Reproducibility with confidence The R community emphasizes reproducibility. Traditional themes include scientific replicability, literate programming with knitr, and version control with git. But internal consistency is important too. Reproducibility carries the promise that your output matches the code and data you say you used. With the exception of non-default triggers and hasty mode, drake strives to keep this promise. 1.3.2.1 Evidence Suppose you are reviewing someone else’s data analysis project for reproducibility. You scrutinize it carefully, checking that the datasets are available and the documentation is thorough. But could you re-create the results without the help of the original author? With drake, it is quick and easy to find out. make(plan) config &lt;- drake_config(plan) outdated(config) With everything already up to date, you have tangible evidence of reproducibility. Even though you did not re-create the results, you know the results are re-creatable. They faithfully show what the code is producing. Given the right package environment and system configuration, you have everything you need to reproduce all the output by yourself. 1.3.2.2 Ease When it comes time to actually rerun the entire project, you have much more confidence. Starting over from scratch is trivially easy. clean() # Remove the original author&#39;s results. make(plan) # Independently re-create the results from the code and input data. 1.3.2.3 Independent replication With even more evidence and confidence, you can invest the time to independently replicate the original code base if necessary. Up until this point, you relied on basic drake functions such as make(), so you may not have needed to peek at any substantive author-defined code in advance. In that case, you can stay usefully ignorant as you reimplement the original author’s methodology. In other words, drake could potentially improve the integrity of independent replication. 1.3.2.4 Readability and transparency Ideally, independent observers should be able to read your code and understand it. drake helps in several ways. The drake plan explicitly outlines the steps of the analysis, and vis_drake_graph() visualizes how those steps depend on each other. drake takes care of the parallel scheduling and high-performance computing (HPC) for you. That means the HPC code is no longer tangled up with the code that actually expresses your ideas. You can generate large collections of targets without necessarily changing your code base of imported functions, another nice separation between the concepts and the execution of your workflow 1.3.3 Aggressively scale up. Not every project can complete in a single R session on your laptop. Some projects need more speed or computing power. Some require a few local processor cores, and some need large high-performance computing systems. But parallel computing is hard. Your tables and figures depend on your analysis results, and your analyses depend on your datasets, so some tasks must finish before others even begin. drake knows what to do. Parallelism is implicit and automatic. See the high-performance computing guide for all the details. # Use the spare cores on your local machine. make(plan, jobs = 4) # Or scale up to a supercomputer. drake_batchtools_tmpl_file(&quot;slurm&quot;) # https://slurm.schedmd.com/ library(future.batchtools) future::plan(batchtools_slurm, template = &quot;batchtools.slurm.tmpl&quot;, workers = 100) make(plan, parallelism = &quot;future_lapply&quot;) 1.4 Documentation The main resources to learn drake are The user manual, which contains a friendly introduction and several long-form tutorials. The documentation website, which serves as a quicker reference. Kirill Müller’s drake workshop from March 5, 2018. 1.4.1 Cheat sheet Thanks to Kirill for preparing a drake cheat sheet for the workshop. 1.4.2 Frequently asked questions The FAQ page is an index of links to appropriately-labeled issues on GitHub. To contribute, please submit a new issue and ask that it be labeled as a frequently asked question. 1.4.3 Function reference The reference section lists all the available functions. Here are the most important ones. drake_plan(): create a workflow data frame (like my_plan). make(): build your project. loadd(): load one or more built targets into your R session. readd(): read and return a built target. drake_config(): create a master configuration list for other user-side functions. vis_drake_graph(): show an interactive visual network representation of your workflow. outdated(): see which targets will be built in the next make(). deps(): check the dependencies of a command or function. failed(): list the targets that failed to build in the last make(). diagnose(): return the full context of a build, including errors, warnings, and messages. 1.4.4 Tutorials Thanks to Kirill for constructing two interactive learnr tutorials: one supporting drake itself, and a prerequisite walkthrough of the cooking package. 1.4.5 Examples Here are some real-world applications of drake in the wild. sol-eng/tensorflow-w-r efcaguab/demografia-del-voto efcaguab/great-white-shark-nsw IndianaCHE/Detailed-SSP-Reports tiernanmartin/home-and-hope There are also multiple drake-powered example projects available here, ranging from beginner-friendly stubs to demonstrations of high-performance computing. You can generate the files for a project with drake_example() (e.g. drake_example(&quot;gsp&quot;)), and you can list the available projects with drake_examples(). You can contribute your own example project with a fork and pull request. 1.4.6 Presentations Author Venue Date Materials Amanda Dobbyn R-Ladies NYC 2019-02-12 slides, source Will Landau Harvard DataFest 2019-01-22 slides, source Karthik Ram RStudio Conference 2019-01-18 video, slides, resources Sina Rüeger Geneva R User Group 2018-10-04 slides, example code Will Landau R in Pharma 2018-08-16 video, slides, source Christine Stawitz R-Ladies Seattle 2018-06-25 materials Kirill Müller Swiss Institute of Bioinformatics 2018-03-05 workshop, slides, source, exercises Kirill Müller RStudio Conference 2018-02-01 slides, source 1.4.7 Context and history For context and history, check out this post on the rOpenSci blog and episode 22 of the R Podcast. 1.5 Help and troubleshooting The following resources document many known issues and challenges. Frequently-asked questions. Cautionary notes and edge cases Debugging and testing drake projects Other known issues (please search both open and closed ones). If you are still having trouble, please submit a new issue with a bug report or feature request, along with a minimal reproducible example where appropriate. The GitHub issue tracker is mainly intended for bug reports and feature requests. While questions about usage etc. are also highly encouraged, you may alternatively wish to post to Stack Overflow and use the drake-r-package tag. 1.6 Similar work 1.6.1 GNU Make The original idea of a time-saving reproducible build system extends back at least as far as GNU Make, which still aids the work of data scientists as well as the original user base of complied language programmers. In fact, the name “drake” stands for “Data Frames in R for Make”. Make is used widely in reproducible research. Below are some examples from Karl Broman’s website. Bostock, Mike (2013). “A map of flowlines from NHDPlus.” https://github.com/mbostock/us-rivers. Powered by the Makefile at https://github.com/mbostock/us-rivers/blob/master/Makefile. Broman, Karl W (2012). “Halotype Probabilities in Advanced Intercross Populations.” G3 2(2), 199-202.Powered by the Makefile at https://github.com/kbroman/ailProbPaper/blob/master/Makefile. Broman, Karl W (2012). “Genotype Probabilities at Intermediate Generations in the Construction of Recombinant Inbred Lines.” *Genetics 190(2), 403-412. Powered by the Makefile at https://github.com/kbroman/preCCProbPaper/blob/master/Makefile. Broman, Karl W and Kim, Sungjin and Sen, Saunak and Ane, Cecile and Payseur, Bret A (2012). “Mapping Quantitative Trait Loci onto a Phylogenetic Tree.” Genetics 192(2), 267-279. Powered by the Makefile at https://github.com/kbroman/phyloQTLpaper/blob/master/Makefile. There are several reasons for R users to prefer drake instead. drake already has a Make-powered parallel backend. Just run make(..., parallelism = &quot;Makefile&quot;, jobs = 2) to enjoy most of the original benefits of Make itself. Improved scalability. With Make, you must write a potentially large and cumbersome Makefile by hand. But with drake, you can use wildcard templating to automatically generate massive collections of targets with minimal code. Lower overhead for light-weight tasks. For each Make target that uses R, a brand new R session must spawn. For projects with thousands of small targets, that means more time may be spent loading R sessions than doing the actual work. With make(..., parallelism = &quot;mclapply, jobs = 4&quot;), drake launches 4 persistent workers up front and efficiently processes the targets in R. Convenient organization of output. With Make, the user must save each target as a file. drake saves all the results for you automatically in a storr cache so you do not have to micromanage the results. 1.6.2 Remake drake overlaps with its direct predecessor, remake. In fact, drake owes its core ideas to remake and Rich Fitzjohn. Remake’s development repository lists several real-world applications. drake surpasses remake in several important ways, including but not limited to the following. High-performance computing. Remake has no native parallel computing support. drake, on the other hand, has a thorough selection of parallel computing technologies and scheduling algorithms. Thanks to future, future.batchtools, and batchtools, it is straightforward to configure a drake project for most popular job schedulers, such as SLURM, TORQUE, and the Grid Engine, as well as systems contained in Docker images. A friendly interface. In remake, the user must manually write a YAML configuration file to arrange the steps of a workflow, which leads to some of the same scalability problems as Make. drake’s data-frame-based interface and wildcard templating functionality easily generate workflows at scale. Thorough documentation. drake contains thorough user manual, a reference website, a comprehensive README, examples in the help files of user-side functions, and accessible example code that users can write with drake::example_drake(). Active maintenance. drake is actively developed and maintained, and issues are usually addressed promptly. Presence on CRAN. At the time of writing, drake is available on CRAN, but remake is not. 1.6.3 Memoise Memoization is the strategic caching of the return values of functions. Every time a memoized function is called with a new set of arguments, the return value is saved for future use. Later, whenever the same function is called with the same arguments, the previous return value is salvaged, and the function call is skipped to save time. The memoise package is an excellent implementation of memoization in R. However, memoization does not go far enough. In reality, the return value of a function depends not only on the function body and the arguments, but also on any nested functions and global variables, the dependencies of those dependencies, and so on upstream. drake surpasses memoise because it uses the entire dependency network graph of a project to decide which pieces need to be rebuilt and which ones can be skipped. 1.6.4 Knitr and R Markdown Much of the R community uses knitr and R Markdown for reproducible research. The idea is to intersperse code chunks in an R Markdown or *.Rnw file and then generate a dynamic report that weaves together code, output, and prose. Knitr is not designed to be a serious pipeline toolkit, and it should not be the primary computational engine for medium to large data analysis projects. Knitr scales far worse than Make or remake. The whole point is to consolidate output and prose, so it deliberately lacks the essential modularity. There is no obvious high-performance computing support. While there is a way to skip chunks that are already up to date (with code chunk options cache and autodep), this functionality is not the focus of knitr. It is deactivated by default, and remake and drake are more dependable ways to skip work that is already up to date. drake was designed to manage the entire workflow with knitr reports as targets. The strategy is analogous for knitr reports within remake projects. 1.6.5 Factual’s Drake Factual’s Drake is similar in concept, but the development effort is completely unrelated to the drake R package. 1.6.6 Other pipeline toolkits There are countless other successful pipeline toolkits. The drake package distinguishes itself with its R-focused approach, Tidyverse-friendly interface, and a thorough selection of parallel computing technologies and scheduling algorithms. 1.7 Acknowledgements Special thanks to Jarad Niemi, my advisor from graduate school, for first introducing me to the idea of Makefiles for research. He originally set me down the path that led to drake. Many thanks to Julia Lowndes, Ben Marwick, and Peter Slaughter for reviewing drake for rOpenSci, and to Maëlle Salmon for such active involvement as the editor. Thanks also to the following people for contributing early in development. Alex Axthelm Chan-Yub Park Daniel Falster Eric Nantz Henrik Bengtsson Ian Watson Jasper Clarkberg Kendon Bell Kirill Müller Credit for images is attributed here. "],
["main.html", "Chapter 2 The main example 2.1 Set the stage. 2.2 Make your results. 2.3 Go back and fix things. 2.4 Try it yourself!", " Chapter 2 The main example A typical data analysis workflow is a sequence of data transformations. Raw data becomes tidy data, then turns into fitted models, summaries, and reports. Other analyses are usually variations of this pattern, and drake can easily accommodate them. 2.1 Set the stage. To set up a project, load your packages, library(drake) library(dplyr) library(ggplot2) load your custom functions, create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram() } check any supporting files (optional), ## Get the files with drake_example(&quot;main&quot;). file.exists(&quot;raw_data.xlsx&quot;) #&gt; [1] TRUE file.exists(&quot;report.Rmd&quot;) #&gt; [1] TRUE and plan what you are going to do. plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 hist create_plot(data) … #&gt; 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… Optionally, visualize your workflow to make sure you set it up correctly. The graph is interactive, so you can click, drag, hover, zoom, and explore. config &lt;- drake_config(plan) vis_drake_graph(config) 2.2 Make your results. So far, we have just been setting the stage. Use make() to do the real work. Targets are built in the correct order regardless of the row order of plan. make(plan) #&gt; target raw_data #&gt; target data #&gt; target fit #&gt; target hist #&gt; target report Except for output files like report.html, your output is stored in a hidden .drake/ folder. Reading it back is easy. readd(data) # See also loadd(). #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; # … with 140 more rows The graph shows everything up to date. vis_drake_graph(config) 2.3 Go back and fix things. You may look back on your work and see room for improvement, but it’s all good! The whole point of drake is to help you go back and change things quickly and painlessly. For example, we forgot to give our histogram a bin width. readd(hist) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. So let’s fix the plotting function. create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + theme_gray(20) } drake knows which results are affected. vis_drake_graph(config) The next make() just builds hist and report. No point in wasting time on the data or model. make(plan) #&gt; target hist #&gt; target report loadd(hist) hist 2.4 Try it yourself! Use drake_example(&quot;main&quot;) to get all the materials. "],
["plans.html", "Chapter 3 drake plans 3.1 What is a drake plan? 3.2 Plans are similar to R scripts. 3.3 So why do we use plans? 3.4 Special custom columns in your plan. 3.5 Large plans 3.6 Create large plans the old way", " Chapter 3 drake plans 3.1 What is a drake plan? Adrake plan is a data frame with columns named target and command. Each target is an R object, and each command is an expression to produce it.1 The drake_plan() function is the best way to set up plans.2 Recall the plan from our previous example: plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 hist create_plot(data) … #&gt; 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… drake_plan() does not run the workflow, it only creates the plan. To build the actual targets, we need to run make(). Creating the plan is like writing an R script, and running make(your_plan) is like calling source(&quot;your_script.R&quot;). 3.2 Plans are similar to R scripts. Your drake plan is like a top-level R script that runs everything from end to end. In fact, you can convert back and forth between plans and scripts using functions plan_to_code() and code_to_plan() (with some caveats). plan_to_code(plan, &quot;new_script.R&quot;) #&gt; Loading required namespace: styler cat(readLines(&quot;new_script.R&quot;), sep = &quot;\\n&quot;) #&gt; raw_data &lt;- readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) #&gt; data &lt;- raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) #&gt; fit &lt;- lm(Sepal.Width ~ Petal.Width + Species, data) #&gt; hist &lt;- create_plot(data) #&gt; report &lt;- rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), #&gt; output_file = file_out(&quot;report.html&quot;), #&gt; quiet = TRUE #&gt; ) code_to_plan(&quot;new_script.R&quot;) #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 4 hist create_plot(data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… And plan_to_notebook() turns plans into R notebooks. plan_to_notebook(plan, &quot;new_notebook.Rmd&quot;) cat(readLines(&quot;new_notebook.Rmd&quot;), sep = &quot;\\n&quot;) #&gt; --- #&gt; title: &quot;My Notebook&quot; #&gt; output: html_notebook #&gt; --- #&gt; #&gt; ```{r my_code} #&gt; raw_data &lt;- readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) #&gt; data &lt;- raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) #&gt; fit &lt;- lm(Sepal.Width ~ Petal.Width + Species, data) #&gt; hist &lt;- create_plot(data) #&gt; report &lt;- rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), #&gt; output_file = file_out(&quot;report.html&quot;), #&gt; quiet = TRUE #&gt; ) #&gt; ``` code_to_plan(&quot;new_notebook.Rmd&quot;) #&gt; # A tibble: 5 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 raw_data readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) … #&gt; 2 data raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … #&gt; 3 fit lm(Sepal.Width ~ Petal.Width + Species, data) … #&gt; 4 hist create_plot(data) … #&gt; 5 report rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), output_file = file_ou… 3.3 So why do we use plans? If you have ever waited more than 10 minutes for an R script to finish, then you know the frustration of having to rerun the whole thing every time you make a change. Plans make life easier. 3.3.1 Plans chop up the work into pieces. Some targets may need an update while others may not. In our first example, make() was smart enough to skip the data cleaning step and just rebuild the plot and report. drake and its plans compartmentalize the work, and this can save you from wasted effort in the long run. 3.3.2 drake uses plans to schedule you work. make() automatically learns the build order of your targets and how to run them in parallel. The underlying magic is static code analysis, which automatically detects the dependencies of each target without having to run its command. create_plot &lt;- function(data) { ggplot(data, aes_string(x = &quot;Petal.Width&quot;, fill = &quot;Species&quot;)) + geom_histogram(bins = 20) } deps_code(create_plot) #&gt; # A tibble: 3 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 geom_histogram globals #&gt; 2 ggplot globals #&gt; 3 aes_string globals deps_code(quote(create_plot(datasets::iris))) #&gt; # A tibble: 2 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 create_plot globals #&gt; 2 datasets::iris namespaced Because of the dependency relationships, row order does not matter once the plan is fully defined. The following plan declares file before plot. small_plan &lt;- drake_plan( file = ggsave(file_out(&quot;plot.png&quot;), plot, width = 7, height = 5), plot = create_plot(datasets::iris) ) But file actually depends on plot. small_config &lt;- drake_config(small_plan) vis_drake_graph(small_config) So make() builds plot first. library(ggplot2) make(small_plan) #&gt; target plot #&gt; target file 3.4 Special custom columns in your plan. You can add other columns besides the required target and command. cbind(small_plan, cpu = c(1, 2)) #&gt; target command cpu #&gt; 1 file ggsave(file_out(&quot;plot.png&quot;), plot, width = 7, height = 5) 1 #&gt; 2 plot create_plot(datasets::iris) 2 Within drake_plan(), target() lets you create any custom column except target, command, and transform, the last of which has a special meaning. drake_plan( file = target( ggsave(file_out(&quot;plot.png&quot;), plot), elapsed = 10 ), create_plot(datasets::iris) ) #&gt; # A tibble: 2 x 3 #&gt; target command elapsed #&gt; &lt;chr&gt; &lt;expr&gt; &lt;dbl&gt; #&gt; 1 file ggsave(file_out(&quot;plot.png&quot;), plot) 10 #&gt; 2 drake_target_1 create_plot(datasets::iris) NA The following columns have special meanings for make(). elapsed and cpu: number of seconds to wait for the target to build before timing out (elapsed for elapsed time and cpu for CPU time). priority: for parallel computing, optionally rank the targets according to priority in the scheduler. resources: target-specific lists of resources for a computing cluster. See the advanced options in the parallel computing chapter for details. retries: number of times to retry building a target in the event of an error. trigger: rule to decide whether a target needs to run. See the trigger chapter to learn more. 3.5 Large plans drake version 7.0.0 will introduce new experimental syntax to make it easier to create plans. To try it out before the next CRAN release, install the current development version from GitHub. install.packages(&quot;remotes&quot;) library(remotes) install_github(&quot;ropensci/drake&quot;) 3.5.1 How to create large plans Ordinarily, drake_plan() requires you to write out all the targets one-by-one. This is a literal pain. drake_plan( data = get_data(), analysis_1_1 = fit_model_x(data, mean = 1, sd = 1), analysis_2_1 = fit_model_x(data, mean = 2, sd = 1), analysis_5_1 = fit_model_x(data, mean = 5, sd = 1), analysis_10_1 = fit_model_x(data, mean = 10, sd = 1), analysis_100_1 = fit_model_x(data, mean = 100, sd = 1), analysis_1000_1 = fit_model_x(data, mean = 1000, sd = 1), analysis_1_2 = fit_model_x(data, mean = 1, sd = 2), analysis_2_2 = fit_model_x(data, mean = 2, sd = 2), analysis_5_2 = fit_model_x(data, mean = 5, sd = 2), analysis_10_2 = fit_model_x(data, mean = 10, sd = 2), analysis_100_2 = fit_model_x(data, mean = 100, sd = 2), analysis_1000_2 = fit_model_x(data, mean = 1000, sd = 2), # UUUGGGHH my wrists are cramping! :( ... ) Transformations reduce typing, especially when combined with tidy evaluation (!!). lots_of_sds &lt;- as.numeric(1:1e3) drake_plan( data = get_data(), analysis = target( fun(data, mean = mean_val, sd = sd_val), transform = cross(mean_val = c(2, 5, 10, 100, 1000), sd_val = !!lots_of_sds) ) ) #&gt; # A tibble: 5,001 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_data() #&gt; 2 analysis_2_1 fun(data, mean = 2, sd = 1) #&gt; 3 analysis_5_1 fun(data, mean = 5, sd = 1) #&gt; 4 analysis_10_1 fun(data, mean = 10, sd = 1) #&gt; 5 analysis_100_1 fun(data, mean = 100, sd = 1) #&gt; 6 analysis_1000_1 fun(data, mean = 1000, sd = 1) #&gt; 7 analysis_2_2 fun(data, mean = 2, sd = 2) #&gt; 8 analysis_5_2 fun(data, mean = 5, sd = 2) #&gt; 9 analysis_10_2 fun(data, mean = 10, sd = 2) #&gt; 10 analysis_100_2 fun(data, mean = 100, sd = 2) #&gt; # … with 4,991 more rows Behind the scenes during a transformation, drake_plan() creates new columns to track what is happening. You can see them with trace = TRUE. drake_plan( data = get_data(), analysis = target( analyze(data, mean, sd), transform = map(mean = c(3, 4), sd = c(1, 2)) ), trace = TRUE ) #&gt; # A tibble: 3 x 5 #&gt; target command mean sd analysis #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 data get_data() &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 analysis_3_1 analyze(data, 3, 1) 3 1 analysis_3_1 #&gt; 3 analysis_4_2 analyze(data, 4, 2) 4 2 analysis_4_2 Because of those columns, you can chain transformations together in complex pipelines. plan1 &lt;- drake_plan( small = get_small_data(), large = get_large_data(), analysis = target( # Analyze each dataset once with a different mean. analyze(data, mean), transform = map(data = c(small, large), mean = c(1, 2)) ), # Calculate 2 different performance metrics on every model fit. metric = target( metric_fun(analysis), # mse = mean squared error, mae = mean absolute error. # Assume these are functions you write. transform = cross(metric_fun = c(mse, mae), analysis) ), # Summarize the performance metrics for each dataset. summ_data = target( summary(metric), transform = combine(metric, .by = data) ), # Same, but for each metric type. summ_metric = target( summary(metric), transform = combine(metric, .by = metric_fun) ) ) plan1 #&gt; # A tibble: 12 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 small get_small_data() … #&gt; 2 large get_large_data() … #&gt; 3 analysis_small_1 analyze(small, 1) … #&gt; 4 analysis_large_2 analyze(large, 2) … #&gt; 5 metric_mse_analysis_l… mse(analysis_large_2) … #&gt; 6 metric_mae_analysis_l… mae(analysis_large_2) … #&gt; 7 metric_mse_analysis_s… mse(analysis_small_1) … #&gt; 8 metric_mae_analysis_s… mae(analysis_small_1) … #&gt; 9 summ_data_large summary(metric_mse_analysis_large_2, metric_mae_… #&gt; 10 summ_data_small summary(metric_mse_analysis_small_1, metric_mae_… #&gt; 11 summ_metric_mae summary(metric_mae_analysis_large_2, metric_mae_… #&gt; 12 summ_metric_mse summary(metric_mse_analysis_large_2, metric_mse_… config1 &lt;- drake_config(plan1) vis_drake_graph(config1) And you can write the transformations in any order. The following plan is equivalent to plan1 despite the rearranged rows. plan2 &lt;- drake_plan( # Calculate 2 different performance metrics on every model fit. summ_metric = target( summary(metric), transform = combine(metric, .by = metric_fun) ), metric = target( metric_fun(analysis), # mse = mean squared error, mae = mean absolute error. # Assume these are functions you write. transform = cross(metric_fun = c(mse, mae), analysis) ), small = get_small_data(), analysis = target( # Analyze each dataset once with a different mean. analyze(data, mean), transform = map(data = c(small, large), mean = c(1, 2)) ), # Summarize the performance metrics for each dataset. summ_data = target( summary(metric), transform = combine(metric, .by = data) ), large = get_large_data() # Same, but for each metric type. ) plan2 #&gt; # A tibble: 12 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 summ_metric_mae summary(metric_mae_analysis_large_2, metric_mae_… #&gt; 2 summ_metric_mse summary(metric_mse_analysis_large_2, metric_mse_… #&gt; 3 metric_mse_analysis_l… mse(analysis_large_2) … #&gt; 4 metric_mae_analysis_l… mae(analysis_large_2) … #&gt; 5 metric_mse_analysis_s… mse(analysis_small_1) … #&gt; 6 metric_mae_analysis_s… mae(analysis_small_1) … #&gt; 7 small get_small_data() … #&gt; 8 analysis_small_1 analyze(small, 1) … #&gt; 9 analysis_large_2 analyze(large, 2) … #&gt; 10 summ_data_large summary(metric_mse_analysis_large_2, metric_mae_… #&gt; 11 summ_data_small summary(metric_mse_analysis_small_1, metric_mae_… #&gt; 12 large get_large_data() … config2 &lt;- drake_config(plan2) vis_drake_graph(config2) 3.5.2 The types of transformations drake supports three types of transformations: map(), cross(), and combine(). These are not actual functions, but you can treat them as functions when you use them in drake_plan(). Each transformation takes after a function from the Tidyverse. drake Tidyverse analogue map() pmap() from purrr cross() crossing() from tidyr combine() summarize() from dplyr 3.5.2.1 map() map() creates a new target for each row in a grid. drake_plan( x = target( simulate_data(center, scale), transform = map(center = c(2, 1, 0), scale = c(3, 2, 1)) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_2_3 simulate_data(2, 3) #&gt; 2 x_1_2 simulate_data(1, 2) #&gt; 3 x_0_1 simulate_data(0, 1) You can supply your own custom grid using the .data argument. Note the use of !! below. my_grid &lt;- tibble( sim_function = c(&quot;rnrom&quot;, &quot;rt&quot;, &quot;rcauchy&quot;), title = c(&quot;Normal&quot;, &quot;Student t&quot;, &quot;Cauchy&quot;) ) my_grid$sim_function &lt;- rlang::syms(my_grid$sim_function) drake_plan( x = target( simulate_data(sim_function, title, center, scale), transform = map( center = c(2, 1, 0), scale = c(3, 2, 1), .data = !!my_grid, .id = sim_function # for pretty target names ) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_rnrom simulate_data(rnrom, &quot;Normal&quot;, 2, 3) #&gt; 2 x_rt simulate_data(rt, &quot;Student t&quot;, 1, 2) #&gt; 3 x_rcauchy simulate_data(rcauchy, &quot;Cauchy&quot;, 0, 1) 3.5.2.2 Special considerations in map() map() column-binds variables together to create a grid. The lengths of those variables need to be conformable just as with data.frame(). drake_plan( x = target( simulate_data(center, scale), transform = map(center = c(2, 1, 0), scale = c(3, 2)) ) ) #&gt; Error: Failed to make a grid of grouping variables for map(). #&gt; Grouping variables in map() must have suitable lengths for coercion to a data frame. #&gt; Possibly uneven groupings detected in map(center = c(2, 1, 0), scale = c(3, 2)): #&gt; c(&quot;2&quot;, &quot;1&quot;, &quot;0&quot;) #&gt; c(&quot;3&quot;, &quot;2&quot;) Sometimes, the results are sensible when grouping variable lengths are multiples of each other, but be careful. drake_plan( x = target( simulate_data(center, scale), transform = map(center = c(2, 1, 0), scale = 4) ) ) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_2_4 simulate_data(2, 4) #&gt; 2 x_1_4 simulate_data(1, 4) #&gt; 3 x_0_4 simulate_data(0, 4) Things get tricker when drake reuses grouping variables from previous transformations. For example, below, each x_* target has an associated nrow value. So if you write transform = map(x), then nrow goes along for the ride. drake_plan( x = target( simulate_data(center), transform = map(center = c(1, 2)) ), y = target( process_data(x, center), transform = map(x) ), trace = TRUE # Adds extra columns for the grouping variables. ) #&gt; # A tibble: 4 x 5 #&gt; target command center x y #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x_1 simulate_data(1) 1 x_1 &lt;NA&gt; #&gt; 2 x_2 simulate_data(2) 2 x_2 &lt;NA&gt; #&gt; 3 y_x_1 process_data(x_1, 1) 1 x_1 y_x_1 #&gt; 4 y_x_2 process_data(x_2, 2) 2 x_2 y_x_2 But if other targets have centers’s of their own, drake_plan() may not know what to do with them. drake_plan( w = target( simulate_data(center), transform = map(center = c(3, 4)) ), x = target( simulate_data_2(center), transform = map(center = c(1, 2)) ), y = target( process_data(w, x, center), transform = map(w, x) ), trace = TRUE ) #&gt; # A tibble: 6 x 6 #&gt; target command center w x y #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 w_3 simulate_data(3) 3 w_3 &lt;NA&gt; &lt;NA&gt; #&gt; 2 w_4 simulate_data(4) 4 w_4 &lt;NA&gt; &lt;NA&gt; #&gt; 3 x_1 simulate_data_2(1) 1 &lt;NA&gt; x_1 &lt;NA&gt; #&gt; 4 x_2 simulate_data_2(2) 2 &lt;NA&gt; x_2 &lt;NA&gt; #&gt; 5 y_w_3_x_1 process_data(w_3, x_1, NA) &lt;NA&gt; w_3 x_1 y_w_3_x_1 #&gt; 6 y_w_4_x_2 process_data(w_4, x_2, NA) &lt;NA&gt; w_4 x_2 y_w_4_x_2 The problems is that there are 4 values of center and only two x_* targets (and two y_* targets). Even if you explicitly supply center to the transformation, map() can only takes the first two values. drake_plan( w = target( simulate_data(center), transform = map(center = c(3, 4)) ), x = target( simulate_data_2(center), transform = map(center = c(1, 2)) ), y = target( process_data(w, x, center), transform = map(w, x, center) ), trace = TRUE ) #&gt; # A tibble: 6 x 6 #&gt; target command center w x y #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 w_3 simulate_data(3) 3 w_3 &lt;NA&gt; &lt;NA&gt; #&gt; 2 w_4 simulate_data(4) 4 w_4 &lt;NA&gt; &lt;NA&gt; #&gt; 3 x_1 simulate_data_2(1) 1 &lt;NA&gt; x_1 &lt;NA&gt; #&gt; 4 x_2 simulate_data_2(2) 2 &lt;NA&gt; x_2 &lt;NA&gt; #&gt; 5 y_w_3_x_1_3 process_data(w_3, x_1, 3) 3 w_3 x_1 y_w_3_x_1_3 #&gt; 6 y_w_4_x_2_4 process_data(w_4, x_2, 4) 4 w_4 x_2 y_w_4_x_2_4 So please inspect the plan before you run it with make(). Once you have a drake_config() object, vis_drake_graph() and deps_target() can help. 3.5.2.3 cross() cross() creates a new target for each combination of argument values. drake_plan( x = target( simulate_data(nrow, ncol), transform = cross(nrow = c(1, 2, 3), ncol = c(4, 5)) ) ) #&gt; # A tibble: 6 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1_4 simulate_data(1, 4) #&gt; 2 x_2_4 simulate_data(2, 4) #&gt; 3 x_3_4 simulate_data(3, 4) #&gt; 4 x_1_5 simulate_data(1, 5) #&gt; 5 x_2_5 simulate_data(2, 5) #&gt; 6 x_3_5 simulate_data(3, 5) 3.5.2.4 combine() In combine(), you can insert multiple targets into individual commands. The closest comparison is the unquote-splice operator !!! from the Tidyverse. plan &lt;- drake_plan( data = target( sim_data(mean = x, sd = y), transform = map(x = c(1, 2), y = c(3, 4)) ), larger = target( bind_rows(data, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data) ) ) plan #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data_1_3 sim_data(mean = 1, sd = 3) … #&gt; 2 data_2_4 sim_data(mean = 2, sd = 4) … #&gt; 3 larger bind_rows(data_1_3, data_2_4, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% … drake_plan_source(plan) #&gt; drake_plan( #&gt; data_1_3 = sim_data(mean = 1, sd = 3), #&gt; data_2_4 = sim_data(mean = 2, sd = 4), #&gt; larger = bind_rows(data_1_3, data_2_4, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) config &lt;- drake_config(plan) vis_drake_graph(config) You can different groups of targets in the same command. plan &lt;- drake_plan( data_group1 = target( sim_data(mean = x, sd = y), transform = map(x = c(1, 2), y = c(3, 4)) ), data_group2 = target( pull_data(url), transform = map(url = c(&quot;example1.com&quot;, &quot;example2.com&quot;)) ), larger = target( bind_rows(data_group1, data_group2, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data_group1, data_group2) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_group1_1_3 = sim_data(mean = 1, sd = 3), #&gt; data_group1_2_4 = sim_data(mean = 2, sd = 4), #&gt; data_group2_.example1.com. = pull_data(&quot;example1.com&quot;), #&gt; data_group2_.example2.com. = pull_data(&quot;example2.com&quot;), #&gt; larger = bind_rows(data_group1_1_3, data_group1_2_4, data_group2_.example1.com., #&gt; data_group2_.example2.com., #&gt; .id = &quot;id&quot; #&gt; ) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) And as with group_by() from dplyr, you can create a separate aggregate for each combination of levels of the arguments. Just pass a symbol or vector of symbols to the optional .by argument of combine(). plan &lt;- drake_plan( data = target( sim_data(mean = x, sd = y, skew = z), transform = cross(x = c(1, 2), y = c(3, 4), z = c(5, 6)) ), combined = target( bind_rows(data, .id = &quot;id&quot;) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data, .by = c(x, y)) ) ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_1_3_5 = sim_data(mean = 1, sd = 3, skew = 5), #&gt; data_2_3_5 = sim_data(mean = 2, sd = 3, skew = 5), #&gt; data_1_4_5 = sim_data(mean = 1, sd = 4, skew = 5), #&gt; data_2_4_5 = sim_data(mean = 2, sd = 4, skew = 5), #&gt; data_1_3_6 = sim_data(mean = 1, sd = 3, skew = 6), #&gt; data_2_3_6 = sim_data(mean = 2, sd = 3, skew = 6), #&gt; data_1_4_6 = sim_data(mean = 1, sd = 4, skew = 6), #&gt; data_2_4_6 = sim_data(mean = 2, sd = 4, skew = 6), #&gt; combined_1_3 = bind_rows(data_1_3_5, data_1_3_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_2_3 = bind_rows(data_2_3_5, data_2_3_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_1_4 = bind_rows(data_1_4_5, data_1_4_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; combined_2_4 = bind_rows(data_2_4_5, data_2_4_6, .id = &quot;id&quot;) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400) #&gt; ) In your post-processing, you may need the values of x and y that underly data_1_3 and data_2_4. Solution: get the trace and the target names. We define a new plan plan &lt;- drake_plan( data = target( sim_data(mean = x, sd = y), transform = map(x = c(1, 2), y = c(3, 4)) ), larger = target( post_process(data, plan = ignore(plan)) %&gt;% arrange(sd) %&gt;% head(n = 400), transform = combine(data) ), trace = TRUE ) drake_plan_source(plan) #&gt; drake_plan( #&gt; data_1_3 = target( #&gt; command = sim_data(mean = 1, sd = 3), #&gt; x = &quot;1&quot;, #&gt; y = &quot;3&quot;, #&gt; data = &quot;data_1_3&quot; #&gt; ), #&gt; data_2_4 = target( #&gt; command = sim_data(mean = 2, sd = 4), #&gt; x = &quot;2&quot;, #&gt; y = &quot;4&quot;, #&gt; data = &quot;data_2_4&quot; #&gt; ), #&gt; larger = target( #&gt; command = post_process(data_1_3, data_2_4, plan = ignore(plan)) %&gt;% #&gt; arrange(sd) %&gt;% #&gt; head(n = 400), #&gt; larger = &quot;larger&quot; #&gt; ) #&gt; ) and a new function post_process &lt;- function(..., plan) { args &lt;- list(...) names(args) &lt;- all.vars(substitute(list(...))) trace &lt;- filter(plan, target %in% names(args)) # Do post-processing with args and trace. } 3.5.3 Grouping variables A grouping variable is an argument to map(), cross(), or combine() that identifies a sub-collection of target names. Grouping variables can be either literals or symbols. Symbols can be scalars or vectors, and you can pass them to transformations with or without argument names. 3.5.3.1 Literal arguments When you pass a grouping variable of literals, you must use an explicit argument name. One does not simply write map(c(1, 2)). drake_plan(x = target(sqrt(y), transform = map(y = c(1, 2)))) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1 sqrt(1) #&gt; 2 x_2 sqrt(2) And if you supply integer sequences the usual way, you may notice some rows are missing. drake_plan(x = target(sqrt(y), transform = map(y = 1:3))) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1 sqrt(1) #&gt; 2 x_3 sqrt(3) Tidy evaluation and as.numeric() make sure all the data points show up. y_vals &lt;- as.numeric(1:3) drake_plan(x = target(sqrt(y), transform = map(y = !!y_vals))) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_1 sqrt(1) #&gt; 2 x_2 sqrt(2) #&gt; 3 x_3 sqrt(3) Character vectors usually work without a hitch, and quotes are converted into dots to make valid target names. drake_plan(x = target(get_data(y), transform = map(y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)))) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_.a. get_data(&quot;a&quot;) #&gt; 2 x_.b. get_data(&quot;b&quot;) #&gt; 3 x_.c. get_data(&quot;c&quot;) y_vals &lt;- letters drake_plan(x = target(get_data(y), transform = map(y = !!y_vals))) #&gt; # A tibble: 26 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_.a. get_data(&quot;a&quot;) #&gt; 2 x_.b. get_data(&quot;b&quot;) #&gt; 3 x_.c. get_data(&quot;c&quot;) #&gt; 4 x_.d. get_data(&quot;d&quot;) #&gt; 5 x_.e. get_data(&quot;e&quot;) #&gt; 6 x_.f. get_data(&quot;f&quot;) #&gt; 7 x_.g. get_data(&quot;g&quot;) #&gt; 8 x_.h. get_data(&quot;h&quot;) #&gt; 9 x_.i. get_data(&quot;i&quot;) #&gt; 10 x_.j. get_data(&quot;j&quot;) #&gt; # … with 16 more rows 3.5.3.2 Named symbol arguments Symbols passed with explicit argument names define new groupings of existing targets on the fly, and only the map() and cross() transformations can accept them this ways. To generate long symbol lists, use the syms() function from the rlang package. Remember to use the tidy evaluation operator !! inside the transformation. vals &lt;- rlang::syms(letters) drake_plan(x = target(get_data(y), transform = map(y = !!vals))) #&gt; # A tibble: 26 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x_a get_data(a) #&gt; 2 x_b get_data(b) #&gt; 3 x_c get_data(c) #&gt; 4 x_d get_data(d) #&gt; 5 x_e get_data(e) #&gt; 6 x_f get_data(f) #&gt; 7 x_g get_data(g) #&gt; 8 x_h get_data(h) #&gt; 9 x_i get_data(i) #&gt; 10 x_j get_data(j) #&gt; # … with 16 more rows The new groupings carry over to downstream targets by default, which you can see with trace = TRUE. Below, the rows for targets w_x and w_y have entries in the and z column. drake_plan( x = abs(mean(rnorm(10))), y = abs(mean(rnorm(100, 1))), z = target(sqrt(val), transform = map(val = c(x, y))), w = target(val + 1, transform = map(val)), trace = TRUE ) #&gt; # A tibble: 6 x 5 #&gt; target command val z w #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x abs(mean(rnorm(10))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 y abs(mean(rnorm(100, 1))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 z_x sqrt(x) x z_x &lt;NA&gt; #&gt; 4 z_y sqrt(y) y z_y &lt;NA&gt; #&gt; 5 w_x x + 1 x z_x w_x #&gt; 6 w_y y + 1 y z_y w_y However, this is incorrect because w does not depend on z_x or z_y. So for w, you should write map(val = c(x, y)) instead of map(val) to tell drake to clear the trace. Then, you will see NAs in the z column for w_x and w_y, which is right and proper. drake_plan( x = abs(mean(rnorm(10))), y = abs(mean(rnorm(100, 1))), z = target(sqrt(val), transform = map(val = c(x, y))), w = target(val + 1, transform = map(val = c(x, y))), trace = TRUE ) #&gt; # A tibble: 6 x 5 #&gt; target command val z w #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x abs(mean(rnorm(10))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 y abs(mean(rnorm(100, 1))) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 z_x sqrt(x) x z_x &lt;NA&gt; #&gt; 4 z_y sqrt(y) y z_y &lt;NA&gt; #&gt; 5 w_x x + 1 x &lt;NA&gt; w_x #&gt; 6 w_y y + 1 y &lt;NA&gt; w_y 3.5.4 Tags Tags are special optional grouping variables. They are ignored while the transformation is happening and then added to the plan to help subsequent transformations. There are two types of tags: In-tags, which contain the target name you start with, and Out-tags, which contain the target names generated by the transformations. drake_plan( x = target( command, transform = map(y = c(1, 2), .tag_in = from, .tag_out = c(to, out)) ), trace = TRUE ) #&gt; # A tibble: 2 x 7 #&gt; target command y x from to out #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x_1 command 1 x_1 x x_1 x_1 #&gt; 2 x_2 command 2 x_2 x x_2 x_2 Subsequent transformations can use tags as grouping variables and add to existing tags. plan &lt;- drake_plan( prep_work = do_prep_work(), local = target( get_local_data(n, prep_work), transform = map(n = c(1, 2), .tag_in = data_source, .tag_out = data) ), online = target( get_online_data(n, prep_work, port = &quot;8080&quot;), transform = map(n = c(1, 2), .tag_in = data_source, .tag_out = data) ), summary = target( summarize(bind_rows(data, .id = &quot;data&quot;)), transform = combine(data, .by = data_source) ), munged = target( munge(bind_rows(data, .id = &quot;data&quot;)), transform = combine(data, .by = n) ) ) plan #&gt; # A tibble: 9 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 prep_work do_prep_work() #&gt; 2 local_1 get_local_data(1, prep_work) #&gt; 3 local_2 get_local_data(2, prep_work) #&gt; 4 online_1 get_online_data(1, prep_work, port = &quot;8080&quot;) #&gt; 5 online_2 get_online_data(2, prep_work, port = &quot;8080&quot;) #&gt; 6 summary_local summarize(bind_rows(local_1, local_2, .id = &quot;data&quot;)) #&gt; 7 summary_online summarize(bind_rows(online_1, online_2, .id = &quot;data&quot;)) #&gt; 8 munged_1 munge(bind_rows(local_1, online_1, .id = &quot;data&quot;)) #&gt; 9 munged_2 munge(bind_rows(local_2, online_2, .id = &quot;data&quot;)) config &lt;- drake_config(plan) vis_drake_graph(config) 3.6 Create large plans the old way drake provides several older utility that increase the flexibility of plan creation. drake_plan() map_plan() evaluate_plan() expand_plan() gather_by() reduce_by() gather_plan() reduce_plan() 3.6.1 map_plan() purrr-like functional programming is like looping, but cleaner. The idea is to iterate the same computation over multiple different data points. You write a function to do something once, and a map()-like helper invokes it on each point in your dataset. drake’s version of map() — or more precisely, pmap_df() — is map_plan(). In the following example, we want to know how well each pair covariates in the mtcars dataset can predict fuel efficiency (in miles per gallon). We will try multiple pairs of covariates using the same statistical analysis, so it is a great time for drake-flavored functional programming with map_plan(). As with its cousin, pmap_df(), map_plan() needs A function. A grid of function arguments. Our function fits a fuel efficiency model given a single pair of covariate names x1 and x2. my_model_fit &lt;- function(x1, x2, data){ lm(as.formula(paste(&quot;mpg ~&quot;, x1, &quot;+&quot;, x2)), data = data) } Our grid of function arguments is a data frame of possible values for x1, x2, and data. covariates &lt;- setdiff(colnames(mtcars), &quot;mpg&quot;) # Exclude the response variable. args &lt;- t(combn(covariates, 2)) # Take all possible pairs. colnames(args) &lt;- c(&quot;x1&quot;, &quot;x2&quot;) # The column names must be the argument names of my_model_fit() args &lt;- tibble::as_tibble(args) # Tibbles are so nice. args$data &lt;- &quot;mtcars&quot; args #&gt; # A tibble: 45 x 3 #&gt; x1 x2 data #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 cyl disp mtcars #&gt; 2 cyl hp mtcars #&gt; 3 cyl drat mtcars #&gt; 4 cyl wt mtcars #&gt; 5 cyl qsec mtcars #&gt; 6 cyl vs mtcars #&gt; 7 cyl am mtcars #&gt; 8 cyl gear mtcars #&gt; 9 cyl carb mtcars #&gt; 10 disp hp mtcars #&gt; # … with 35 more rows Each row of args corresponds to a call to my_model_fit(). To actually write out all those function calls, we use map_plan(). map_plan(args, my_model_fit) #&gt; The interface at https://ropenscilabs.github.io/drake-manual/plans.html#large-plans is better than evaluate_plan(), map_plan(), gather_by(), etc. #&gt; # A tibble: 45 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 my_model_fit_501e051c my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;disp&quot;, data = &quot;mtc… #&gt; 2 my_model_fit_d5de1d57 my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;hp&quot;, data = &quot;mtcar… #&gt; 3 my_model_fit_eac6cd8b my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;drat&quot;, data = &quot;mtc… #&gt; 4 my_model_fit_3900ef48 my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;wt&quot;, data = &quot;mtcar… #&gt; 5 my_model_fit_a2d797f6 my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;qsec&quot;, data = &quot;mtc… #&gt; 6 my_model_fit_f5c0ac7a my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;vs&quot;, data = &quot;mtcar… #&gt; 7 my_model_fit_507d2929 my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;am&quot;, data = &quot;mtcar… #&gt; 8 my_model_fit_b5f9a8a3 my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;gear&quot;, data = &quot;mtc… #&gt; 9 my_model_fit_8c4c5d9d my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;carb&quot;, data = &quot;mtc… #&gt; 10 my_model_fit_f9bb916e my_model_fit(x1 = &quot;disp&quot;, x2 = &quot;hp&quot;, data = &quot;mtca… #&gt; # … with 35 more rows We now have a plan, but it has a couple issues. The data argument should be a symbol. In other words, we want my_model_fit(data = mtcars), not my_model_fit(data = &quot;mtcars&quot;). So we use the syms() function from the rlang package turn args$data into a list of symbols. The default argument names are ugly, so we can add a new &quot;id&quot; column to args (or select one with the id argument of map_plan()). # Fixes (1) args$data &lt;- rlang::syms(args$data) # Alternative if each element of `args$data` is code with multiple symbols: # args$data &lt;- purrr::map(args$data, rlang::parse_expr) # Fixes (2) args$id &lt;- paste0(&quot;fit_&quot;, args$x1, &quot;_&quot;, args$x2) args #&gt; # A tibble: 45 x 4 #&gt; x1 x2 data id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 cyl disp &lt;sym&gt; fit_cyl_disp #&gt; 2 cyl hp &lt;sym&gt; fit_cyl_hp #&gt; 3 cyl drat &lt;sym&gt; fit_cyl_drat #&gt; 4 cyl wt &lt;sym&gt; fit_cyl_wt #&gt; 5 cyl qsec &lt;sym&gt; fit_cyl_qsec #&gt; 6 cyl vs &lt;sym&gt; fit_cyl_vs #&gt; 7 cyl am &lt;sym&gt; fit_cyl_am #&gt; 8 cyl gear &lt;sym&gt; fit_cyl_gear #&gt; 9 cyl carb &lt;sym&gt; fit_cyl_carb #&gt; 10 disp hp &lt;sym&gt; fit_disp_hp #&gt; # … with 35 more rows Much better. plan &lt;- map_plan(args, my_model_fit) plan #&gt; # A tibble: 45 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 fit_cyl_disp my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;disp&quot;, data = mtcars) #&gt; 2 fit_cyl_hp my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;hp&quot;, data = mtcars) #&gt; 3 fit_cyl_drat my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;drat&quot;, data = mtcars) #&gt; 4 fit_cyl_wt my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;wt&quot;, data = mtcars) #&gt; 5 fit_cyl_qsec my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;qsec&quot;, data = mtcars) #&gt; 6 fit_cyl_vs my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;vs&quot;, data = mtcars) #&gt; 7 fit_cyl_am my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;am&quot;, data = mtcars) #&gt; 8 fit_cyl_gear my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;gear&quot;, data = mtcars) #&gt; 9 fit_cyl_carb my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;carb&quot;, data = mtcars) #&gt; 10 fit_disp_hp my_model_fit(x1 = &quot;disp&quot;, x2 = &quot;hp&quot;, data = mtcars) #&gt; # … with 35 more rows We may also want to retain information about the constituent function arguments of each target. With map_plan(trace = TRUE), we can append the columns of args alongside the usual &quot;target&quot; and &quot;command&quot; columns of our plan. map_plan(args, my_model_fit, trace = TRUE) #&gt; # A tibble: 45 x 6 #&gt; target command x1 x2 data id #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;exp&gt; &lt;chr&gt; #&gt; 1 fit_cyl_d… my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;d… cyl disp mtca… fit_cyl_… #&gt; 2 fit_cyl_hp my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;h… cyl hp mtca… fit_cyl_… #&gt; 3 fit_cyl_d… my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;d… cyl drat mtca… fit_cyl_… #&gt; 4 fit_cyl_wt my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;w… cyl wt mtca… fit_cyl_… #&gt; 5 fit_cyl_q… my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;q… cyl qsec mtca… fit_cyl_… #&gt; 6 fit_cyl_vs my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;v… cyl vs mtca… fit_cyl_… #&gt; 7 fit_cyl_am my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;a… cyl am mtca… fit_cyl_… #&gt; 8 fit_cyl_g… my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;g… cyl gear mtca… fit_cyl_… #&gt; 9 fit_cyl_c… my_model_fit(x1 = &quot;cyl&quot;, x2 = &quot;c… cyl carb mtca… fit_cyl_… #&gt; 10 fit_disp_… my_model_fit(x1 = &quot;disp&quot;, x2 = &quot;… disp hp mtca… fit_disp… #&gt; # … with 35 more rows In any case, we can now fit our models. make(plan, verbose = FALSE) And inspect the output. readd(fit_cyl_disp) #&gt; #&gt; Call: #&gt; lm(formula = as.formula(paste(&quot;mpg ~&quot;, x1, &quot;+&quot;, x2)), data = data) #&gt; #&gt; Coefficients: #&gt; (Intercept) cyl disp #&gt; 34.66099 -1.58728 -0.02058 3.6.2 Wildcard templating In drake, you can write plans with wildcards. These wildcards are placeholders for text in commands. By iterating over the possible values of a wildcard, you can easily generate plans with thousands of targets. Let’s say you are running a simulation study, and you need to generate sets of random numbers from different distributions. plan &lt;- drake_plan( t = rt(1000, df = 5), normal = runif(1000, mean = 0, sd = 1) ) If you need to generate many datasets with different means, you may wish to write out each target individually. drake_plan( t = rt(1000, df = 5), normal_0 = runif(1000, mean = 0, sd = 1), normal_1 = runif(1000, mean = 1, sd = 1), normal_2 = runif(1000, mean = 2, sd = 1), normal_3 = runif(1000, mean = 3, sd = 1), normal_4 = runif(1000, mean = 4, sd = 1), normal_5 = runif(1000, mean = 5, sd = 1), normal_6 = runif(1000, mean = 6, sd = 1), normal_7 = runif(1000, mean = 7, sd = 1), normal_8 = runif(1000, mean = 8, sd = 1), normal_9 = runif(1000, mean = 9, sd = 1) ) But writing all that code manually is a pain and prone to human error. Instead, use evaluate_plan() plan &lt;- drake_plan( t = rt(1000, df = 5), normal = runif(1000, mean = mean__, sd = 1) ) evaluate_plan(plan, wildcard = &quot;mean__&quot;, values = 0:9) #&gt; # A tibble: 11 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 t rt(1000, df = 5) #&gt; 2 normal_0 runif(1000, mean = 0, sd = 1) #&gt; 3 normal_1 runif(1000, mean = 1, sd = 1) #&gt; 4 normal_2 runif(1000, mean = 2, sd = 1) #&gt; 5 normal_3 runif(1000, mean = 3, sd = 1) #&gt; 6 normal_4 runif(1000, mean = 4, sd = 1) #&gt; 7 normal_5 runif(1000, mean = 5, sd = 1) #&gt; 8 normal_6 runif(1000, mean = 6, sd = 1) #&gt; 9 normal_7 runif(1000, mean = 7, sd = 1) #&gt; 10 normal_8 runif(1000, mean = 8, sd = 1) #&gt; 11 normal_9 runif(1000, mean = 9, sd = 1) You can specify multiple wildcards at once. If multiple wildcards appear in the same command, you will get a new target for each unique combination of values. plan &lt;- drake_plan( t = rt(1000, df = df__), normal = runif(1000, mean = mean__, sd = sd__) ) evaluate_plan( plan, rules = list( mean__ = c(0, 1), sd__ = c(3, 4), df__ = 5:7 ) ) #&gt; # A tibble: 7 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 t_5 rt(1000, df = 5) #&gt; 2 t_6 rt(1000, df = 6) #&gt; 3 t_7 rt(1000, df = 7) #&gt; 4 normal_0_3 runif(1000, mean = 0, sd = 3) #&gt; 5 normal_0_4 runif(1000, mean = 0, sd = 4) #&gt; 6 normal_1_3 runif(1000, mean = 1, sd = 3) #&gt; 7 normal_1_4 runif(1000, mean = 1, sd = 4) Wildcards for evaluate_plan() do not need to have the double-underscore suffix. Any valid symbol will do. plan &lt;- drake_plan( t = rt(1000, df = .DF.), normal = runif(1000, mean = `{MEAN}`, sd = ..sd) ) evaluate_plan( plan, rules = list( &quot;`{MEAN}`&quot; = c(0, 1), ..sd = c(3, 4), .DF. = 5:7 ) ) #&gt; # A tibble: 7 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 t_5 rt(1000, df = 5) #&gt; 2 t_6 rt(1000, df = 6) #&gt; 3 t_7 rt(1000, df = 7) #&gt; 4 normal_0_3 runif(1000, mean = 0, sd = 3) #&gt; 5 normal_0_4 runif(1000, mean = 0, sd = 4) #&gt; 6 normal_1_3 runif(1000, mean = 1, sd = 3) #&gt; 7 normal_1_4 runif(1000, mean = 1, sd = 4) Set expand to FALSE to disable expansion. plan &lt;- drake_plan( t = rpois(samples__, lambda = mean__), normal = runif(samples__, mean = mean__) ) evaluate_plan( plan, rules = list( samples__ = c(50, 100), mean__ = c(1, 5) ), expand = FALSE ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 t rpois(50, lambda = 1) #&gt; 2 normal runif(100, mean = 5) Wildcard templating can sometimes be tricky. For example, suppose your project is to analyze school data, and your workflow checks several metrics of several schools. The idea is to write a drake plan with your metrics and let the wildcard templating expand over the available schools. hard_plan &lt;- drake_plan( credits = check_credit_hours(school__), students = check_students(school__), grads = check_graduations(school__), public_funds = check_public_funding(school__) ) evaluate_plan( hard_plan, rules = list(school__ = c(&quot;schoolA&quot;, &quot;schoolB&quot;, &quot;schoolC&quot;)) ) #&gt; # A tibble: 12 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 credits_schoolA check_credit_hours(schoolA) #&gt; 2 credits_schoolB check_credit_hours(schoolB) #&gt; 3 credits_schoolC check_credit_hours(schoolC) #&gt; 4 students_schoolA check_students(schoolA) #&gt; 5 students_schoolB check_students(schoolB) #&gt; 6 students_schoolC check_students(schoolC) #&gt; 7 grads_schoolA check_graduations(schoolA) #&gt; 8 grads_schoolB check_graduations(schoolB) #&gt; 9 grads_schoolC check_graduations(schoolC) #&gt; 10 public_funds_schoolA check_public_funding(schoolA) #&gt; 11 public_funds_schoolB check_public_funding(schoolB) #&gt; 12 public_funds_schoolC check_public_funding(schoolC) But what if some metrics do not make sense? For example, what if schoolC is a completely privately-funded school? With no public funds, check_public_funds(schoolC) may quit in error if we are not careful. This is where setting up drake plans requires a little creativity. In this case, we recommend that you use two wildcards: one for all the schools and another for just the public schools. The new plan has no twelfth row. plan_template &lt;- drake_plan( school = get_school_data(&quot;school__&quot;), credits = check_credit_hours(all_schools__), students = check_students(all_schools__), grads = check_graduations(all_schools__), public_funds = check_public_funding(public_schools__) ) evaluate_plan( plan = plan_template, rules = list( school__ = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), all_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;), public_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;) ) ) #&gt; # A tibble: 14 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 school_A get_school_data(&quot;A&quot;) #&gt; 2 school_B get_school_data(&quot;B&quot;) #&gt; 3 school_C get_school_data(&quot;C&quot;) #&gt; 4 credits_school_A check_credit_hours(school_A) #&gt; 5 credits_school_B check_credit_hours(school_B) #&gt; 6 credits_school_C check_credit_hours(school_C) #&gt; 7 students_school_A check_students(school_A) #&gt; 8 students_school_B check_students(school_B) #&gt; 9 students_school_C check_students(school_C) #&gt; 10 grads_school_A check_graduations(school_A) #&gt; 11 grads_school_B check_graduations(school_B) #&gt; 12 grads_school_C check_graduations(school_C) #&gt; 13 public_funds_school_A check_public_funding(school_A) #&gt; 14 public_funds_school_B check_public_funding(school_B) Thanks to Alex Axthelm for this use case in issue 235. 3.6.3 Wildcard clusters With evaluate_plan(trace = TRUE), you can generate columns that show how the targets were generated from the wildcards. plan_template &lt;- drake_plan( school = get_school_data(&quot;school__&quot;), credits = check_credit_hours(all_schools__), students = check_students(all_schools__), grads = check_graduations(all_schools__), public_funds = check_public_funding(public_schools__) ) plan &lt;- evaluate_plan( plan = plan_template, rules = list( school__ = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), all_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;), public_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;) ), trace = TRUE ) plan #&gt; # A tibble: 14 x 8 #&gt; target command school__ school___from all_schools__ all_schools___f… #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 schoo… get_sc… A school &lt;NA&gt; &lt;NA&gt; #&gt; 2 schoo… get_sc… B school &lt;NA&gt; &lt;NA&gt; #&gt; 3 schoo… get_sc… C school &lt;NA&gt; &lt;NA&gt; #&gt; 4 credi… check_… &lt;NA&gt; &lt;NA&gt; school_A credits #&gt; 5 credi… check_… &lt;NA&gt; &lt;NA&gt; school_B credits #&gt; 6 credi… check_… &lt;NA&gt; &lt;NA&gt; school_C credits #&gt; 7 stude… check_… &lt;NA&gt; &lt;NA&gt; school_A students #&gt; 8 stude… check_… &lt;NA&gt; &lt;NA&gt; school_B students #&gt; 9 stude… check_… &lt;NA&gt; &lt;NA&gt; school_C students #&gt; 10 grads… check_… &lt;NA&gt; &lt;NA&gt; school_A grads #&gt; 11 grads… check_… &lt;NA&gt; &lt;NA&gt; school_B grads #&gt; 12 grads… check_… &lt;NA&gt; &lt;NA&gt; school_C grads #&gt; 13 publi… check_… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 14 publi… check_… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; # … with 2 more variables: public_schools__ &lt;chr&gt;, #&gt; # public_schools___from &lt;chr&gt; And then when you visualize the dependency graph, you can cluster nodes based on the wildcard info. config &lt;- drake_config(plan) vis_drake_graph( config, group = &quot;all_schools__&quot;, clusters = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;) ) See the visualization guide for more details. 3.6.4 Non-wildcard functions 3.6.4.1 expand_plan() Sometimes, you just want multiple replicates of the same targets. plan &lt;- drake_plan( fake_data = simulate_from_model(), bootstrapped_data = bootstrap_from_real_data(real_data) ) expand_plan(plan, values = 1:3) #&gt; # A tibble: 6 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 fake_data_1 simulate_from_model() #&gt; 2 fake_data_2 simulate_from_model() #&gt; 3 fake_data_3 simulate_from_model() #&gt; 4 bootstrapped_data_1 bootstrap_from_real_data(real_data) #&gt; 5 bootstrapped_data_2 bootstrap_from_real_data(real_data) #&gt; 6 bootstrapped_data_3 bootstrap_from_real_data(real_data) 3.6.4.2 gather_plan() and gather_by() Other times, you want to combine multiple targets into one. plan &lt;- drake_plan( small = data.frame(type = &quot;small&quot;, x = rnorm(25), y = rnorm(25)), large = data.frame(type = &quot;large&quot;, x = rnorm(1000), y = rnorm(1000)) ) gather_plan(plan, target = &quot;combined&quot;) #&gt; # A tibble: 1 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 combined list(small = small, large = large) In this case, small and large are data frames, so it may be more convenient to combine the rows together. gather_plan(plan, target = &quot;combined&quot;, gather = &quot;rbind&quot;) #&gt; # A tibble: 1 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 combined rbind(small = small, large = large) See also gather_by() to gather multiple groups of targets based on other columns in the plan (e.g. from evaluate_plan(trace = TRUE)). 3.6.4.3 reduce_plan() and reduce_by() reduce_plan() is similar to gather_plan(), but it allows you to combine multiple targets together in pairs. This is useful if combining everything at once requires too much time or computer memory, or if you want to parallelize the aggregation. plan &lt;- drake_plan( a = 1, b = 2, c = 3, d = 4 ) reduce_plan(plan) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 target_1 a + b #&gt; 2 target_2 c + d #&gt; 3 target target_1 + target_2 You can control how each pair of targets gets combined. reduce_plan(plan, begin = &quot;c(&quot;, op = &quot;, &quot;, end = &quot;)&quot;) #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 target_1 c(a, b) #&gt; 2 target_2 c(c, d) #&gt; 3 target c(target_1, target_2) See also reduce_by() to do reductions on multiple groups of targets based on other columns in the plan (e.g. from evaluate_plan(trace = TRUE)). 3.6.5 Custom metaprogramming The drake plan is just a data frame. There is nothing magic about it, and you can create it any way you want. With your own custom metaprogramming, you don’t even need the drake_plan() function. The following example could more easily be implemented with map_plan(), but we use other techniques to demonstrate the versatility of custom metaprogramming. Let’s consider a file-based example workflow. Here, our targets execute Linux commands to process input files and create output files. cat in1.txt > out1.txt cat in2.txt > out2.txt The glue package can automatically generate these Linux commands. library(glue) glue_data( list( inputs = c(&quot;in1.txt&quot;, &quot;in2.txt&quot;), outputs = c(&quot;out1.txt&quot;, &quot;out2.txt&quot;) ), &quot;cat {inputs} &gt; {outputs}&quot; ) #&gt; cat in1.txt &gt; out1.txt #&gt; cat in2.txt &gt; out2.txt Our drake commands will use system() to execute the Linux commands that glue generates. Technically, we could use drake_plan() if we wanted. library(tidyverse) drake_plan( glue_data( list( inputs = file_in(c(&quot;in1.txt&quot;, &quot;in2.txt&quot;)), outputs = file_out(c(&quot;out1.txt&quot;, &quot;out2.txt&quot;)) ), &quot;cat {inputs} &gt; {outputs}&quot; ) %&gt;% lapply(FUN = system) ) #&gt; # A tibble: 1 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 drake_target… glue_data(list(inputs = file_in(c(&quot;in1.txt&quot;, &quot;in2.txt&quot;)), … But what if we want to generate these glue commands instead of writing them literally in our plan? This is a job for custom metaprogramming with tidy evaluation. First, we create a function to generate the drake command of an arbitrary target. library(rlang) # for tidy evaluation write_command &lt;- function(cmd, inputs = NULL , outputs = NULL){ inputs &lt;- enexpr(inputs) outputs &lt;- enexpr(outputs) expr({ glue_data( list( inputs = file_in(!!inputs), outputs = file_out(!!outputs) ), !!cmd ) %&gt;% lapply(FUN = system) }) %&gt;% expr_text } write_command( cmd = &quot;cat {inputs} &gt; {outputs}&quot;, inputs = c(&quot;in1.txt&quot;, &quot;in2.txt&quot;), outputs = c(&quot;out1.txt&quot;, &quot;out2.txt&quot;) ) %&gt;% cat #&gt; { #&gt; glue_data(list(inputs = file_in(c(&quot;in1.txt&quot;, &quot;in2.txt&quot;)), #&gt; outputs = file_out(c(&quot;out1.txt&quot;, &quot;out2.txt&quot;))), &quot;cat {inputs} &gt; {outputs}&quot;) %&gt;% #&gt; lapply(FUN = system) #&gt; } Then, we lay out all the arguments we will pass to write_command(). Here, each row corresponds to a separate target. meta_plan &lt;- tribble( ~cmd, ~inputs, ~outputs, &quot;cat {inputs} &gt; {outputs}&quot;, c(&quot;in1.txt&quot;, &quot;in2.txt&quot;), c(&quot;out1.txt&quot;, &quot;out2.txt&quot;), &quot;cat {inputs} {inputs} &gt; {outputs}&quot;, c(&quot;out1.txt&quot;, &quot;out2.txt&quot;), c(&quot;out3.txt&quot;, &quot;out4.txt&quot;) ) %&gt;% print #&gt; # A tibble: 2 x 3 #&gt; cmd inputs outputs #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 cat {inputs} &gt; {outputs} &lt;chr [2]&gt; &lt;chr [2]&gt; #&gt; 2 cat {inputs} {inputs} &gt; {outputs} &lt;chr [2]&gt; &lt;chr [2]&gt; Finally, we create our drake plan without any built-in drake functions. plan &lt;- tibble( target = paste0(&quot;target_&quot;, seq_len(nrow(meta_plan))), command = pmap_chr(meta_plan, write_command) ) %&gt;% print #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 target_1 &quot;{\\n glue_data(list(inputs = file_in(c(\\&quot;in1.txt\\&quot;, \\&quot;in2.tx… #&gt; 2 target_2 &quot;{\\n glue_data(list(inputs = file_in(c(\\&quot;out1.txt\\&quot;, \\&quot;out2.… writeLines(&quot;in1&quot;, &quot;in1.txt&quot;) writeLines(&quot;in2&quot;, &quot;in2.txt&quot;) vis_drake_graph(drake_config(plan)) Alternatively, you could use as.call() instead of tidy evaluation to generate your plan. Use as.call() to construct calls to file_in(), file_out(), and custom functions in your commands. library(purrr) # pmap_chr() is particularly useful here. # A function that will be called in your commands. command_function &lt;- function(cmd, inputs, outputs){ glue_data( list( inputs = inputs, outputs = outputs ), cmd ) %&gt;% purrr::walk(system) } # A function to generate quoted calls to command_function(), # which in turn contain quoted calls to file_in() and file_out(). write_command &lt;- function(...){ args &lt;- list(...) args$inputs &lt;- as.call(list(quote(file_in), args$inputs)) args$outputs &lt;- as.call(list(quote(file_out), args$outputs)) c(quote(command_function), args) %&gt;% as.call() %&gt;% rlang::expr_text() } plan &lt;- tibble( target = paste0(&quot;target_&quot;, seq_len(nrow(meta_plan))), command = pmap_chr(meta_plan, write_command) ) %&gt;% print #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 target_1 &quot;command_function(cmd = \\&quot;cat {inputs} &gt; {outputs}\\&quot;, inputs = … #&gt; 2 target_2 &quot;command_function(cmd = \\&quot;cat {inputs} {inputs} &gt; {outputs}\\&quot;, … Metaprogramming gets much simpler if you do not need to construct literal calls to file_in(), file_out(), etc. in your commands. The construction of model_plan in the gross state product exmaple is an example. Thanks to Chris Hammill for presenting this scenario and contributing to the solution. You can turn the command column of your plan into a character vector (e.g. plan$command &lt;- purrr::map_chr(plan$command, rlang::expr_text)) and drake will still understand you. However, the recommended format is a list of expressions. drake_plan() and friends always supply expression lists.↩ drake_plan() is the best way to create plans, but you can create plans any way you like. drake will understand plans you create directly using data.frame() or tibble().↩ "],
["projects.html", "Chapter 4 drake projects 4.1 Code files 4.2 Safer interactivity 4.3 R scripts pitfalls 4.4 Workflows as R packages 4.5 Other tools", " Chapter 4 drake projects drake’s design philosophy is extremely R-focused. It embraces in-memory configuration, in-memory dependencies, interactivity, and flexibility. 4.1 Code files The names and locations of the files are entirely up to you, but this pattern is particularly useful to start with. make.R R/ ├── packages.R ├── functions.R └── plan.R Here, make.R is a master script that Loads your packages, functions, and other in-memory data. Creates the drake plan. Calls make(). Let’s consider the main example, which you can download with drake_example(&quot;main&quot;). Here, our master script is called make.R: source(&quot;R/packages.R&quot;) # loads packages source(&quot;R/functions.R&quot;) # defines the create_plot() function source(&quot;R/plan.R&quot;) # creates the drake plan # options(clustermq.scheduler = &quot;multicore&quot;) # optional parallel computing make( plan, # defined in R/plan.R verbose = 2 ) We have an R folder containing our supporting files, including packages.R: library(drake) require(dplyr) require(ggplot2) functions.R: create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + theme_gray(20) } and plan.R: plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) To run the example project above, Start a clean new R session. Run the make.R script. On Mac and Linux, you can do this by opening a terminal and entering R CMD BATCH make.R. On Windows, restart your R session and call source(&quot;make.R&quot;) in the R console. Note: this part of drake does not inherently focus on your script files. There is nothing magical about the names make.R, packages.R, functions.R, or plan.R. Different projects may require different file structures. drake has other functions to inspect your results and examine your workflow. Before invoking them interactively, it is best to start with a clean new R session. # Restart R. &gt; interactive() #&gt; [1] TRUE source(&quot;R/packages.R&quot;) source(&quot;R/functions.R&quot;) source(&quot;R/plan.R&quot;) config &lt;- drake_config(plan) vis_drake_graph() 4.2 Safer interactivity Caution: functions r_make() etc. described below are still experiemntal. 4.2.1 Motivation A serious drake workflow should be consistent and reliable, ideally with the help of a master R script. Before it builds your targets, this script should begin in a fresh R session and load your packages and functions in a dependable manner. Batch mode makes sure all this goes according to plan. If you use a single persistent interactive R session to repeatedly invoke make() while you develop the workflow, then over time, your session could grow stale and accidentally invalidate targets. For example, if you interactively tinker with a new version of create_plot(), targets hist and report will fall out of date without warning. If this happens, the quickest fix is to restart R and source() your setup scripts all over again. However, a better solution is to use r_make() and friends. r_make() runs make() in a new transient R session so that accidental changes to your interactive environment do not break your workflow. 4.2.2 Usage To use r_make(), you need a configuration R script. Unless you supply a custom file path (e.g. r_make(source = &quot;your_file.R&quot;) or options(drake_source = &quot;your_file.R&quot;)) drake assumes this configuration script is called _drake.R. (So the file name really is magical in this case). The suggested file structure becomes: _drake.R R/ ├── packages.R ├── functions.R └── plan.R Like our previous make.R script, _drake.R runs all our pre-make() setup steps. But this time, rather than calling make(), it ends with a call to drake_config(). Example _drake.R: source(&quot;R/packages.R&quot;) source(&quot;R/functions.R&quot;) source(&quot;R/plan.R&quot;) # options(clustermq.scheduler = &quot;multicore&quot;) # optional parallel computing drake_config(plan, verbose = 2) Here is what happens when you call r_make(). drake launches a new transient R session using callr::r(). The remaining steps all happen within this transient session. Run the configuration script (e.g. _drake.R) to Load the packages, functions, global options, drake plan, etc. into the session’s environnment, and Run the call to drake_config()and store the results in a variable called config. Execute make(config = config) The purpose of drake_config() is to collect and sanitize all the parameters and settings that make() needs to do its job. In fact, if you do not set the config argument explicitly, then make() invokes drake_config() behind the scenes. make(plan, parallelism = &quot;clustermq&quot;, jobs = 2, verbose = 6) is equivalent to config &lt;- drake_config(plan, parallelism = &quot;clustermq&quot;, jobs = 2, verbose = 6) make(config = config) There are many more r_*() functions besides r_make(), each of which launches a fresh session and runs an inner drake function on the config object from _drake.R. Outer function call Inner function call r_make() make(config = config) r_drake_build(...) drake_build(config, ...) r_outdated(...) outdated(config, ...) r_missed(...) missed(config, ...) r_vis_drake_graph(...) vis_drake_graph(config, ...) r_sankey_drake_graph(...) sankey_drake_graph(config, ...) r_drake_ggraph(...) drake_ggraph(config, ...) r_drake_graph_info(...) drake_graph_info(config, ...) r_predict_runtime(...) predict_runtime(config, ...) r_predict_workers(...) predict_workers(config, ...) clean() r_outdated() #&gt; Loading required package: dplyr #&gt; #&gt; Attaching package: ‘dplyr’ #&gt; #&gt; The following objects are masked from ‘package:stats’: #&gt; #&gt; filter, lag #&gt; #&gt; The following objects are masked from ‘package:base’: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; #&gt; Loading required package: ggplot2 #&gt; [1] &quot;data&quot; &quot;fit&quot; &quot;hist&quot; &quot;raw_data&quot; &quot;report&quot; r_make() #&gt; Loading required package: dplyr #&gt; #&gt; Attaching package: ‘dplyr’ #&gt; #&gt; The following objects are masked from ‘package:stats’: #&gt; #&gt; filter, lag #&gt; #&gt; The following objects are masked from ‘package:base’: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; #&gt; Loading required package: ggplot2 #&gt; target raw_data #&gt; target data #&gt; target fit #&gt; target hist #&gt; target report r_outdated() #&gt; Loading required package: dplyr #&gt; #&gt; Attaching package: ‘dplyr’ #&gt; #&gt; The following objects are masked from ‘package:stats’: #&gt; #&gt; filter, lag #&gt; #&gt; The following objects are masked from ‘package:base’: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; #&gt; Loading required package: ggplot2 #&gt; character(0) r_vis_drake_graph(targets_only = TRUE) #&gt; Loading required package: dplyr #&gt; #&gt; Attaching package: ‘dplyr’ #&gt; #&gt; The following objects are masked from ‘package:stats’: #&gt; #&gt; filter, lag #&gt; #&gt; The following objects are masked from ‘package:base’: #&gt; #&gt; intersect, setdiff, setequal, union #&gt; #&gt; Loading required package: ggplot2 Remarks: You can run r_make() in an interactive session, but the transient process it launches will not be interactive. Thus, any browser() statements in the commands in your drake plan will be ignored. You can select and configure the underlying callr function using arguments r_fn and r_args, respectively. For example code, you can download the updated main example (drake_example(&quot;main&quot;)) and experiment with files _drake.R and interactive.R. 4.3 R scripts pitfalls Despite the above discussion of R scripts, drake plans rely more on in-memory functions. You might be tempted to write a plan like the following, but then drake cannot tell that my_analysis depends on my_data. bad_plan &lt;- drake_plan( my_data = source(file_in(&quot;get_data.R&quot;)), my_analysis = source(file_in(&quot;analyze_data.R&quot;)), my_summaries = source(file_in(&quot;summarize_data.R&quot;)) ) bad_config &lt;- drake_config(bad_plan) vis_drake_graph(bad_config, targets_only = TRUE) When it comes to plans, use functions instead. source(&quot;my_functions.R&quot;) # defines get_data(), analyze_data(), etc. good_plan &lt;- drake_plan( my_data = get_data(file_in(&quot;data.csv&quot;)), # External files need to be in commands explicitly. # nolint my_analysis = analyze_data(my_data), my_summaries = summarize_results(my_data, my_analysis) ) good_config &lt;- drake_config(good_plan) vis_drake_graph(good_config, targets_only = TRUE) 4.4 Workflows as R packages The R package structure is a great way to organize and quality-control a data analysis project. If you write a drake workflow as a package, you will need Use expose_imports() to properly account for all your nested function dependencies, and If you load the package with devtools::load_all(), set the prework argument of make(): e.g. make(prework = &quot;devtools::load_all()&quot;). For a minimal example, see Tiernan Martin’s drakepkg. 4.5 Other tools drake enhances reproducibility, but not in all respects. Local library managers, containerization, and session management tools offer more robust solutions in their respective domains. Reproducibility encompasses a wide variety of tools and techniques all working together. Comprehensive overviews: PLOS article by Wilson et al. RStudio Conference 2019 presentation by Karthik Ram. rrtools by Ben Marwick. "],
["packages.html", "Chapter 5 An analysis of R package download trends 5.1 Get the code. 5.2 Overview 5.3 Analysis 5.4 Other ways to trigger downloads", " Chapter 5 An analysis of R package download trends This chapter explores R package download trends using the cranlogs package, and it shows how drake’s custom triggers can help with workflows with remote data sources. 5.1 Get the code. Write the code files to your workspace. drake_example(&quot;packages&quot;) The new packages folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 5.2 Overview This small data analysis project explores some trends in R package downloads over time. The datasets are downloaded using the cranlogs package. library(cranlogs) cran_downloads(packages = &quot;dplyr&quot;, when = &quot;last-week&quot;) #&gt; date count package #&gt; 1 2019-03-03 13868 dplyr #&gt; 2 2019-03-04 29135 dplyr #&gt; 3 2019-03-05 30970 dplyr #&gt; 4 2019-03-06 31001 dplyr #&gt; 5 2019-03-07 30836 dplyr #&gt; 6 2019-03-08 26289 dplyr #&gt; 7 2019-03-09 14249 dplyr Above, each count is the number of times dplyr was downloaded from the RStudio CRAN mirror on the given day. To stay up to date with the latest download statistics, we need to refresh the data frequently. With drake, we can bring all our work up to date without restarting everything from scratch. 5.3 Analysis First, we load the required packages. drake detects the packages you install and load. library(cranlogs) library(drake) library(dplyr) library(ggplot2) library(knitr) library(rvest) We will want custom functions to summarize the CRAN logs we download. make_my_table &lt;- function(downloads){ group_by(downloads, package) %&gt;% summarize(mean_downloads = mean(count)) } make_my_plot &lt;- function(downloads){ ggplot(downloads) + geom_line(aes(x = date, y = count, group = package, color = package)) } Next, we generate the plan. We want to explore the daily downloads from the knitr, Rcpp, and ggplot2 packages. We will use the cranlogs package to get daily logs of package downloads from RStudio’s CRAN mirror. In our drake_plan(), we declare targets older and recent to contain snapshots of the logs. The following drake_plan() syntax is described here, which is supported in drake 7.0.0 and above (and the current development version). plan &lt;- drake_plan( older = cran_downloads( packages = c(&quot;knitr&quot;, &quot;Rcpp&quot;, &quot;ggplot2&quot;), from = &quot;2016-11-01&quot;, to = &quot;2016-12-01&quot;, ), recent = target( command = cran_downloads( packages = c(&quot;knitr&quot;, &quot;Rcpp&quot;, &quot;ggplot2&quot;), when = &quot;last-month&quot;), trigger = trigger(change = latest_log_date()) ), averages = target( make_my_table(data), transform = map(data = c(older, recent)) ), plot = target( make_my_plot(data), transform = map(data) ), report = knit( knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE ) ) Notice the custom trigger for the target recent. Here, we are telling drake to rebuild recent whenever a new day’s log is uploaded to http://cran-logs.rstudio.com. In other words, drake keeps track of the return value of latest_log_date() and recomputes recent (during make()) if that value changed since the last make(). Here, latest_log_date() is one of our custom imported functions. We use it to scrape http://cran-logs.rstudio.com using the rvest package. latest_log_date &lt;- function(){ read_html(&quot;http://cran-logs.rstudio.com/&quot;) %&gt;% html_nodes(&quot;li:last-of-type&quot;) %&gt;% html_nodes(&quot;a:last-of-type&quot;) %&gt;% html_text() %&gt;% max } Now, we run the project to download the data and analyze it. The results will be summarized in the knitted report, report.md, but you can also read the results directly from the cache. make(plan) #&gt; target older #&gt; target recent #&gt; target averages_older #&gt; target plot_older #&gt; target averages_recent #&gt; target plot_recent #&gt; target report readd(averages_recent) #&gt; # A tibble: 3 x 2 #&gt; package mean_downloads #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ggplot2 25105. #&gt; 2 knitr 16472. #&gt; 3 Rcpp 29807. readd(averages_older) #&gt; # A tibble: 3 x 2 #&gt; package mean_downloads #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ggplot2 14641. #&gt; 2 knitr 9069. #&gt; 3 Rcpp 14408. readd(plot_recent) readd(plot_older) If we run make() again right away, we see that everything is up to date. But if we wait until a new day’s log is uploaded, make() will update recent and everything that depends on it. make(plan) #&gt; All targets are already up to date. To visualize the build behavior, you can plot the dependency network. config &lt;- drake_config(plan) vis_drake_graph(config) 5.4 Other ways to trigger downloads Sometimes, our remote data sources get revised, and web scraping may not be the best way to detect changes. We may want to look at our remote dataset’s modification time or HTTP ETag. To see how this works, consider the CRAN log file from February 9, 2018. url &lt;- &quot;http://cran-logs.rstudio.com/2018/2018-02-09-r.csv.gz&quot; We can track the modification date using the httr package. library(httr) # For querying websites. HEAD(url)$headers[[&quot;last-modified&quot;]] #&gt; [1] &quot;Mon, 12 Feb 2018 16:34:48 GMT&quot; In our drake plan, we can track this timestamp and trigger a download whenever it changes. plan &lt;- drake_plan( logs = target( get_logs(url), trigger = trigger(change = HEAD(url)$headers[[&quot;last-modified&quot;]]) ) ) plan #&gt; # A tibble: 1 x 3 #&gt; target command trigger #&gt; &lt;chr&gt; &lt;expr&gt; &lt;expr&gt; #&gt; 1 logs get_logs(url) trigger(change = HEAD(url)$headers[[&quot;last-modified&quot;… where library(R.utils) # For unzipping the files we download. library(curl) # For downloading data. get_logs &lt;- function(url){ curl_download(url, &quot;logs.csv.gz&quot;) # Get a big file. gunzip(&quot;logs.csv.gz&quot;, overwrite = TRUE) # Unzip it. out &lt;- read.csv(&quot;logs.csv&quot;, nrows = 4) # Extract the data you need. unlink(c(&quot;logs.csv.gz&quot;, &quot;logs.csv&quot;)) # Remove the big files out # Value of the target. } When we are ready, we run the workflow. make(plan) #&gt; target logs readd(logs) #&gt; date time size version os country ip_id #&gt; 1 2018-02-09 13:01:13 82375220 3.4.3 win RO 1 #&gt; 2 2018-02-09 13:02:06 74286541 3.3.3 win US 2 #&gt; 3 2018-02-09 13:02:10 82375216 3.4.3 win US 3 #&gt; 4 2018-02-09 13:03:30 82375220 3.4.3 win IS 4 If the log file at the url ever changes, the timestamp will update remotely, and make() will download the file again. "],
["gsp.html", "Chapter 6 Finding the best model of gross state product 6.1 Get the code. 6.2 Objective and methods 6.3 Data 6.4 Analysis 6.5 Results 6.6 Comparison with GNU Make 6.7 References", " Chapter 6 Finding the best model of gross state product The following data analysis workflow shows off drake’s ability to generate lots of reproducibly-tracked tasks with ease. The same technique would be cumbersome, even intractable, with GNU Make. 6.1 Get the code. Write the code files to your workspace. drake_example(&quot;gsp&quot;) The new gsp folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 6.2 Objective and methods The goal is to search for factors closely associated with the productivity of states in the USA around the 1970s and 1980s. For the sake of simplicity, we use gross state product as a metric of productivity, and we restrict ourselves to multiple linear regression models with three variables. For each of the 84 possible models, we fit the data and then evaluate the root mean squared prediction error (RMSPE). \\[ \\begin{aligned} \\text{RMSPE} = \\sqrt{(\\text{y} - \\widehat{y})^T(y - \\widehat{y})} \\end{aligned} \\] Here, \\(y\\) is the vector of observed gross state products in the data, and \\(\\widehat{y}\\) is the vector of predicted gross state products under one of the models. We take the best variables to be the triplet in the model with the lowest RMSPE. 6.3 Data The Produc dataset from the Ecdat package contains data on the Gross State Product from 1970 to 1986. Each row is a single observation on a single state for a single year. The dataset has the following variables as columns. See the references later in this report for more details. gsp: gross state product. state: the state. year: the year. pcap: private capital stock. hwy: highway and streets. water: water and sewer facilities. util: other public buildings and structures. pc: public capital. emp: labor input measured by the employment in non-agricultural payrolls. unemp: state unemployment rate. library(Ecdat) data(Produc) head(Produc) #&gt; state year pcap hwy water util pc gsp emp #&gt; 1 ALABAMA 1970 15032.67 7325.80 1655.68 6051.20 35793.80 28418 1010.5 #&gt; 2 ALABAMA 1971 15501.94 7525.94 1721.02 6254.98 37299.91 29375 1021.9 #&gt; 3 ALABAMA 1972 15972.41 7765.42 1764.75 6442.23 38670.30 31303 1072.3 #&gt; 4 ALABAMA 1973 16406.26 7907.66 1742.41 6756.19 40084.01 33430 1135.5 #&gt; 5 ALABAMA 1974 16762.67 8025.52 1734.85 7002.29 42057.31 33749 1169.8 #&gt; 6 ALABAMA 1975 17316.26 8158.23 1752.27 7405.76 43971.71 33604 1155.4 #&gt; unemp #&gt; 1 4.7 #&gt; 2 5.2 #&gt; 3 4.7 #&gt; 4 3.9 #&gt; 5 5.5 #&gt; 6 7.7 6.4 Analysis First, we load the required packages. drake is aware of all the packages you load with library() or require(). library(biglm) # lightweight models, easier to store than with lm() library(drake) library(Ecdat) # econometrics datasets library(ggplot2) library(knitr) library(purrr) library(tidyverse) Next, we construct our plan. The following code uses drake’s special new language for generating plans (learn more here). predictors &lt;- setdiff(colnames(Produc), &quot;gsp&quot;) # We will try all combinations of three covariates. combos &lt;- combn(predictors, 3) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) %&gt;% setNames(c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;)) head(combos) #&gt; x1 x2 x3 #&gt; 1 state year pcap #&gt; 2 state year hwy #&gt; 3 state year water #&gt; 4 state year util #&gt; 5 state year pc #&gt; 6 state year emp # We need to list each covariate as a symbol. for (col in colnames(combos)) { combos[[col]] &lt;- rlang::syms(combos[[col]]) } # Requires drake &gt;= 7.0.0 or the development version # at github.com/ropensci/drake. # Install with remotes::install_github(&quot;ropensci/drake&quot;). plan &lt;- drake_plan( model = target( biglm(gsp ~ x1 + x2 + x3, data = Ecdat::Produc), transform = map(.data = !!combos) # Remember the bang-bang!! ), rmspe_i = target( get_rmspe(model, Ecdat::Produc), transform = map(model) ), rmspe = target( bind_rows(rmspe_i, .id = &quot;model&quot;), transform = combine(rmspe_i) ), plot = ggsave(filename = file_out(&quot;rmspe.pdf&quot;), plot = plot_rmspe(rmspe)), report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) ) plan #&gt; # A tibble: 171 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 model_state_year_pcap biglm(gsp ~ state + year + pcap, data = Ecdat::Pr… #&gt; 2 model_state_year_hwy biglm(gsp ~ state + year + hwy, data = Ecdat::Pro… #&gt; 3 model_state_year_wat… biglm(gsp ~ state + year + water, data = Ecdat::P… #&gt; 4 model_state_year_util biglm(gsp ~ state + year + util, data = Ecdat::Pr… #&gt; 5 model_state_year_pc biglm(gsp ~ state + year + pc, data = Ecdat::Prod… #&gt; 6 model_state_year_emp biglm(gsp ~ state + year + emp, data = Ecdat::Pro… #&gt; 7 model_state_year_une… biglm(gsp ~ state + year + unemp, data = Ecdat::P… #&gt; 8 model_state_pcap_hwy biglm(gsp ~ state + pcap + hwy, data = Ecdat::Pro… #&gt; 9 model_state_pcap_wat… biglm(gsp ~ state + pcap + water, data = Ecdat::P… #&gt; 10 model_state_pcap_util biglm(gsp ~ state + pcap + util, data = Ecdat::Pr… #&gt; # … with 161 more rows We also need to define functions for summaries and plots. get_rmspe &lt;- function(model_fit, data){ y &lt;- data$gsp yhat &lt;- as.numeric(predict(model_fit, newdata = data)) terms &lt;- attr(model_fit$terms, &quot;term.labels&quot;) data.frame( rmspe = sqrt(mean((y - yhat)^2)), # nolint X1 = terms[1], X2 = terms[2], X3 = terms[3] ) } plot_rmspe &lt;- function(rmspe){ ggplot(rmspe) + geom_histogram(aes(x = rmspe), bins = 15) } We have a report.Rmd file to summarize our results at the end. drake_example(&quot;gsp&quot;) file.copy(from = &quot;gsp/report.Rmd&quot;, to = &quot;.&quot;, overwrite = TRUE) #&gt; [1] TRUE We can inspect the project before we run it. config &lt;- drake_config(plan) vis_drake_graph(config) Now, we can run the project. make(plan, verbose = FALSE) 6.5 Results Here are the root mean squared prediction errors of all the models. results &lt;- readd(rmspe) library(ggplot2) plot_rmspe(rmspe = results) And here are the best models. The best variables are in the top row under X1, X2, and X3. head(results[order(results$rmspe, decreasing = FALSE), ]) #&gt; model rmspe X1 X2 X3 #&gt; 28 28 2613.669 state hwy emp #&gt; 44 44 2664.842 state water emp #&gt; 41 41 2665.744 state util emp #&gt; 33 33 2666.058 state pc emp #&gt; 35 35 2675.336 state pcap emp #&gt; 27 27 2692.687 state emp unemp 6.6 Comparison with GNU Make If we were using Make instead of drake with the same set of targets, the analogous Makefile would look something like this pseudo-code sketch. models = model_state_year_pcap.rds model_state_year_hwy.rds ... # 84 of these model_% Rscript -e 'saveRDS(lm(...), ...)' rmspe_%: model_% Rscript -e 'saveRDS(get_rmspe(...), ...)' rmspe.rds: rmspe_% Rscript -e 'saveRDS(rbind(...), ...)' rmspe.pdf: rmspe.rds Rscript -e 'ggplot2::ggsave(plot_rmspe(readRDS(\"rmspe.rds\")), \"rmspe.pdf\")' report.md: report.Rmd Rscript -e 'knitr::knit(\"report.Rmd\")' There are three main disadvantages to this approach. Every target requires a new call to Rscript, which means that more time is spent initializing R sessions than doing the actual work. The user must micromanage nearly one hundred output files (in this case, *.rds files), which is cumbersome, messy, and inconvenient. drake, on the other hand, automatically manages storage using a storr cache. The user needs to write the names of the 84 models near the top of the Makefile, which is less convenient than maintaining a data frame in R. 6.7 References Baltagi, Badi H (2003). Econometric analysis of panel data, John Wiley and sons, http://www.wiley.com/legacy/wileychi/baltagi/. Baltagi, B. H. and N. Pinnoi (1995). “Public capital stock and state productivity growth: further evidence”, Empirical Economics, 20, 351-359. Munnell, A. (1990). “Why has productivity growth declined? Productivity and public investment”&quot;, New England Economic Review, 3-22. Yves Croissant (2016). Ecdat: Data Sets for Econometrics. R package version 0.3-1. https://CRAN.R-project.org/package=Ecdat. "],
["mtcars.html", "Chapter 7 The mtcars example and drake plan generation 7.1 Get the code. 7.2 Quick examples 7.3 The motivation of the mtcars example 7.4 Set up the mtcars example 7.5 The drake plan 7.6 Generate the plan 7.7 Run the workflow", " Chapter 7 The mtcars example and drake plan generation This chapter is a walkthrough of drake’s main functionality based on the mtcars example. It sets up the project and runs it repeatedly to demonstrate drake’s most important functionality. 7.1 Get the code. Write the code files to your workspace. drake_example(&quot;mtcars&quot;) The new mtcars folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 7.2 Quick examples Inspect and run your project. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). config &lt;- drake_config(my_plan) # Master configuration list vis_drake_graph(config) # Hover, click, drag, zoom, pan. make(my_plan) # Run the workflow. outdated(config) # Everything is up to date. Debug errors. failed() # Targets that failed in the most recent `make()` context &lt;- diagnose(large) # Diagnostic metadata: errors, warnings, etc. error &lt;- context$error str(error) # Object of class &quot;error&quot; error$message error$call error$calls # Full traceback of nested calls leading up to the error. # nolint Dive deeper into the built-in examples. drake_example(&quot;mtcars&quot;) # Write the code files. drake_examples() # List the other examples. 7.3 The motivation of the mtcars example Is there an association between the weight and the fuel efficiency of cars? To find out, we use the mtcars dataset from the datasets package. The mtcars dataset originally came from the 1974 Motor Trend US magazine, and it contains design and performance data on 32 models of automobile. # ?mtcars # more info head(mtcars) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 #&gt; Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 #&gt; Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Here, wt is weight in tons, and mpg is fuel efficiency in miles per gallon. We want to figure out if there is an association between wt and mpg. The mtcars dataset itself only has 32 rows, so we generate two larger bootstrapped datasets and then analyze them with regression models. We summarize the regression models to see if there is an association. 7.4 Set up the mtcars example Before you run your project, you need to set up the workspace. In other words, you need to gather the “imports”: functions, pre-loaded data objects, and saved files that you want to be available before the real work begins. library(knitr) # drake knows which packages you load. library(drake) We need a function to bootstrap larger datasets from mtcars. # Pick a random subset of n rows from a dataset random_rows &lt;- function(data, n){ data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } # Bootstrapped datasets from mtcars. simulate &lt;- function(n){ # Pick a random set of cars to bootstrap from the mtcars data. data &lt;- random_rows(data = mtcars, n = n) # x is the car&#39;s weight, and y is the fuel efficiency. data.frame( x = data$wt, y = data$mpg ) } We also need functions to apply the regression models we need for detecting associations. # Is fuel efficiency linearly related to weight? reg1 &lt;- function(d){ lm(y ~ + x, data = d) } # Is fuel efficiency related to the SQUARE of the weight? reg2 &lt;- function(d){ d$x2 &lt;- d$x ^ 2 lm(y ~ x2, data = d) } We want to summarize the final results in an R Markdown report, so we need the the report.Rmd source file. You can get it with drake_example(&quot;mtcars&quot;) or load_mtcars_example(). drake_example(&quot;mtcars&quot;, overwrite = TRUE) file.copy(&quot;mtcars/report.Rmd&quot;, &quot;.&quot;, overwrite = TRUE) #&gt; [1] TRUE Here are the contents of the report. It will serve as a final summary of our work, and we will process it at the very end. Admittedly, some of the text spoils the punch line. cat(readLines(&quot;report.Rmd&quot;), sep = &quot;\\n&quot;) #&gt; --- #&gt; title: &quot;Final results report for the mtcars example&quot; #&gt; author: You #&gt; output: html_document #&gt; --- #&gt; #&gt; # The weight and fuel efficiency of cars #&gt; #&gt; Is there an association between the weight and the fuel efficiency of cars? To find out, we use the `mtcars` dataset from the `datasets` package. The `mtcars` data originally came from the 1974 Motor Trend US magazine, and it contains design and performance data on 32 models of automobile. #&gt; #&gt; ```{r showmtcars} #&gt; # ?mtcars # more info #&gt; head(mtcars) #&gt; ``` #&gt; #&gt; Here, `wt` is weight in tons, and `mpg` is fuel efficiency in miles per gallon. We want to figure out if there is an association between `wt` and `mpg`. The `mtcars` dataset itself only has 32 rows, so we generated two larger bootstrapped datasets. We called them `small` and `large`. #&gt; #&gt; ```{r example_chunk} #&gt; library(drake) #&gt; head(readd(small)) # 48 rows #&gt; loadd(large) # 64 rows #&gt; head(large) #&gt; ``` #&gt; #&gt; Then, we fit a couple regression models to the `small` and `large` to try to detect an association between `wt` and `mpg`. Here are the coefficients and p-values from one of the model fits. #&gt; #&gt; ```{r second_example_chunk} #&gt; readd(coef_regression2_small) #&gt; ``` #&gt; #&gt; Since the p-value on `x2` is so small, there may be an association between weight and fuel efficiency after all. #&gt; #&gt; # A note on knitr reports in drake projects. #&gt; #&gt; Because of the calls to `readd()` and `loadd()`, `drake` knows that `small`, `large`, and `coef_regression2_small` are dependencies of this R Markdown report. This dependency relationship is what causes the report to be processed at the very end. Now, all our imports are set up. When the real work begins, drake will import functions and data objects from your R session environment ls() #&gt; [1] &quot;args&quot; &quot;bad_config&quot; &quot;bad_plan&quot; #&gt; [4] &quot;check_credit_hours&quot; &quot;check_graduations&quot; &quot;check_public_funding&quot; #&gt; [7] &quot;check_students&quot; &quot;col&quot; &quot;combos&quot; #&gt; [10] &quot;command_function&quot; &quot;config&quot; &quot;config1&quot; #&gt; [13] &quot;config2&quot; &quot;covariates&quot; &quot;create_plot&quot; #&gt; [16] &quot;get_logs&quot; &quot;get_rmspe&quot; &quot;get_school_data&quot; #&gt; [19] &quot;good_config&quot; &quot;good_plan&quot; &quot;hard_plan&quot; #&gt; [22] &quot;hist&quot; &quot;latest_log_date&quot; &quot;lots_of_sds&quot; #&gt; [25] &quot;make_my_plot&quot; &quot;make_my_table&quot; &quot;meta_plan&quot; #&gt; [28] &quot;my_grid&quot; &quot;my_model_fit&quot; &quot;plan&quot; #&gt; [31] &quot;plan_template&quot; &quot;plan1&quot; &quot;plan2&quot; #&gt; [34] &quot;plot_rmspe&quot; &quot;predictors&quot; &quot;Produc&quot; #&gt; [37] &quot;random_rows&quot; &quot;reg1&quot; &quot;reg2&quot; #&gt; [40] &quot;results&quot; &quot;simulate&quot; &quot;small_config&quot; #&gt; [43] &quot;small_plan&quot; &quot;tmp&quot; &quot;url&quot; #&gt; [46] &quot;vals&quot; &quot;write_command&quot; &quot;y_vals&quot; and saved files from your file system. list.files() #&gt; [1] &quot;_book&quot; &quot;_bookdown.yml&quot; &quot;appendix.Rmd&quot; #&gt; [4] &quot;build.R&quot; &quot;caution.Rmd&quot; &quot;CONDUCT.md&quot; #&gt; [7] &quot;debug.Rmd&quot; &quot;deploy.sh&quot; &quot;DESCRIPTION&quot; #&gt; [10] &quot;drake-manual_files&quot; &quot;drake-manual.Rmd&quot; &quot;drake-manual.Rproj&quot; #&gt; [13] &quot;examples.Rmd&quot; &quot;faq.R&quot; &quot;faq.Rmd&quot; #&gt; [16] &quot;footer.html&quot; &quot;functionality.Rmd&quot; &quot;gsp.Rmd&quot; #&gt; [19] &quot;hpc.Rmd&quot; &quot;images&quot; &quot;index.Rmd&quot; #&gt; [22] &quot;LICENSE&quot; &quot;main.Rmd&quot; &quot;mtcars&quot; #&gt; [25] &quot;mtcars.Rmd&quot; &quot;NEWS.md&quot; &quot;packages.Rmd&quot; #&gt; [28] &quot;plans.Rmd&quot; &quot;projects.Rmd&quot; &quot;README.md&quot; #&gt; [31] &quot;report.Rmd&quot; &quot;start.Rmd&quot; &quot;store.Rmd&quot; #&gt; [34] &quot;time.Rmd&quot; &quot;triggers.Rmd&quot; &quot;visuals.Rmd&quot; 7.5 The drake plan Now that your workspace of imports is prepared, we can outline the real work step by step in a drake plan. load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). my_plan #&gt; # A tibble: 15 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;),… #&gt; 2 small simulate(48) … #&gt; 3 large simulate(64) … #&gt; 4 regression1_small reg1(small) … #&gt; 5 regression1_large reg1(large) … #&gt; 6 regression2_small reg2(small) … #&gt; 7 regression2_large reg2(large) … #&gt; 8 summ_regression1_s… suppressWarnings(summary(regression1_small$residual… #&gt; 9 summ_regression1_l… suppressWarnings(summary(regression1_large$residual… #&gt; 10 summ_regression2_s… suppressWarnings(summary(regression2_small$residual… #&gt; 11 summ_regression2_l… suppressWarnings(summary(regression2_large$residual… #&gt; 12 coef_regression1_s… suppressWarnings(summary(regression1_small))$coeffi… #&gt; 13 coef_regression1_l… suppressWarnings(summary(regression1_large))$coeffi… #&gt; 14 coef_regression2_s… suppressWarnings(summary(regression2_small))$coeffi… #&gt; 15 coef_regression2_l… suppressWarnings(summary(regression2_large))$coeffi… Each row is an intermediate step, and each command generates a single target. A target is an output R object (cached when generated) or an output file (specified with single quotes), and a command just an ordinary piece of R code (not necessarily a single function call). Commands make use of targets generated by other commands, objects your environment, input files, and namespaced objects/functions from packages (referenced with :: or :::). These dependencies give your project an underlying network representation. # Hover, click, drag, zoom, and pan. config &lt;- drake_config(my_plan) vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) # Also drake_graph() You can also check the dependencies of individual targets and imported functions. deps_code(reg2) #&gt; # A tibble: 4 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 x globals #&gt; 2 y globals #&gt; 3 lm globals #&gt; 4 x2 globals deps_code(my_plan$command[[1]]) #&gt; # A tibble: 6 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 knit globals #&gt; 2 large loadd #&gt; 3 small readd #&gt; 4 coef_regression2_small readd #&gt; 5 report.md file_out #&gt; 6 report.Rmd knitr_in deps_code(my_plan$command[[nrow(my_plan)]]) #&gt; # A tibble: 4 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 summary globals #&gt; 2 suppressWarnings globals #&gt; 3 regression2_large globals #&gt; 4 coefficients globals List all the reproducibly-tracked objects and files. tracked(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;datasets::mtcars&quot; #&gt; [7] &quot;file report.Rmd&quot; &quot;file report.md&quot; #&gt; [9] &quot;random_rows&quot; &quot;reg1&quot; #&gt; [11] &quot;reg2&quot; &quot;regression1_large&quot; #&gt; [13] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [15] &quot;regression2_small&quot; &quot;report&quot; #&gt; [17] &quot;simulate&quot; &quot;small&quot; #&gt; [19] &quot;summ_regression1_large&quot; &quot;summ_regression1_small&quot; #&gt; [21] &quot;summ_regression2_large&quot; &quot;summ_regression2_small&quot; 7.6 Generate the plan 7.6.1 The easy way drake version 7.0.0 will support new special syntax to create complicated drake plans from boilerplate code. See the chapter on plans for more details. To get the funcionality early, install development drake. install.packages(&quot;remotes&quot;) library(remotes) install_github(&quot;ropensci/drake&quot;) Then, use transformations to generate the plan. my_plan &lt;- drake_plan( report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE), small = simulate(48), large = simulate(64), regression1 = target( reg1(data), transform = map(data = c(small, large), .tag_out = reg) ), regression2 = target( reg2(data), transform = map(data, .tag_out = reg) ), summ = target( suppressWarnings(summary(reg$residuals)), transform = map(reg) ), coef = target( suppressWarnings(summary(reg))$coefficients, transform = map(reg) ) ) my_plan #&gt; # A tibble: 15 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;),… #&gt; 2 small simulate(48) … #&gt; 3 large simulate(64) … #&gt; 4 regression1_small reg1(small) … #&gt; 5 regression1_large reg1(large) … #&gt; 6 regression2_large reg2(large) … #&gt; 7 regression2_small reg2(small) … #&gt; 8 summ_regression1_l… suppressWarnings(summary(regression1_large$residual… #&gt; 9 summ_regression1_s… suppressWarnings(summary(regression1_small$residual… #&gt; 10 summ_regression2_l… suppressWarnings(summary(regression2_large$residual… #&gt; 11 summ_regression2_s… suppressWarnings(summary(regression2_small$residual… #&gt; 12 coef_regression1_l… suppressWarnings(summary(regression1_large))$coeffi… #&gt; 13 coef_regression1_s… suppressWarnings(summary(regression1_small))$coeffi… #&gt; 14 coef_regression2_l… suppressWarnings(summary(regression2_large))$coeffi… #&gt; 15 coef_regression2_s… suppressWarnings(summary(regression2_small))$coeffi… In the first row above, knitr_in() indicates that report.Rmd is a dependency and targets loaded with loadd() and readd() in active code chunks are also dependencies. Use file_out() to tell drake that the target is a file output. 7.6.2 The old way drake has old wildcard templating functions to help generate plans. It is more difficult to adapt them to practical use cases, but they have been around since the early days of drake. Here are the commands to generate the bootstrapped datasets. my_datasets &lt;- drake_plan( small = simulate(48), large = simulate(64)) my_datasets #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 small simulate(48) #&gt; 2 large simulate(64) For multiple replicates: expand_plan(my_datasets, values = c(&quot;rep1&quot;, &quot;rep2&quot;)) #&gt; # A tibble: 4 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 small_rep1 simulate(48) #&gt; 2 small_rep2 simulate(48) #&gt; 3 large_rep1 simulate(64) #&gt; 4 large_rep2 simulate(64) Here is a template for applying our regression models to our bootstrapped datasets. methods &lt;- drake_plan( regression1 = reg1(dataset__), regression2 = reg2(dataset__)) methods #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 regression1 reg1(dataset__) #&gt; 2 regression2 reg2(dataset__) We evaluate the dataset__ wildcard to generate all the regression commands we need. my_analyses &lt;- evaluate_plan( methods, wildcard = &quot;dataset__&quot;, values = my_datasets$target ) my_analyses #&gt; # A tibble: 4 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 regression1_small reg1(small) #&gt; 2 regression1_large reg1(large) #&gt; 3 regression2_small reg2(small) #&gt; 4 regression2_large reg2(large) Next, we summarize each analysis of each dataset. We calculate descriptive statistics on the residuals, and we collect the regression coefficients and their p-values. summary_types &lt;- drake_plan( summ = suppressWarnings(summary(analysis__$residuals)), coef = suppressWarnings(summary(analysis__))$coefficients ) summary_types #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 summ suppressWarnings(summary(analysis__$residuals)) #&gt; 2 coef suppressWarnings(summary(analysis__))$coefficients my_summaries &lt;- evaluate_plan( summary_types, wildcard = &quot;analysis__&quot;, values = my_analyses$target ) my_summaries #&gt; # A tibble: 8 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 summ_regression1_sma… suppressWarnings(summary(regression1_small$residua… #&gt; 2 summ_regression1_lar… suppressWarnings(summary(regression1_large$residua… #&gt; 3 summ_regression2_sma… suppressWarnings(summary(regression2_small$residua… #&gt; 4 summ_regression2_lar… suppressWarnings(summary(regression2_large$residua… #&gt; 5 coef_regression1_sma… suppressWarnings(summary(regression1_small))$coeff… #&gt; 6 coef_regression1_lar… suppressWarnings(summary(regression1_large))$coeff… #&gt; 7 coef_regression2_sma… suppressWarnings(summary(regression2_small))$coeff… #&gt; 8 coef_regression2_lar… suppressWarnings(summary(regression2_large))$coeff… For your knitr reports, use knitr_in() in your commands so that report.Rmd is a dependency and targets loaded with loadd() and readd() in active code chunks are also dependencies. Use file_out() to tell drake that the target is a file output. report &lt;- drake_plan( report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) ) report #&gt; # A tibble: 1 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) Finally, consolidate your workflow using rbind(). Row order does not matter. my_plan &lt;- rbind(report, my_datasets, my_analyses, my_summaries) my_plan #&gt; # A tibble: 15 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;),… #&gt; 2 small simulate(48) … #&gt; 3 large simulate(64) … #&gt; 4 regression1_small reg1(small) … #&gt; 5 regression1_large reg1(large) … #&gt; 6 regression2_small reg2(small) … #&gt; 7 regression2_large reg2(large) … #&gt; 8 summ_regression1_s… suppressWarnings(summary(regression1_small$residual… #&gt; 9 summ_regression1_l… suppressWarnings(summary(regression1_large$residual… #&gt; 10 summ_regression2_s… suppressWarnings(summary(regression2_small$residual… #&gt; 11 summ_regression2_l… suppressWarnings(summary(regression2_large$residual… #&gt; 12 coef_regression1_s… suppressWarnings(summary(regression1_small))$coeffi… #&gt; 13 coef_regression1_l… suppressWarnings(summary(regression1_large))$coeffi… #&gt; 14 coef_regression2_s… suppressWarnings(summary(regression2_small))$coeffi… #&gt; 15 coef_regression2_l… suppressWarnings(summary(regression2_large))$coeffi… 7.7 Run the workflow You may want to check for outdated or missing targets/imports first. config &lt;- drake_config(my_plan, verbose = FALSE) outdated(config) # Targets that need to be (re)built. #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; #&gt; [7] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [9] &quot;regression2_small&quot; &quot;report&quot; #&gt; [11] &quot;small&quot; &quot;summ_regression1_large&quot; #&gt; [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; #&gt; [15] &quot;summ_regression2_small&quot; missed(config) # Checks your workspace. #&gt; character(0) Then just make(my_plan). make(my_plan) #&gt; target large #&gt; target small #&gt; target regression1_large #&gt; target regression2_large #&gt; target regression1_small #&gt; target regression2_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large #&gt; target summ_regression2_large #&gt; target coef_regression2_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target report For the reg2() model on the small dataset, the p-value on x2 is so small that there may be an association between weight and fuel efficiency after all. readd(coef_regression2_small) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 27.8369441 1.07966324 25.782988 5.443436e-29 #&gt; x2 -0.6359335 0.07138918 -8.907981 1.408597e-11 The non-file dependencies of your last target are already loaded in your workspace. ls() #&gt; [1] &quot;args&quot; &quot;bad_config&quot; &quot;bad_plan&quot; #&gt; [4] &quot;check_credit_hours&quot; &quot;check_graduations&quot; &quot;check_public_funding&quot; #&gt; [7] &quot;check_students&quot; &quot;col&quot; &quot;combos&quot; #&gt; [10] &quot;command_function&quot; &quot;config&quot; &quot;config1&quot; #&gt; [13] &quot;config2&quot; &quot;covariates&quot; &quot;create_plot&quot; #&gt; [16] &quot;get_logs&quot; &quot;get_rmspe&quot; &quot;get_school_data&quot; #&gt; [19] &quot;good_config&quot; &quot;good_plan&quot; &quot;hard_plan&quot; #&gt; [22] &quot;hist&quot; &quot;latest_log_date&quot; &quot;lots_of_sds&quot; #&gt; [25] &quot;make_my_plot&quot; &quot;make_my_table&quot; &quot;meta_plan&quot; #&gt; [28] &quot;methods&quot; &quot;my_analyses&quot; &quot;my_datasets&quot; #&gt; [31] &quot;my_grid&quot; &quot;my_model_fit&quot; &quot;my_plan&quot; #&gt; [34] &quot;my_summaries&quot; &quot;plan&quot; &quot;plan_template&quot; #&gt; [37] &quot;plan1&quot; &quot;plan2&quot; &quot;plot_rmspe&quot; #&gt; [40] &quot;predictors&quot; &quot;Produc&quot; &quot;random_rows&quot; #&gt; [43] &quot;reg1&quot; &quot;reg2&quot; &quot;results&quot; #&gt; [46] &quot;simulate&quot; &quot;small_config&quot; &quot;small_plan&quot; #&gt; [49] &quot;summary_types&quot; &quot;tmp&quot; &quot;url&quot; #&gt; [52] &quot;vals&quot; &quot;write_command&quot; &quot;y_vals&quot; outdated(config) # Everything is up to date. #&gt; character(0) build_times(digits = 4) # How long did it take to make each target? #&gt; # A tibble: 15 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; #&gt; 1 coef_regression1_large 0.004s 0.004s 0s #&gt; 2 coef_regression1_small 0.003s 0s 0.004s #&gt; 3 coef_regression2_large 0.003s 0.004s 0s #&gt; 4 coef_regression2_small 0.003s 0s 0s #&gt; 5 large 0.004s 0.004s 0s #&gt; 6 regression1_large 0.005s 0.004s 0s #&gt; 7 regression1_small 0.007s 0.008s 0s #&gt; 8 regression2_large 0.005s 0s 0.004s #&gt; 9 regression2_small 0.008s 0.008s 0s #&gt; 10 report 0.053s 0.052s 0s #&gt; 11 small 0.011s 0.012s 0s #&gt; 12 summ_regression1_large 0.005s 0.004s 0s #&gt; 13 summ_regression1_small 0.003s 0s 0.004s #&gt; 14 summ_regression2_large 0.003s 0.004s 0s #&gt; 15 summ_regression2_small 0.003s 0s 0s See also predict_runtime() and rate_limiting_times(). In the new graph, the black nodes from before are now green. # Hover, click, drag, zoom, and explore. vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) Optionally, get visNetwork nodes and edges so you can make your own plot with visNetwork() or render_drake_graph(). drake_graph_info(config) Use readd() and loadd() to load targets into your workspace. (They are cached in the hidden .drake/ folder using storr). There are many more functions for interacting with the cache. readd(coef_regression2_large) #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 27.7764504 0.85871209 32.34664 1.569624e-40 #&gt; x2 -0.7056179 0.06942808 -10.16329 7.950479e-15 loadd(small) head(small) #&gt; x y #&gt; 1 5.424 10.4 #&gt; 2 3.440 17.8 #&gt; 3 3.440 19.2 #&gt; 4 3.170 15.8 #&gt; 5 3.730 17.3 #&gt; 6 3.845 19.2 rm(small) cached() #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; #&gt; [7] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [9] &quot;regression2_small&quot; &quot;report&quot; #&gt; [11] &quot;small&quot; &quot;summ_regression1_large&quot; #&gt; [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; #&gt; [15] &quot;summ_regression2_small&quot; drake::progress() #&gt; # A tibble: 15 x 2 #&gt; target progress #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 coef_regression1_large done #&gt; 2 coef_regression1_small done #&gt; 3 coef_regression2_large done #&gt; 4 coef_regression2_small done #&gt; 5 large done #&gt; 6 regression1_large done #&gt; 7 regression1_small done #&gt; 8 regression2_large done #&gt; 9 regression2_small done #&gt; 10 report done #&gt; 11 small done #&gt; 12 summ_regression1_large done #&gt; 13 summ_regression1_small done #&gt; 14 summ_regression2_large done #&gt; 15 summ_regression2_small done The next time you run make(my_plan), nothing will build because drake knows everything is already up to date. make(my_plan) #&gt; All targets are already up to date. But if you change one of your functions, commands, or other dependencies, drake will update the affected targets. Suppose we change the quadratic term to a cubic term in reg2(). We might want to do this if we suspect a cubic relationship between tons and miles per gallon. reg2 &lt;- function(d) { d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } The targets that depend on reg2() need to be rebuilt. config &lt;- drake_config(my_plan) outdated(config) #&gt; [1] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [3] &quot;regression2_large&quot; &quot;regression2_small&quot; #&gt; [5] &quot;report&quot; &quot;summ_regression2_large&quot; #&gt; [7] &quot;summ_regression2_small&quot; Advanced: To get a rough idea of why a target is out of date, you can use dependency_profile(). It will tell you if any of the following changed since the last make(): The command in the drake plan. At least one non-file dependency. (For this, the imports have to be up to date and cached, either with make(), make(skip_targets = TRUE), outdated(), or similar.) At least one input file declared with file_in() or knitr_in(). At least one output file declared with file_out(). dependency_profile(target = regression2_small, config = config) #&gt; Warning: dependency_profile() in drake is deprecated. Use deps_profile() #&gt; instead. #&gt; # A tibble: 4 x 4 #&gt; hash changed old_hash new_hash #&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 command FALSE 39e1321ff2265cac 39e1321ff2265cac #&gt; 2 depend TRUE ab16d2a0b3c4d844 4d6e115872246cea #&gt; 3 file_in FALSE &quot;&quot; &quot;&quot; #&gt; 4 file_out FALSE &quot;&quot; &quot;&quot; # Hover, click, drag, zoom, and explore. vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) The next make() will rebuild the targets depending on reg2() and leave everything else alone. make(my_plan) #&gt; target regression2_small #&gt; target regression2_large #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target summ_regression2_large #&gt; target coef_regression2_large #&gt; target report Trivial changes to whitespace and comments are totally ignored. reg2 &lt;- function(d) { d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) # I indented here. } outdated(config) # Everything is up to date. #&gt; character(0) drake cares about nested functions too: nontrivial changes to random_rows() will propagate to simulate() and all the downstream targets. random_rows &lt;- function(data, n){ n &lt;- n + 1 data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } outdated(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; #&gt; [7] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [9] &quot;regression2_small&quot; &quot;report&quot; #&gt; [11] &quot;small&quot; &quot;summ_regression1_large&quot; #&gt; [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; #&gt; [15] &quot;summ_regression2_small&quot; make(my_plan) #&gt; target large #&gt; target small #&gt; target regression1_large #&gt; target regression2_large #&gt; target regression1_small #&gt; target regression2_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large #&gt; target summ_regression2_large #&gt; target coef_regression2_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target report Need to add new work on the fly? Just append rows to the drake plan. If the rest of your workflow is up to date, only the new work is run. new_simulation &lt;- function(n){ data.frame(x = rnorm(n), y = rnorm(n)) } additions &lt;- drake_plan( new_data = new_simulation(36) + sqrt(10)) additions #&gt; # A tibble: 1 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 new_data new_simulation(36) + sqrt(10) my_plan &lt;- rbind(my_plan, additions) my_plan #&gt; # A tibble: 16 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;),… #&gt; 2 small simulate(48) … #&gt; 3 large simulate(64) … #&gt; 4 regression1_small reg1(small) … #&gt; 5 regression1_large reg1(large) … #&gt; 6 regression2_small reg2(small) … #&gt; 7 regression2_large reg2(large) … #&gt; 8 summ_regression1_s… suppressWarnings(summary(regression1_small$residual… #&gt; 9 summ_regression1_l… suppressWarnings(summary(regression1_large$residual… #&gt; 10 summ_regression2_s… suppressWarnings(summary(regression2_small$residual… #&gt; 11 summ_regression2_l… suppressWarnings(summary(regression2_large$residual… #&gt; 12 coef_regression1_s… suppressWarnings(summary(regression1_small))$coeffi… #&gt; 13 coef_regression1_l… suppressWarnings(summary(regression1_large))$coeffi… #&gt; 14 coef_regression2_s… suppressWarnings(summary(regression2_small))$coeffi… #&gt; 15 coef_regression2_l… suppressWarnings(summary(regression2_large))$coeffi… #&gt; 16 new_data new_simulation(36) + sqrt(10) … make(my_plan) #&gt; target new_data If you ever need to erase your work, use clean(). The next make() will rebuild any cleaned targets, so be careful. You may notice that by default, the size of the cache does not go down very much. To purge old data, you could use clean(garbage_collection = TRUE, purge = TRUE). To do garbage collection without removing any important targets, use drake_gc(). # Uncaches individual targets and imported objects. clean(small, reg1, verbose = FALSE) clean(verbose = FALSE) # Cleans all targets out of the cache. drake_gc(verbose = FALSE) # Just garbage collection. clean(destroy = TRUE, verbose = FALSE) # removes the cache entirely "],
["vis.html", "Chapter 8 Visualization with drake 8.1 Underlying graph data: node and edge data frames 8.2 Visualizing target status 8.3 Subgraphs 8.4 Control the vis_drake_graph() legend. 8.5 Clusters 8.6 Output files", " Chapter 8 Visualization with drake Data analysis projects have complicated networks of dependencies, and drake can help you visualize them with vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() (note the two g’s). 8.0.1 vis_drake_graph() Powered by visNetwork. Colors represent target status, and shapes represent data type. These graphs are interactive, so you can click, drag, zoom, and and pan to adjust the size and position. Double-click on nodes to contract neighborhoods into clusters or expand them back out again. If you hover over a node, you will see text in a tooltip showing the first few lines of The command of a target, or The body of an imported function, or The content of an imported text file. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). config &lt;- drake_config(my_plan) vis_drake_graph(config) To save this interactive widget for later, just supply the name of an HTML file. vis_drake_graph(config, file = &quot;graph.html&quot;) To save a static image file, supply a file name that ends in &quot;.png&quot;, &quot;.pdf&quot;, &quot;.jpeg&quot;, or &quot;.jpg&quot;. vis_drake_graph(config, file = &quot;graph.png&quot;) 8.0.2 sankey_drake_graph() These interactive networkD3 Sankey diagrams have more nuance: the height of each node is proportional to its number of connections. Nodes with many incoming connnections tend to fall out of date more often, and nodes with many outgoing connections can invalidate bigger chunks of the downstream pipeline. sankey_drake_graph(config) Saving the graphs is the same as before. sankey_drake_graph(config, file = &quot;graph.html&quot;) # Interactive HTML widget sankey_drake_graph(config, file = &quot;graph.png&quot;) # Static image file Unfortunately, a legend is not yet available for Sankey diagrams, but drake exposes a separate legend for the colors and shapes. library(visNetwork) legend_nodes() #&gt; # A tibble: 10 x 6 #&gt; label color shape font.color font.size id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 Up to date #228B22 dot black 20 1 #&gt; 2 Outdated #000000 dot black 20 2 #&gt; 3 Running #FF7221 dot black 20 3 #&gt; 4 Failed #AA0000 dot black 20 4 #&gt; 5 Imported #1874CD dot black 20 5 #&gt; 6 Missing #9A32CD dot black 20 6 #&gt; 7 Object #888888 dot black 20 7 #&gt; 8 Function #888888 triangle black 20 8 #&gt; 9 File #888888 square black 20 9 #&gt; 10 Cluster #888888 diamond black 20 10 visNetwork(nodes = legend_nodes()) 8.0.3 drake_ggraph() Powered by ggraph, these graphs are static ggplot2 objects, and you can save them with ggsave(). plan &lt;- drake_plan(data = get_data(), model = data, plot = data) plan #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 data get_data() #&gt; 2 model data #&gt; 3 plot data get_data &lt;- function(){} config &lt;- drake_config(plan) drake_ggraph(config) 8.1 Underlying graph data: node and edge data frames drake_graph_info() is used behind the scenes in vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() to get the graph information ready for rendering. To save time, you can call drake_graph_info() to get these internals and then call render_drake_graph(), render_sankey_drake_graph(), or render_drake_ggraph(). str(drake_graph_info(config)) #&gt; List of 4 #&gt; $ nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 10 variables: #&gt; ..$ id : chr [1:4] &quot;data&quot; &quot;get_data&quot; &quot;model&quot; &quot;plot&quot; #&gt; ..$ imported : logi [1:4] FALSE TRUE FALSE FALSE #&gt; ..$ label : chr [1:4] &quot;data&quot; &quot;get_data&quot; &quot;model&quot; &quot;plot&quot; #&gt; ..$ status : chr [1:4] &quot;outdated&quot; &quot;imported&quot; &quot;outdated&quot; &quot;outdated&quot; #&gt; ..$ type : chr [1:4] &quot;object&quot; &quot;function&quot; &quot;object&quot; &quot;object&quot; #&gt; ..$ font.size: num [1:4] 20 20 20 20 #&gt; ..$ color : chr [1:4] &quot;#000000&quot; &quot;#1874CD&quot; &quot;#000000&quot; &quot;#000000&quot; #&gt; ..$ shape : chr [1:4] &quot;dot&quot; &quot;triangle&quot; &quot;dot&quot; &quot;dot&quot; #&gt; ..$ level : num [1:4] 2 1 3 3 #&gt; ..$ title : chr [1:4] &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; &quot;Call drake_graph_info(hover = TRUE) for informative text.&quot; #&gt; $ edges :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3 obs. of 4 variables: #&gt; ..$ from : chr [1:3] &quot;get_data&quot; &quot;data&quot; &quot;data&quot; #&gt; ..$ to : chr [1:3] &quot;data&quot; &quot;model&quot; &quot;plot&quot; #&gt; ..$ arrows: chr [1:3] &quot;to&quot; &quot;to&quot; &quot;to&quot; #&gt; ..$ smooth: logi [1:3] TRUE TRUE TRUE #&gt; $ legend_nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 6 variables: #&gt; ..$ label : chr [1:4] &quot;Outdated&quot; &quot;Imported&quot; &quot;Object&quot; &quot;Function&quot; #&gt; ..$ color : chr [1:4] &quot;#000000&quot; &quot;#1874CD&quot; &quot;#888888&quot; &quot;#888888&quot; #&gt; ..$ shape : chr [1:4] &quot;dot&quot; &quot;dot&quot; &quot;dot&quot; &quot;triangle&quot; #&gt; ..$ font.color: chr [1:4] &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; #&gt; ..$ font.size : num [1:4] 20 20 20 20 #&gt; ..$ id : int [1:4] 2 5 7 8 #&gt; $ default_title: chr &quot;Dependency graph&quot; 8.2 Visualizing target status drake’s visuals tell you which targets are up to date and which are outdated. make(my_plan, verbose = FALSE) config &lt;- drake_config(my_plan, jobs = 2, verbose = FALSE) #&gt; Warning: In make(), `parallelism` should not be &quot;loop&quot; if `jobs` &gt; 1 outdated(config) #&gt; character(0) sankey_drake_graph(config) When you change a dependency, some targets fall out of date (black nodes). reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } sankey_drake_graph(config) 8.3 Subgraphs Graphs can grow enormous for serious projects, so there are multiple ways to focus on a manageable subgraph. The most brute-force way is to just pick a manual subset of nodes. However, with the subset argument, the graphing functions can drop intermediate nodes and edges. vis_drake_graph( config, subset = c(&quot;regression2_small&quot;, &quot;large&quot;) ) The rest of the subgraph functionality preserves connectedness. Use targets_only to ignore the imports. vis_drake_graph(config, targets_only = TRUE) Similarly, you can just show downstream nodes. vis_drake_graph(config, from = c(&quot;regression2_small&quot;, &quot;regression2_large&quot;)) Or upstream ones. vis_drake_graph(config, from = &quot;small&quot;, mode = &quot;in&quot;) In fact, let us just take a small neighborhood around a target in both directions. For the graph below, given order is 1, but all the custom file_out() output files of the neighborhood’s targets appear as well. This ensures consistent behavior between show_output_files = TRUE and show_output_files = FALSE (more on that later). vis_drake_graph(config, from = &quot;small&quot;, mode = &quot;all&quot;, order = 1) 8.4 Control the vis_drake_graph() legend. Some arguments to vis_drake_graph() control the legend. vis_drake_graph(config, full_legend = TRUE, ncol_legend = 2) To remove the legend altogether, set the ncol_legend argument to 0. vis_drake_graph(config, ncol_legend = 0) 8.5 Clusters With the group and clusters arguments to the graphing functions, you can condense nodes into clusters. This is handy for workflows with lots of targets. Take the schools scenario from the drake plan guide. Our plan was generated with drake_plan(trace = TRUE), so it has wildcard columns that group nodes into natural clusters already. You can manually add such columns if you wish. # Visit https://ropenscilabs.github.io/drake-manual/plans.html#create-large-plans-the-easy-way # to learn about the syntax with target(transform = ...). plan &lt;- drake_plan( school = target( get_school_data(id), transform = map(id = c(1, 2, 3)) ), credits = target( fun(school), transform = cross( school, fun = c(check_credit_hours, check_students, check_graduations) ) ), public_funds_school = target( command = check_public_funding(school), transform = map(school = c(school_1, school_2)) ), trace = TRUE ) plan #&gt; # A tibble: 14 x 7 #&gt; target command id school public_funds_sch… fun credits #&gt; &lt;chr&gt; &lt;expr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 school_1 get_school… 1 school… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 school_2 get_school… 2 school… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 school_3 get_school… 3 school… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 credits_c… check_cred… 1 school… &lt;NA&gt; check… credits_c… #&gt; 5 credits_c… check_stud… 1 school… &lt;NA&gt; check… credits_c… #&gt; 6 credits_c… check_grad… 1 school… &lt;NA&gt; check… credits_c… #&gt; 7 credits_c… check_cred… 2 school… &lt;NA&gt; check… credits_c… #&gt; 8 credits_c… check_stud… 2 school… &lt;NA&gt; check… credits_c… #&gt; 9 credits_c… check_grad… 2 school… &lt;NA&gt; check… credits_c… #&gt; 10 credits_c… check_cred… 3 school… &lt;NA&gt; check… credits_c… #&gt; 11 credits_c… check_stud… 3 school… &lt;NA&gt; check… credits_c… #&gt; 12 credits_c… check_grad… 3 school… &lt;NA&gt; check… credits_c… #&gt; 13 public_fu… check_publ… &lt;NA&gt; school… public_funds_sch… &lt;NA&gt; &lt;NA&gt; #&gt; 14 public_fu… check_publ… &lt;NA&gt; school… public_funds_sch… &lt;NA&gt; &lt;NA&gt; Ordinarily, the workflow graph gives a separate node to each individual import object or target. config &lt;- drake_config(plan) vis_drake_graph(config) For large projects with hundreds of nodes, this can get quite cumbersome. But here, we can choose a wildcard column (or any other column in the plan, even custom columns) to condense nodes into natural clusters. For the group argument to the graphing functions, choose the name of a column in plan or a column you know will be in drake_graph_info(config)$nodes. Then for clusters, choose the values in your group column that correspond to nodes you want to bunch together. The new graph is not as cumbersome. config &lt;- drake_config(plan) vis_drake_graph(config, group = &quot;school&quot;, clusters = c(&quot;school_1&quot;, &quot;school_2&quot;, &quot;school_3&quot;) ) As I mentioned, you can group on any column in drake_graph_info(config)$nodes. Let’s return to the mtcars project for demonstration. config &lt;- drake_config(my_plan) vis_drake_graph(config) Let’s condense all the imports into one node and all the up-to-date targets into another. That way, the outdated targets stand out. vis_drake_graph( config, group = &quot;status&quot;, clusters = c(&quot;imported&quot;, &quot;up to date&quot;) ) 8.6 Output files drake can reproducibly track multiple output files per target and show them in the graph. plan &lt;- drake_plan( target1 = { file.copy(file_in(&quot;in1.txt&quot;), file_out(&quot;out1.txt&quot;)) file.copy(file_in(&quot;in2.txt&quot;), file_out(&quot;out2.txt&quot;)) }, target2 = { file.copy(file_in(&quot;out1.txt&quot;), file_out(&quot;out3.txt&quot;)) file.copy(file_in(&quot;out2.txt&quot;), file_out(&quot;out4.txt&quot;)) } ) writeLines(&quot;in1&quot;, &quot;in1.txt&quot;) writeLines(&quot;in2&quot;, &quot;in2.txt&quot;) make(plan) #&gt; target target1 #&gt; target target2 config &lt;- drake_config(plan) writeLines(&quot;abcdefg&quot;, &quot;out3.txt&quot;) vis_drake_graph(config, targets_only = TRUE) If your graph is too busy, you can hide the output files with show_output_files = FALSE. vis_drake_graph(config, show_output_files = FALSE, targets_only = TRUE) "],
["debug.html", "Chapter 9 Debugging and testing drake projects 9.1 Dependencies 9.2 Diagnose failures. 9.3 Timeouts and retries 9.4 More help", " Chapter 9 Debugging and testing drake projects This chapter is a guide to debugging and testing drake projects. 9.1 Dependencies drake automatically detects dependency relationships among your targets and imports. While this is convenient most of the time, it can lead to some pitfalls. This section describes techniques to understand you project’s dependency structure and diagnose and debug issues. 9.1.1 Visualize your dependency graph. To avoid frustration early on, please use drake’s dependency graph visualizations to see how the steps of your workflow fit together. drake resolves the dependency relationships in the graph by analyzing the code in your commands and the functions in your environment. load_mtcars_example() config &lt;- drake_config(my_plan) # Hover, click, drag, zoom, and pan. See args &#39;from&#39; and &#39;to&#39;. vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) 9.1.2 Check specific dependency information. With the deps_code() function, you can see for yourself how drake detects first-order dependencies from code. print(simulate) #&gt; function (n) #&gt; { #&gt; data &lt;- random_rows(data = datasets::mtcars, n = n) #&gt; data.frame(x = data$wt, y = data$mpg) #&gt; } deps_code(simulate) #&gt; # A tibble: 5 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 data.frame globals #&gt; 2 mpg globals #&gt; 3 wt globals #&gt; 4 random_rows globals #&gt; 5 datasets::mtcars namespaced # knitr_in() makes sure your target depends on `report.Rmd` # and any dependencies loaded with loadd() and readd() # in the report&#39;s active code chunks. my_plan$command[[1]] #&gt; knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) deps_code(my_plan$command[[1]]) #&gt; # A tibble: 6 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 knit globals #&gt; 2 large loadd #&gt; 3 small readd #&gt; 4 coef_regression2_small readd #&gt; 5 report.md file_out #&gt; 6 report.Rmd knitr_in my_plan$command[[nrow(my_plan)]] #&gt; suppressWarnings(summary(regression2_large))$coefficients deps_code(my_plan$command[[nrow(my_plan)]]) #&gt; # A tibble: 4 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 summary globals #&gt; 2 suppressWarnings globals #&gt; 3 regression2_large globals #&gt; 4 coefficients globals With deps_target(), you can see the dependencies that drake has already detected for your targets and imports. deps_target(&quot;simulate&quot;, config) #&gt; # A tibble: 2 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 random_rows globals #&gt; 2 datasets::mtcars namespaced deps_target(&quot;small&quot;, config) #&gt; # A tibble: 1 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 simulate globals deps_target(&quot;report&quot;, config) #&gt; # A tibble: 5 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 large loadd #&gt; 2 small readd #&gt; 3 coef_regression2_small readd #&gt; 4 report.md file_out #&gt; 5 report.Rmd knitr_in And with tracked(), you can list all the reproducibly tracked objects and files. tracked(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;datasets::mtcars&quot; #&gt; [7] &quot;file report.Rmd&quot; &quot;file report.md&quot; #&gt; [9] &quot;random_rows&quot; &quot;reg1&quot; #&gt; [11] &quot;reg2&quot; &quot;regression1_large&quot; #&gt; [13] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [15] &quot;regression2_small&quot; &quot;report&quot; #&gt; [17] &quot;simulate&quot; &quot;small&quot; #&gt; [19] &quot;summ_regression1_large&quot; &quot;summ_regression1_small&quot; #&gt; [21] &quot;summ_regression2_large&quot; &quot;summ_regression2_small&quot; 9.1.3 Outdated targets and missing dependencies missed() shows any imports missing from your environment missed(config) # Nothing is missing right now. #&gt; character(0) outdated() reports any targets that are outdated. outdated(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; #&gt; [7] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [9] &quot;regression2_small&quot; &quot;report&quot; #&gt; [11] &quot;small&quot; &quot;summ_regression1_large&quot; #&gt; [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; #&gt; [15] &quot;summ_regression2_small&quot; make(my_plan) #&gt; target large #&gt; target small #&gt; target regression1_large #&gt; target regression2_large #&gt; target regression1_small #&gt; target regression2_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large #&gt; target summ_regression2_large #&gt; target coef_regression2_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target report outdated(config) #&gt; character(0) 9.1.4 But why are my targets out of date? drake has the option to produce a cache log with the fingerprint of every target and import. head(drake_cache_log()) #&gt; # A tibble: 6 x 3 #&gt; hash type name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 4e6e4d4c0bcda263 target coef_regression1_large #&gt; 2 5ad25785a84e0cac target coef_regression1_small #&gt; 3 b2662785d55b28c1 target coef_regression2_large #&gt; 4 23c66b36be56905b target coef_regression2_small #&gt; 5 7b6504d4c0dcceab target large #&gt; 6 f4b89e63bc92af79 import datasets::mtcars We highly recommend that you automatically produce a cache log file on every make() and put it under version control with the rest of your project. make(my_plan, cache_log_file = &quot;cache_log.txt&quot;) #&gt; All targets are already up to date. read.table(&quot;cache_log.txt&quot;, nrows = 6, header = TRUE) #&gt; hash type name #&gt; 1 4e6e4d4c0bcda263 target coef_regression1_large #&gt; 2 5ad25785a84e0cac target coef_regression1_small #&gt; 3 b2662785d55b28c1 target coef_regression2_large #&gt; 4 23c66b36be56905b target coef_regression2_small #&gt; 5 7b6504d4c0dcceab target large #&gt; 6 f4b89e63bc92af79 import datasets::mtcars Suppose we go back and add input checking to one of our functions. print(random_rows) #&gt; function (data, n) #&gt; { #&gt; data[sample.int(n = nrow(data), size = n, replace = TRUE), #&gt; ] #&gt; } #&gt; &lt;bytecode: 0x157f19a8&gt; random_rows &lt;- function(data, n){ stopifnot(n &gt; 0) data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } Then, we forget to run make() again, and we leave the the project for several months. When we come back, all our targets are suddenly out of date. outdated(config) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; #&gt; [7] &quot;regression1_small&quot; &quot;regression2_large&quot; #&gt; [9] &quot;regression2_small&quot; &quot;report&quot; #&gt; [11] &quot;small&quot; &quot;summ_regression1_large&quot; #&gt; [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; #&gt; [15] &quot;summ_regression2_small&quot; At first, we may not know why all our targets are outdated. But we can generate another cache log and check any hashes that changed. Our call to outdated() already re-cached the imports, so any changed imports will show up in the new log file. drake_cache_log_file(file = &quot;cache_log2.txt&quot;) #&gt; Warning: `drake_cache_log_file()` is deprecated. To ensure cache log #&gt; is always up to date, create the cache log using `make()` with the #&gt; `cache_log_file` argument. system2(&quot;diff&quot;, &quot;cache_log.txt cache_log2.txt&quot;, stdout = TRUE) %&gt;% cat(sep = &quot;\\n&quot;) #&gt; Warning in system2(&quot;diff&quot;, &quot;cache_log.txt cache_log2.txt&quot;, stdout = TRUE): #&gt; running command &#39;&#39;diff&#39; cache_log.txt cache_log2.txt&#39; had status 1 #&gt; 10c10 #&gt; &lt; db84c9f752635a13 import random_rows #&gt; --- #&gt; &gt; 089a5c1023c1c51a import random_rows #&gt; 18c18 #&gt; &lt; f4d57e5ba38b744b import simulate #&gt; --- #&gt; &gt; 739a28336ceb4da2 import simulate Now, we see that random_rows() has changed since last time, and we have a new dependency stopifnot(). simulate() shows up in the changes too, but only because random_rows() is nested in the body of simulate(). If we revert random_rows() to its original state, all our targets are up to date again. random_rows &lt;- function(data, n){ data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } outdated(config) #&gt; character(0) drake_cache_log_file(file = &quot;cache_log3.txt&quot;) #&gt; Warning: `drake_cache_log_file()` is deprecated. To ensure cache log #&gt; is always up to date, create the cache log using `make()` with the #&gt; `cache_log_file` argument. system2(&quot;diff&quot;, &quot;cache_log.txt cache_log3.txt&quot;, stdout = TRUE) #&gt; character(0) 9.2 Diagnose failures. drake records diagnostic metadata on all your targets, including the latest errors, warnings, messages, and other bits of context. f &lt;- function(x){ if (x &lt; 0){ stop(&quot;`x` cannot be negative.&quot;) } x } bad_plan &lt;- drake_plan( a = 12, b = -a, my_target = f(b) ) bad_plan #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 a 12 #&gt; 2 b -a #&gt; 3 my_target f(b) withr::with_message_sink( new = stdout(), make(bad_plan) ) #&gt; target a #&gt; target b #&gt; target my_target #&gt; fail my_target #&gt; Error: Target `my_target` failed. Call `diagnose(my_target)` for details. Error message: #&gt; `x` cannot be negative. failed(verbose = FALSE) # from the last make() only #&gt; [1] &quot;my_target&quot; # See also warnings and messages. error &lt;- diagnose(my_target, verbose = FALSE)$error error$message #&gt; [1] &quot;`x` cannot be negative.&quot; error$call #&gt; f(b) str(error$calls) # View the traceback. #&gt; List of 8 #&gt; $ : language local({ f(b) ... #&gt; $ : language eval.parent(substitute(eval(quote(expr), envir))) #&gt; $ : language eval(expr, p) #&gt; $ : language eval(expr, p) #&gt; $ : language eval(quote({ f(b) ... #&gt; $ : language eval(quote({ f(b) ... #&gt; $ : language f(b) #&gt; $ : language stop(&quot;`x` cannot be negative.&quot;) #&gt; ..- attr(*, &quot;srcref&quot;)= &#39;srcref&#39; int [1:8] 3 5 3 35 5 35 3 3 #&gt; .. ..- attr(*, &quot;srcfile&quot;)=Classes &#39;srcfilecopy&#39;, &#39;srcfile&#39; &lt;environment: 0x12ff0bc0&gt; To figure out what went wrong, you could try to build the failed target interactively. To do that, simply call drake_build() or drake_debug(). These functions first call loadd(deps = TRUE) to load any missing dependencies (see the replace argument here) and then build your target. drake_build() simply runs the command, and drake_debug() runs the command in debug mode using debugonce(). # Pretend we just opened a new R session. library(drake) # Unloads target `b`. config &lt;- drake_config(plan = bad_plan) # my_target depends on b. &quot;b&quot; %in% ls() #&gt; [1] FALSE # Try to build my_target until the error is fixed. # Skip all that pesky work checking dependencies. drake_build(my_target, config = config) # See also drake_debug(). #&gt; target my_target #&gt; fail my_target #&gt; Error: Target `my_target` failed. Call `diagnose(my_target)` for details. Error message: #&gt; `x` cannot be negative. # The target failed, but the dependency was loaded. &quot;b&quot; %in% ls() #&gt; [1] FALSE # What was `b` again? b #&gt; Error in eval(expr, envir, enclos): object &#39;b&#39; not found # How was `b` used? diagnose(my_target)$message #&gt; NULL diagnose(my_target)$call #&gt; NULL f #&gt; function(x){ #&gt; if (x &lt; 0){ #&gt; stop(&quot;`x` cannot be negative.&quot;) #&gt; } #&gt; x #&gt; } #&gt; &lt;bytecode: 0x14aa4278&gt; # Aha! The error was in f(). Let&#39;s fix it and try again. f &lt;- function(x){ x &lt;- abs(x) if (x &lt; 0){ stop(&quot;`x` cannot be negative.&quot;) } x } # Now it works! # Since you called make() previously, `config` is read from the cache # if you do not supply it. drake_build(my_target, config) # See also drake_debug(). #&gt; target my_target readd(my_target) #&gt; [1] 12 9.3 Timeouts and retries See the elapsed, cpu, and retries argument to make(). clean(verbose = FALSE) f &lt;- function(...){ Sys.sleep(1) } debug_plan &lt;- drake_plan(x = 1, y = f(x)) debug_plan #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 x 1 #&gt; 2 y f(x) withr::with_message_sink( stdout(), make(debug_plan, elapsed = 1e-3, retries = 2) ) #&gt; target x #&gt; target y #&gt; retry y: 1 of 2 To tailor these settings to each individual target, create new elapsed, cpu, or retries columns in your drake plan. These columns override the analogous arguments to make(). clean(verbose = FALSE) debug_plan$elapsed &lt;- c(1e-3, 2e-3) debug_plan$retries &lt;- 1:2 debug_plan #&gt; # A tibble: 2 x 4 #&gt; target command elapsed retries #&gt; &lt;chr&gt; &lt;expr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 x 1 0.001 1 #&gt; 2 y f(x) 0.002 2 withr::with_message_sink( new = stdout(), make(debug_plan, elapsed = Inf, retries = 0) ) #&gt; target x #&gt; target y #&gt; fail y #&gt; Error: Target `y` failed. Call `diagnose(y)` for details. Error message: #&gt; reached elapsed time limit 9.4 More help Please also see the compendium of cautionary notes, which addresses drake’s known edge cases, pitfalls, and weaknesses that may or may not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the issue tracker, where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version. "],
["hpc.html", "Chapter 10 High-performance computing 10.1 Batch mode for long workflows 10.2 Let make() schedule your targets. 10.3 Parallel backends 10.4 The clustermq backend 10.5 The future backend 10.6 Advanced options", " Chapter 10 High-performance computing This chapter provides guidance on time-consuming drake workflows and high-level parallel computation. library(drake) load_mtcars_example() make(my_plan, jobs = 2) 10.1 Batch mode for long workflows If you expect make() to take a long time, create a master script for your project (say, drake-work.R) and run it in a persistent background process. The following should work in the Mac/Linux terminal/shell. nohup nice -19 R CMD BATCH drake_work.R & where: nohup: Keep the job running even if you log out of the machine. nice -19: This is a low-priority job that should not consume many resources. Other processes should take priority. R CMD BATCH drake_work.R: Run the drake_work.R script in a new R session. &amp;: Run this job in the background so you can do other stuff in the terminal window. 10.2 Let make() schedule your targets. drake uses your project’s implicit dependency graph to figure out which targets can run in parallel and which ones need to wait for dependencies. load_mtcars_example() config &lt;- drake_config(my_plan) vis_drake_graph(config) You do not need to not micromanage the timing among targets, and you do not need to run parallel instances of make(). As the next sections describe, drake has built-in parallel and distributed computing support. 10.3 Parallel backends Choose the parallel backend with the parallelism argument and set the jobs argument to scale the work appropriately. make(my_plan, parallelism = &quot;future&quot;, jobs = 2) The two primary backends with long term support are clustermq and future. If you can install ZeroMQ, the best choice is usually clustermq. (It is faster than future.) However, future is more accessible: it does not require ZeroMQ, it supports parallel computing on Windows, it can work with more restrictive wall time limits on clusters, and it can deploy targets to Docker images (drake_example(&quot;Docker-psock&quot;)). 10.4 The clustermq backend 10.4.1 Persistent workers The make(parallelism = &quot;clustermq&quot;, jobs = 2) launches 2 parallel persistent workers. The master process assigns targets to workers, and the workers simultaneously traverse the dependency graph. 10.4.2 Installation You must first install ZeroMQ (instructions here) and then install the clustermq package. install.packages(&quot;clustermq&quot;) # CRAN release # Alternatively, install the GitHub development version. devtools::install_github(&quot;mschubert/clustermq&quot;, ref = &quot;develop&quot;) 10.4.3 On your local machine To run your targets in parallel over the cores of your local machine, set the global option below and run make(). options(clustermq.scheduler = &quot;multicore&quot;) make(plan, parallelism = &quot;clustermq&quot;, jobs = 2) 10.4.4 On a cluster Set the clustermq global options to register your computing resources. For SLURM: options(clustermq.scheduler = &quot;slurm&quot;, clustermq.template = &quot;slurm_clustermq.tmpl&quot;) Here, slurm_clustermq.tmpl is a template file with configuration details. Use drake_hpc_template_file() to write one of the available examples. drake_hpc_template_file(&quot;slurm_clustermq.tmpl&quot;) # Write the file slurm_clustermq.tmpl. After modifying slurm_clustermq.tmpl by hand to meet your needs, call make() as usual. make(plan, parallelism = &quot;clustermq&quot;, jobs = 4) 10.5 The future backend 10.5.1 Transient workers make(parallelism = &quot;future&quot;, jobs = 2) launches transient workers to build your targets. When a target is ready to build, the master process creates a fresh worker to build it, and the worker terminates when the target is done. jobs = 2 means that at most 2 transient workers are allowed to run at a given time. 10.5.2 Installation Install the future package. install.packages(&quot;future&quot;) # CRAN release # Alternatively, install the GitHub development version. devtools::install_github(&quot;HenrikBengtsson/future&quot;, ref = &quot;develop&quot;) If you intend to use a cluster, be sure to install the future.batchtools package too. The future ecosystem contains even more packages that extend future’s parallel computing functionality, such as future.callr. 10.5.3 On your local machine First, select a future plan to tell future how to create the workers. See this table for descriptions of the core options. future::plan(future::multiprocess) Next, run make(). make(plan, parallelism = &quot;future&quot;, jobs = 2) 10.5.4 On a cluster Install the future.batchtools package and use this list to select a future plan that matches your resources. You will also need a compatible template file with configuration details. As with clustermq, drake can generate some examples: drake_hpc_template_file(&quot;slurm_batchtools.tmpl&quot;) # Edit by hand. Next, register the template file with a plan. library(future.batchtools) future::plan(batchtools_slurm, template = &quot;slurm_batchtools.tmpl&quot;) Finally, run make(). make(plan, parallelism = &quot;future&quot;, jobs = 2) 10.6 Advanced options 10.6.1 Memory By default, make() keeps targets in memory during runtime. Some targets are dependencies of other targets downstream, while others may be no longer actually need to be in memory. The memory_strategy argument to make() allows you to choose the tradeoff that best suits your project. Options: &quot;speed&quot;: Once a target is loaded in memory, just keep it there. This choice maximizes speed and hogs memory. &quot;memory&quot;: Just before building each new target, unload everything from memory except the target’s direct dependencies. This option conserves memory, but it sacrifices speed because each new target needs to reload any previously unloaded targets from storage. &quot;lookahead&quot;: Just before building each new target, search the dependency graph to find targets that will not be needed for the rest of the current make() session. In this mode, targets are only in memory if they need to be loaded, and we avoid superfluous reads from the cache. However, searching the graph takes time, and it could even double the computational overhead for large projects. 10.6.2 Storage In make(caching = &quot;master&quot;), the workers send the targets to the master process, and the master process stores them one by one in the cache. caching = &quot;master&quot; is compatible with all storr cache formats, including the more esoteric ones like storr_dbi() and storr_environment(). In make(caching = &quot;worker&quot;), the parallel workers are responsible for writing the targets to the cache. Some output-heavy projects can benefit from this form of parallelism. However, it can sometimes add slowness on clusters due to lag from network file systems. And there are additional restrictions: All the workers must have the same file system and the same working directory as the master process. Only the default storr_rds() cache may be used. Other formats like storr_dbi() and storr_environment() cannot accommodate parallel cache operations. See the storage chapter for details. 10.6.3 The template argument for persistent workers For more control and flexibility in the clustermq backend, you can parameterize your template file and use the template argument of make(). For example, suppose you want to programatically set the number of “slots” (basically cores) per job on an SGE system (clustermq guide to SGE setup here). Begin with a parameterized template file sge_clustermq.tmpl with a custom n_slots placeholder. # File: sge_clustermq.tmpl # Modified from https://github.com/mschubert/clustermq/wiki/SGE #$ -N {{ job_name }} # job name #$ -t 1-{{ n_jobs }} # submit jobs as array #$ -j y # combine stdout/error in one file #$ -o {{ log_file | /dev/null }} # output file #$ -cwd # use pwd as work dir #$ -V # use environment variable #$ -pe smp {{ n_slots | 1 }} # request n_slots cores per job module load R ulimit -v $(( 1024 * {{ memory | 4096 }} )) CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ master }}&quot;)&#39; Then when you run make(), use the template argument to set n_slots. options(clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge_clustermq.tmpl&quot;) library(drake) load_mtcars_example() make( my_plan, parallelism = &quot;clustermq&quot;, jobs = 16, template = list(n_slots = 4) # Request 4 cores per persistent worker. ) Custom placeholders like n_slots are processed with the infuser package. 10.6.4 The resources column for transient workers Different targets may need different resources. For example, plan &lt;- drake_plan( data = download_data(), model = big_machine_learning_model(data) ) The model needs a GPU and multiple CPU cores, and the data only needs the bare minimum resources. Declare these requirements in a new list column of the plan. Here, each element is a named list for the resources argument of future::future(). plan$resources &lt;- list( list(cores = 1, gpus = 0), list(cores = 4, gpus = 1) ) Next, plug your resources into the brew patterns of your batchtools template file. The following sge_batchtools.tmpl file shows how to do it, but the file itself probably requires modification before it will work with your own machine. #!/bin/bash #$ -cwd #$ -j y #$ -o &lt;%= log.file %&gt; #$ -V #$ -N &lt;%= job.name %&gt; #$ -pe smp &lt;%= resources[[&quot;cores&quot;]] %&gt; # CPU cores #$ -l gpu=&lt;%= resources[[&quot;gpus&quot;]] %&gt; # GPUs. Rscript -e &#39;batchtools::doJobCollection(&quot;&lt;%= uri %&gt;&quot;)&#39; exit 0 Finally, register the template file and run your project. library(drake) library(future.batchtools) future::plan(batchtools_sge, template = &quot;sge_batchtools.tmpl&quot;) make(plan, parallelism = &quot;future&quot;, jobs = 2) 10.6.5 Custom job schedulers It is possible to supply a custom job scheduler function to the parallelism argument of make(). The backend_future_lapply_staged() function from the drake.future.lapply.staged package is an example. You might consider writing your own such function if you wish to Experiment with a more efficient job scheduler before proposing a patch to core drake, or Aggressively optimize drake for your specialized computing resources. This feature is very advanced, and you should only attempt it in production if you really know what you are doing. Use at your own risk. 10.6.6 Parallel computing within targets You may wish to invoke parallel computing within individual targets, e.g. plan &lt;- drake_plan( a = parallel::mclapply(1:8, sqrt, mc.cores = 4), b = parallel::mclapply(1:4, sqrt, mc.cores = 2) ) or even plan &lt;- drake_plan( a = target( parallel::mclapply( 1:8, sqrt, # You can change the # of cores without changing the command. mc.cores = from_plan(&quot;cores&quot;) ), cores = 4 # Changes to this number do not invalidate `a`. ), b = target( parallel::mclapply(1:4, sqrt, mc.cores = from_plan(&quot;cores&quot;)), cores = 2 ) ) Unfortunately, for reasons described here and here, make(plan) will fail in each case. Workarounds: Avoid mclapply(). furrr::map() and parallel::parLapply() are more dependable alternatives anyway. In the case of furrr, invoke future::plan(future.callr::callr) or future::plan(future::multisession) first. In make(), set the lock_envir argument to FALSE. This approach deactivates important reproducibility guardrails, so use with caution. In mclapply(), set the mc.set.seed argument to FALSE. If your computations require pseudo-random numbers (rnorm(), runif(), etc.) you will need to manually set a different seed for each parallel process, e.g. parallel::mclapply(X = 1:4, mc.cores = 4, FUN = function(i) { set.seed(sum(.Random.seed) + i) # Do some work... }) 10.6.7 Hasty mode The drake.hasty package is a bare-bones spin-off of drake. It sacrifices reproducibility to aggressively boost speed when scheduling and executing your targets. It is not recommended for most serious production use cases, but it can useful for experimentation. "],
["triggers.html", "Chapter 11 Triggers: decision rules for building targets 11.1 What are triggers? 11.2 Customization 11.3 Alternative trigger modes 11.4 Consider hasty mode 11.5 A more practical example", " Chapter 11 Triggers: decision rules for building targets When you call make(), drake tries to skip as many targets as possible. If it thinks a command will return the same value as last time, it does not bother running it. In other words, drake is lazy, and laziness saves you time. 11.1 What are triggers? To figure out whether it can skip a target, drake goes through an intricate checklist of triggers: The missing trigger: Do we lack a return value from a previous make()? Maybe you are building the target for the first time or you removed it from the cache with clean(). The command trigger: did the command in the drake plan change nontrivially since the last make()? Changes to spacing, formatting, and comments are ignored. The depend trigger: did any non-file dependencies change since the last make()? These could be: Other targets. Imported objects. Imported functions (ignoring changes to spacing, formatting, and comments). Any dependencies of imported functions. Any dependencies of dependencies of imported functions, and so on. The file trigger: did any file inputs or file outputs change since the last make()? These files are the ones explicitly declared in the command with file_in(), knitr_in(), and file_out(). The condition trigger: an optional user-defined piece of code that evaluates to a TRUE/FALSE value. The target builds if the value is TRUE. The change trigger: an optional user-defined piece of code that evaluates to any value (preferably small and quick to compute). The target builds if the value changed since the last make(). If any trigger detects something wrong or different with the target or its dependencies, the next make() will run the command and (re)build the target. 11.2 Customization With the trigger() function, you can create your own customized checklist of triggers. Let’s run a simple workflow with just the missing trigger. We deactivate the command, depend, and file triggers by setting the respective command, depend, and file arguments to FALSE. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2, psi_2 = (sqrt(5) - 1) / 2 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_1 #&gt; target psi_2 Now, even if you wreck all the commands, nothing rebuilds. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = (sqrt(5) - 1) / 2 - 9999999999999 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; All targets are already up to date. You can also give different targets to different triggers. Triggers in the drake plan override the trigger argument to make(). Below, psi_2 always builds, but psi_1 only builds if it has never been built before. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = target( command = (sqrt(5) - 1) / 2 - 9999999999999, trigger = trigger(condition = psi_1 &gt; 0) ) ) plan #&gt; # A tibble: 2 x 3 #&gt; target command trigger #&gt; &lt;chr&gt; &lt;expr&gt; &lt;expr&gt; #&gt; 1 psi_1 (sqrt(5) + 1)/2 + 9999999999999 NA #&gt; 2 psi_2 (sqrt(5) - 1)/2 - 9999999999999 trigger(condition = psi_1 &gt; 0) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_2 make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) #&gt; target psi_2 Interestingly, psi_2 now depends on psi_1. Since psi_1 is part of the because of the condition trigger, it needs to be up to date before we attempt psi_2. However, since psi_1 is not part of the command, changing it will not trip the other triggers such as depend. vis_drake_graph(drake_config(plan)) In the next toy example below, drake reads from a file to decide whether to build x. Try it out. plan &lt;- drake_plan( x = target( 1 + 1, trigger = trigger(condition = file_in(readRDS(&quot;file.rds&quot;))) ) ) saveRDS(TRUE, &quot;file.rds&quot;) make(plan) #&gt; target x make(plan) #&gt; target x make(plan) #&gt; target x saveRDS(FALSE, &quot;file.rds&quot;) make(plan) #&gt; All targets are already up to date. make(plan) #&gt; All targets are already up to date. make(plan) #&gt; All targets are already up to date. In a real project with remote data sources, you may want to use the condition trigger to limit your builds to times when enough bandwidth is available for a large download. For example, drake_plan( x = target( command = download_large_dataset(), trigger = trigger(condition = is_enough_bandwidth()) ) ) Since the change trigger can return any value, it is often easier to use than the condition trigger. clean(destroy = TRUE) plan &lt;- drake_plan( x = target( command = 1 + 1, trigger = trigger(change = sqrt(y)) ) ) y &lt;- 1 make(plan) #&gt; target x make(plan) #&gt; All targets are already up to date. y &lt;- 2 make(plan) #&gt; target x In practice, you may want to use the change trigger to check a large remote before downloading it. drake_plan( x = target( command = download_large_dataset(), trigger = trigger( condition = is_enough_bandwidth(), change = date_last_modified() ) ) ) A word of caution: every non-NULL change trigger is always evaluated, and its value is carried around in memory throughout make(). So if you are not careful, heavy use of the change trigger could slow down your workflow and consume extra resources. The change trigger should return small values (and should ideally be quick to evaluate). To reduce memory consumption, you may want to return a fingerprint of your trigger value rather than the value itself. See the digest package for more information on computing hashes/fingerprints. library(digest) drake_plan( x = target( command = download_large_dataset(), trigger = trigger( change = digest(download_medium_dataset()) ) ) ) 11.3 Alternative trigger modes Sometimes, you may want to suppress a target without having to worry about turning off every single trigger. That is why the trigger() function has a mode argument, which controls the role of the condition trigger in the decision to build or skip a target. The available trigger modes are &quot;whitelist&quot; (default), &quot;blacklist&quot;, and &quot;condition&quot;. trigger(mode = &quot;whitelist&quot;): we rebuild the target whenever condition evaluates to TRUE. Otherwise, we defer to the other triggers. This is the default behaviro described above in this chapter. trigger(mode = &quot;blacklist&quot;): we skip the target whenever condition evaluates to FALSE. Otherwise, we defer to the other triggers. trigger(mode = &quot;condition&quot;): here, the condition trigger is the only decider, and we ignore all the other triggers. We rebuild target whenever condition evaluates to TRUE and skip it whenever condition evaluates to FALSE. 11.4 Consider hasty mode In hasty mode, drake acts as a job scheduler without watching dependencies. In other words, make(parallelism = &quot;hasty&quot;) always runs all the targets, and computational overhead is dramatically reduced. Read more here. 11.5 A more practical example See the “packages” example for a more practical demonstration of triggers and their usefulness. "],
["time.html", "Chapter 12 Time: logging, prediction, and strategy 12.1 Predict total runtime 12.2 Strategize your high-performance computing", " Chapter 12 Time: logging, prediction, and strategy Thanks to Jasper Clarkberg, drake records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan) #&gt; target large #&gt; target small #&gt; target regression1_large #&gt; target regression2_large #&gt; target regression1_small #&gt; target regression2_small #&gt; target summ_regression1_large #&gt; target coef_regression1_large #&gt; target summ_regression2_large #&gt; target coef_regression2_large #&gt; target summ_regression1_small #&gt; target coef_regression1_small #&gt; target coef_regression2_small #&gt; target summ_regression2_small #&gt; target report build_times(digits = 8) # From the cache. #&gt; # A tibble: 15 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; #&gt; 1 coef_regression1_large 0.004s 0.004s 0s #&gt; 2 coef_regression1_small 0.003s 0.004s 0s #&gt; 3 coef_regression2_large 0.003s 0.004s 0s #&gt; 4 coef_regression2_small 0.004s 0.004s 0s #&gt; 5 large 0.004s 0s 0.004s #&gt; 6 regression1_large 0.005s 0.004s 0s #&gt; 7 regression1_small 0.005s 0.004s 0s #&gt; 8 regression2_large 0.005s 0.004s 0s #&gt; 9 regression2_small 0.004s 0.004s 0s #&gt; 10 report 0.057s 0.06s 0s #&gt; 11 small 0.007s 0.004s 0.004s #&gt; 12 summ_regression1_large 0.003s 0.004s 0s #&gt; 13 summ_regression1_small 0.004s 0.004s 0s #&gt; 14 summ_regression2_large 0.003s 0s 0s #&gt; 15 summ_regression2_small 0.003s 0.004s 0s ## `dplyr`-style `tidyselect` commands build_times(starts_with(&quot;coef&quot;), digits = 8) #&gt; # A tibble: 4 x 4 #&gt; target elapsed user system #&gt; &lt;chr&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; &lt;S4: Duration&gt; #&gt; 1 coef_regression1_large 0.004s 0.004s 0s #&gt; 2 coef_regression1_small 0.003s 0.004s 0s #&gt; 3 coef_regression2_large 0.003s 0.004s 0s #&gt; 4 coef_regression2_small 0.004s 0.004s 0s 12.1 Predict total runtime drake uses these times to predict the runtime of the next make(). At this moment, everything is up to date in the current example, so the next make() should ideally take no time at all (except for preprocessing overhead). config &lt;- drake_config(my_plan, verbose = FALSE) predict_runtime(config) #&gt; [1] &quot;0s&quot; Suppose we change a dependency to make some targets out of date. Now, the next make() should take longer since some targets are out of date. reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } predict_runtime(config) #&gt; [1] &quot;0.079s&quot; And what if you plan to delete the cache and build all the targets from scratch? predict_runtime(config, from_scratch = TRUE) #&gt; [1] &quot;0.114s&quot; 12.2 Strategize your high-performance computing Let’s say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run make(), your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let’s write down these known times in seconds. known_times &lt;- rep(7200, nrow(my_plan)) names(known_times) &lt;- my_plan$target known_times[&quot;report&quot;] &lt;- 5 known_times #&gt; report small large #&gt; 5 7200 7200 #&gt; regression1_small regression1_large regression2_small #&gt; 7200 7200 7200 #&gt; regression2_large summ_regression1_small summ_regression1_large #&gt; 7200 7200 7200 #&gt; summ_regression2_small summ_regression2_large coef_regression1_small #&gt; 7200 7200 7200 #&gt; coef_regression1_large coef_regression2_small coef_regression2_large #&gt; 7200 7200 7200 How many parallel jobs should you use in the next make()? The predict_runtime() function can help you decide. predict_runtime(jobs = n) simulates persistent parallel workers and reports the estimated total runtime of make(jobs = n). (See also predict_workers().) time &lt;- c() for (jobs in 1:12){ time[jobs] &lt;- predict_runtime( config, jobs = jobs, from_scratch = TRUE, known_times = known_times ) } library(ggplot2) ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) + geom_line(aes(x = jobs, y = time, group = group)) + scale_y_continuous(breaks = 0:10 * 4, limits = c(0, 29)) + theme_gray(16) + xlab(&quot;jobs argument of make()&quot;) + ylab(&quot;Predicted runtime of make() (hours)&quot;) We see serious potential speed gains up to 4 jobs, but beyond that point, we have to double the jobs to shave off another 2 hours. Your choice of jobs for make() ultimately depends on the runtime you can tolerate and the computing resources at your disposal. A final note on predicting runtime: the output of predict_runtime() and predict_workers() also depends the optional workers column of your drake_plan(). If you micromanage which workers are allowed to build which targets, you may minimize reads from disk, but you could also slow down your workflow if you are not careful. See the high-performance computing guide for more. "],
["store.html", "Chapter 13 Storage 13.1 Cache formats 13.2 Hash algorithms", " Chapter 13 Storage When you run make(), drake stores your targets in a cache. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan, verbose = FALSE) The default cache is a hidden .drake folder. find_cache() ### [1] &quot;/home/you/project/.drake&quot; find_project() ### [1] &quot;/home/you/project&quot; drake uses the storr package to create and modify caches. library(storr) cache &lt;- storr_rds(&quot;.drake&quot;) head(cache$list()) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; head(cache$get(&quot;small&quot;)) #&gt; x y #&gt; 1 5.424 10.4 #&gt; 2 3.440 17.8 #&gt; 3 3.440 19.2 #&gt; 4 3.170 15.8 #&gt; 5 3.730 17.3 #&gt; 6 3.845 19.2 drake has its own interface on top of storr to make it easier to work with the default .drake/ cache. The loadd(), readd(), and cached() functions explore saved targets. head(cached()) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;regression1_large&quot; head(readd(small)) #&gt; x y #&gt; 1 5.424 10.4 #&gt; 2 3.440 17.8 #&gt; 3 3.440 19.2 #&gt; 4 3.170 15.8 #&gt; 5 3.730 17.3 #&gt; 6 3.845 19.2 loadd(large) head(large) #&gt; x y #&gt; 1 3.170 15.8 #&gt; 2 5.345 14.7 #&gt; 3 4.070 16.4 #&gt; 4 1.615 30.4 #&gt; 5 3.170 15.8 #&gt; 6 1.513 30.4 rm(large) # Does not remove `large` from the cache. Functions get_cache(), storr::storr_rds(), and new_cache() recover and create caches. cache &lt;- get_cache(path = getwd()) # root or subdirectory of a drake project cache$driver$path #&gt; [1] &quot;/home/travis/build/ropenscilabs/drake-manual/.drake&quot; cache &lt;- storr::storr_rds(path = &quot;.drake&quot;) # actual cache folder cache2 &lt;- new_cache(&quot;my_new_cache&quot;) cache2$driver$path #&gt; [1] &quot;/home/travis/build/ropenscilabs/drake-manual/my_new_cache&quot; You can supply your own cache to make() and similar functions. cache2$list() #&gt; character(0) plan2 &lt;- drake_plan(x = 1, y = sqrt(x)) make(plan2, cache = cache2) #&gt; Unloading targets from environment: #&gt; y #&gt; target x #&gt; target y cache2$list() #&gt; [1] &quot;x&quot; &quot;y&quot; config &lt;- drake_config(plan = plan2, cache = cache2) vis_drake_graph(config) Destroy caches to remove them from your file system. cache$destroy() cache2$destroy() 13.1 Cache formats 13.1.1 RDS caches By default, drake uses storr_rds() caches because they allow make(jobs = 4) to safely store multiple targets in parallel. To achieve this thread safety, however, storr splits up the data into a pool of tiny cryptically-named files. make(my_plan, verbose = FALSE) head(list.files(&quot;.drake/data&quot;)) #&gt; [1] &quot;056249a12157c5d6.rds&quot; &quot;065f93613f1cb914.rds&quot; &quot;0e995089f06221c5.rds&quot; #&gt; [4] &quot;10022d9bd7c306d2.rds&quot; &quot;18d2a134f7a9c2b2.rds&quot; &quot;1cf6c530f6d8624f.rds&quot; head(list.files(&quot;.drake/keys/objects&quot;)) #&gt; [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; #&gt; [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; #&gt; [5] &quot;large&quot; &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; This makes RDS caches difficult to share with collaborators and put under version control. For the sake of portability, you may wish to work with database cashes as the next section describes. Alternatively, you can track changes in a cache log file with fingerprints of all your targets. drake_cache_log_file(&quot;log.txt&quot;) #&gt; Warning: `drake_cache_log_file()` is deprecated. To ensure cache log #&gt; is always up to date, create the cache log using `make()` with the #&gt; `cache_log_file` argument. cat(head(readLines(&quot;log.txt&quot;)), sep = &quot;\\n&quot;) #&gt; hash type name #&gt; 4e6e4d4c0bcda263 target coef_regression1_large #&gt; 5ad25785a84e0cac target coef_regression1_small #&gt; b2662785d55b28c1 target coef_regression2_large #&gt; 23c66b36be56905b target coef_regression2_small #&gt; 7b6504d4c0dcceab target large Use the cache_log_file argument of make() to refresh the cache log file every time you run make(). Then, if you put this file under version control (e.g. with git/GitHub) then the commit history will tell you how your data objects change over time. 13.1.2 Database caches It is possible use a single SQLite database file as the cache. mydb &lt;- DBI::dbConnect(RSQLite::SQLite(), &quot;database-file.sqlite&quot;) cache &lt;- storr::storr_dbi(&quot;datatable&quot;, &quot;keystable&quot;, mydb) make(my_plan, cache = cache, verbose = FALSE) loadd(small, cache = cache) head(small) #&gt; x y #&gt; 1 5.424 10.4 #&gt; 2 3.440 17.8 #&gt; 3 3.440 19.2 #&gt; 4 3.170 15.8 #&gt; 5 3.730 17.3 #&gt; 6 3.845 19.2 But be careful: for safe parallel computing (jobs &gt; 1) there are additional requirements for make(): Select either parallelism = &quot;clustermq&quot; or parallelism = &quot;future&quot;. Select caching = &quot;master&quot; to ensure that only the master process touches the cache. For a more complete demonstration, please see this example code, which you can download with drake_example(&quot;dbi&quot;). 13.1.3 Environment caches Environment caches live in computer memory, not your file system, so they are a nice way to run small and fast experiments. However, unless you save the cache manually, all your data will be lost when you quit your R session. And for large projects, you may not be able to fit all your data in memory anyway. cache &lt;- storr_environment() make(my_plan, cache = cache) 13.2 Hash algorithms storr caches use hash functions to keep track of stored objects. A hash function is just a way to fingerprint data. The idea is to represent an arbitrary chunks of data using (nearly) unique strings of fixed size. library(digest) # package for hashing objects and files smaller_data &lt;- 12 larger_data &lt;- rnorm(1000) digest(smaller_data) # compute the hash #&gt; [1] &quot;23c80a31c0713176016e6e18d76a5f31&quot; digest(larger_data) #&gt; [1] &quot;b03c1a142bc71a1b8afed7d23ce60f9f&quot; The digest package (used by both drake and storr) supports a wide variety of hash functions. Some generate larger hash keys, some are slower to compute, and others are more prone to “collisions” (where two different data objects are given the same hash key). The digest package supports a variety of hash algorithms. digest(larger_data, algo = &quot;sha512&quot;) #&gt; [1] &quot;cca91831466b966d82ae4440dea3be3a1b5555f6189563eb3d8b1a83c8c70d68daae9a35f62a00a96fefa241a26ce5c183a7f8a8d584bbbe023acb4b254080e9&quot; digest(larger_data, algo = &quot;md5&quot;) #&gt; [1] &quot;b03c1a142bc71a1b8afed7d23ce60f9f&quot; digest(larger_data, algo = &quot;xxhash64&quot;) #&gt; [1] &quot;ec927d1d9050ae96&quot; digest(larger_data, algo = &quot;murmur32&quot;) #&gt; [1] &quot;6faecd4e&quot; For drake, the default hash algorithm is xxhash64. You can choose different hash functions with the hash_algorithm argument to new_cache(), storr_rds(), and similar functions. (For drake version 6.2.1 and earlier, see the long_hash_algo and short_hash_algo arguments.) cache3 &lt;- new_cache(&quot;cache_3_path&quot;, hash_algorithm = &quot;murmur32&quot;) cache4 &lt;- storr_rds(&quot;cache_4_path&quot;, hash_algorithm = &quot;crc32&quot;) "],
["faq.html", "A Frequently-asked questions", " A Frequently-asked questions This FAQ is a compendium of pedagogically useful issues tagged on GitHub. To contribute, please submit a new issue and ask that it be labeled a frequently asked question. Dynamically scale clustermq workers knitr file paths List columns don’t work in map(.data) map() back to original variables after after combine() FAQ: Functions as data Avoid re-running targets if supplied args are the same as default args How to create a jagged cross() transform How to combine() while keeping track of the sources of targets target invalidated when referenced from another plan Within-target parallelism fails cannot remove bindings from a locked environment Functions that depend on targets Erroneous circular workflow error when using NSE in function function dependencies are missing: drake_config() in a magrittr pipe Best practices for including a drake workflow in a package Can you have multiple drake plans? evaluate file.path and variables in file_out and friends Working with HPC time limits Reproducibility with random numbers How should I mix non-R code (e.g. Python and shell scripts) in a large drake workflow? Reproducible remote data sources Trouble with caches sent through Dropbox How to add .R files to drake_plan() "],
["caution.html", "B Cautionary notes B.1 Workflow plans B.2 Execution B.3 Dependencies B.4 High-performance computing B.5 Storage", " B Cautionary notes This chapter addresses drake’s known edge cases, pitfalls, and weaknesses that might not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the issue tracker, where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version of drake. For a guide to debugging and testing drake projects, please refer to the separate guide to debugging and testing drake projects. B.1 Workflow plans B.1.1 Externalizing commands in R script files It is common practice to divide the work of a project into multiple R files, but if you do this, you will not get the most out of drake. Please see the chapter on organizing your files for more details. B.1.2 Commands are NOT perfectly flexible. In your drake plan (produced by drake_plan() and accepted by make()), your commands can usually be flexible R expressions. drake_plan( target1 = 1 + 1 - sqrt(sqrt(3)), target2 = my_function(web_scraped_data) %&gt;% my_tidy ) #&gt; # A tibble: 2 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 target1 1 + 1 - sqrt(sqrt(3)) #&gt; 2 target2 my_function(web_scraped_data) %&gt;% my_tidy However, please try to avoid formulas and function definitions in your commands. You may be able to get away with drake_plan(f = function(x){x + 1}) or drake_plan(f = y ~ x) in some use cases, but be careful. It is generally to define functions and formulas in your workspace and then let make() import them. (Alternatively, use the envir argument to make() to tightly control which imported functions are available.) Use the check_plan() function to help screen and quality-control your drake plan, use tracked() to see the items that are reproducibly tracked, and use vis_drake_graph() and build_drake_graph() to see the dependency structure of your project. B.2 Execution B.2.1 Install drake properly. You must properly install drake using install.packages(), devtools::install_github(), or a similar approach. Functions like devtools::load_all() are insufficient, particularly for parallel computing functionality in which separate new R sessions try to require(drake). B.2.2 Install all your packages. Your workflow may depend on external packages such as ggplot2, dplyr, and MASS. Such packages must be formally installed with install.packages(), devtools::install_github(), devtools::install_local(), or a similar command. If you load uninstalled packages with devtools::load_all(), results may be unpredictable and incorrect. B.2.3 A note on tidy evaluation Running commands in your R console is not always exactly like running them with make(). That’s because make() uses tidy evaluation as implemented in the rlang package. ## This `drake` plan uses rlang&#39;s quasiquotation operator `!!`. my_plan &lt;- drake_plan(list = c( little_b = &quot;\\&quot;b\\&quot;&quot;, letter = &quot;!!little_b&quot; )) #&gt; Warning in drake_plan(list = c(little_b = &quot;\\&quot;b\\&quot;&quot;, letter = &quot;!!little_b&quot;)): #&gt; The `list` argument of `drake_plan()` is deprecated. Use the interface #&gt; described at https://ropenscilabs.github.io/drake-manual/plans.html#large- #&gt; plans. #&gt; Error in enexpr(expr): object &#39;little_b&#39; not found my_plan #&gt; # A tibble: 15 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 report knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;),… #&gt; 2 small simulate(48) … #&gt; 3 large simulate(64) … #&gt; 4 regression1_small reg1(small) … #&gt; 5 regression1_large reg1(large) … #&gt; 6 regression2_small reg2(small) … #&gt; 7 regression2_large reg2(large) … #&gt; 8 summ_regression1_s… suppressWarnings(summary(regression1_small$residual… #&gt; 9 summ_regression1_l… suppressWarnings(summary(regression1_large$residual… #&gt; 10 summ_regression2_s… suppressWarnings(summary(regression2_small$residual… #&gt; 11 summ_regression2_l… suppressWarnings(summary(regression2_large$residual… #&gt; 12 coef_regression1_s… suppressWarnings(summary(regression1_small))$coeffi… #&gt; 13 coef_regression1_l… suppressWarnings(summary(regression1_large))$coeffi… #&gt; 14 coef_regression2_s… suppressWarnings(summary(regression2_small))$coeffi… #&gt; 15 coef_regression2_l… suppressWarnings(summary(regression2_large))$coeffi… make(my_plan) #&gt; Unloading targets from environment: #&gt; small #&gt; Warning: knitr/rmarkdown report &#39;report.Rmd&#39; does not exist and cannot be #&gt; inspected for dependencies. #&gt; Warning: missing input files: #&gt; report.Rmd #&gt; target report #&gt; Warning: Missing files for target report: #&gt; report.md #&gt; Warning: target report warnings: #&gt; cannot open file &#39;report.Rmd&#39;: No such file or directory #&gt; fail report #&gt; Error: Target `report` failed. Call `diagnose(report)` for details. Error message: #&gt; cannot open the connection readd(letter) #&gt; Error: key &#39;letter&#39; (&#39;objects&#39;) not found For the commands you specify the free-form ... argument, drake_plan() also supports tidy evaluation. For example, it supports quasiquotation with the !! argument. Use tidy_evaluation = FALSE or the list argument to suppress this behavior. my_variable &lt;- 5 drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;) ) #&gt; Warning in drake_plan(a = !!my_variable, b = !!my_variable + 1, list = c(d #&gt; = &quot;!!my_variable&quot;)): The `list` argument of `drake_plan()` is deprecated. #&gt; Use the interface described at https://ropenscilabs.github.io/drake-manual/ #&gt; plans.html#large-plans. #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 a 5 #&gt; 2 b 5 + 1 #&gt; 3 d 5 drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;), tidy_evaluation = FALSE ) #&gt; Warning in drake_plan(a = !!my_variable, b = !!my_variable + 1, list = c(d #&gt; = &quot;!!my_variable&quot;), : The `list` argument of `drake_plan()` is deprecated. #&gt; Use the interface described at https://ropenscilabs.github.io/drake-manual/ #&gt; plans.html#large-plans. #&gt; # A tibble: 3 x 2 #&gt; target command #&gt; &lt;chr&gt; &lt;expr&gt; #&gt; 1 a !!my_variable #&gt; 2 b !!my_variable + 1 #&gt; 3 d expression(!!my_variable) For instances of !! that remain in the drake plan, make() will run these commands in tidy fashion, evaluating the !! operator using the environment you provided. B.2.4 Find and diagnose your errors. When make() fails, use failed() and diagnose() to debug. Try the following out yourself. ## Targets with available diagnostic metadata, incluing errors, warnings, etc. diagnose() #&gt; [1] &quot;n-MRQXIYLTMV2HGOR2NV2GGYLSOM&quot; &quot;p-OJSXA33SOQXFE3LE&quot; #&gt; [3] &quot;random_rows&quot; &quot;reg1&quot; #&gt; [5] &quot;reg2&quot; &quot;report&quot; #&gt; [7] &quot;simulate&quot; f &lt;- function(){ stop(&quot;unusual error&quot;) } bad_plan &lt;- drake_plan(target = f()) withr::with_message_sink( stdout(), make(bad_plan) ) #&gt; target target #&gt; fail target #&gt; Error: Target `target` failed. Call `diagnose(target)` for details. Error message: #&gt; unusual error failed() # From the last make() only #&gt; [1] &quot;target&quot; error &lt;- diagnose(target)$error # See also warnings and messages. error$message #&gt; [1] &quot;unusual error&quot; error$call #&gt; f() error$calls # View the traceback. #&gt; [[1]] #&gt; local({ #&gt; f() #&gt; }) #&gt; #&gt; [[2]] #&gt; eval.parent(substitute(eval(quote(expr), envir))) #&gt; #&gt; [[3]] #&gt; eval(expr, p) #&gt; #&gt; [[4]] #&gt; eval(expr, p) #&gt; #&gt; [[5]] #&gt; eval(quote({ #&gt; f() #&gt; }), new.env()) #&gt; #&gt; [[6]] #&gt; eval(quote({ #&gt; f() #&gt; }), new.env()) #&gt; #&gt; [[7]] #&gt; f() #&gt; #&gt; [[8]] #&gt; stop(&quot;unusual error&quot;) B.2.5 Refresh the drake_config() list early and often. The master configuration list returned by drake_config() is important to drake’s internals, and you will need it for functions like outdated() and vis_drake_graph(). The config list corresponds to a single call to make(), and you should not modify it by hand afterwards. For example, modifying the targets element post-hoc will have no effect because the graph element will remain the same. It is best to just call drake_config() again. B.2.6 Workflows as R packages. The R package structure is a great way to organize the files of your project. Writing your own package to contain your data science workflow is a good idea, but you will need to Use expose_imports() to properly account for all your nested function dependencies, and If you load the package with devtools::load_all(), set the prework argument of make(): e.g. make(prework = &quot;devtools::load_all()&quot;). See the file organization chapter and ?expose_imports for detailed explanations. Thanks to Jasper Clarkberg for the workaround. B.2.7 The lazy_load flag does not work with &quot;parLapply&quot; parallelism. Ordinarily, drake prunes the execution environment at every parallelizable stage. In other words, it loads all the dependencies and unloads anything superfluous for entire batches of targets. This approach may require too much memory for some use cases, so there is an option to delay the loading of dependencies using the lazy_load argument to make() (powered by delayedAssign()). There are two major risks. make(..., lazy_load = TRUE, parallelism = &quot;parLapply&quot;, jobs = 2) does not work. If you want to use local multisession parallelism with multiple jobs and lazy loading, try &quot;future_lapply&quot; parallelism instead. library(future) future::plan(multisession) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan, lazy_load = TRUE, parallelism = &quot;future_lapply&quot;) Delayed evaluation may cause the same dependencies to be loaded multiple times, and these duplicated loads could be slow. B.2.8 Timeouts may be unreliable. You can call make(..., timeout = 10) to time out all each target after 10 seconds. However, timeouts rely on R.utils::withTimeout(), which in turn relies on setTimeLimit(). These functions are the best that R can offer right now, but they have known issues, and timeouts may fail to take effect for certain environments. B.3 Dependencies B.3.1 Objects that contain functions may rebuild too often For example, an R6 class changes whenever a new R6 object of that class is created. library(digest) library(R6) circle_class &lt;- R6Class( &quot;circle_class&quot;, private = list(radius = NULL), public = list( initialize = function(radius){ private$radius &lt;- radius }, area = function(){ pi * private$radius ^ 2 } ) ) digest(circle_class) #&gt; [1] &quot;d069ca9e44cf8ec18db20523dd9eca56&quot; circle &lt;- circle_class$new(radius = 5) digest(circle_class) # example_class changed #&gt; [1] &quot;96106cb01733c534f0482d163d283729&quot; rm(circle) Ordinarily, drake overreacts to this change and builds targets repeatedly. clean() plan &lt;- drake_plan( circle = circle_class$new(radius = 10), area = circle$area() ) make(plan) # `circle_class` changes because it is referenced. #&gt; target circle #&gt; target area make(plan) # Builds `circle` again because `circle_class` changed. #&gt; target circle The solution is to define your R6 class inside a function. drake does the right thing when it comes to tracking changes to functions. clean() new_circle &lt;- function(radius){ circle_class &lt;- R6Class( &quot;circle_class&quot;, private = list(radius = NULL), public = list( initialize = function(radius){ private$radius &lt;- radius }, area = function(){ pi * private$radius ^ 2 } ) ) circle_class$new(radius = radius) } plan &lt;- drake_plan( circle = new_circle(radius = 10), area = circle$area() ) make(plan) #&gt; target circle #&gt; target area make(plan) #&gt; All targets are already up to date. B.3.2 Dependencies are not tracked in some edge cases. You should explicitly learn the items in your workflow and the dependencies of your targets. ?deps ?tracked ?vis_drake_graph drake can be fooled into skipping objects that should be treated as dependencies. For example: f &lt;- function(){ b &lt;- get(&quot;x&quot;, envir = globalenv()) # x is incorrectly ignored digest::digest(file_dependency) } deps_code(f) #&gt; # A tibble: 4 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 globalenv globals #&gt; 2 file_dependency globals #&gt; 3 get globals #&gt; 4 digest::digest namespaced command &lt;- &quot;x &lt;- digest::digest(file_in(\\&quot;input_file.rds\\&quot;)); assign(\\&quot;x\\&quot;, 1); x&quot; # nolint deps_code(command) #&gt; # A tibble: 3 x 2 #&gt; name type #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 assign globals #&gt; 2 digest::digest namespaced #&gt; 3 input_file.rds file_in drake takes special precautions so that a target/import does not depend on itself. For example, deps_code(f) might return &quot;f&quot; if f() is a recursive function, but make() just ignores this conflict and runs as expected. In other words, make() automatically removes all self-referential loops in the dependency network. B.3.3 Dependencies of knitr reports If you have knitr reports, you can use knitr_report() in your commands so that your reports are refreshed every time one of their dependencies changes. See drake_example(&quot;mtcars&quot;) and the ?knitr_in() help file examples for demonstrations. Dependencies are detected if you call loadd() or readd() in your code chunks. But beware: an empty call to loadd() does not account for any dependencies even though it loads all the available targets into your R session. B.3.4 S3 and generic methods If you reference S3 methods, only the generic method is tracked as a dependency. plan &lt;- drake_plan(x = base::print(data.frame(y = 1))) cache &lt;- storr::storr_environment() make(plan, cache = cache) #&gt; target x #&gt; y #&gt; 1 1 readd(&quot;base::print&quot;, character_only = TRUE, cache = cache) #&gt; [1] &quot;function (x, ...) \\nUseMethod(\\&quot;print\\&quot;)&quot; #&gt; [2] &quot;&quot; But print() itself is not actually very helpful. Because of S3, print.data.frame() is actually doing the work. If you were to write your own S3 system and change a specific method like print.data.frame(), changes would not be reproducibly tracked because drake only finds the generic function. This is unavoidable because drake uses static code analysis to detect dependencies. It finds generics like print(), but it has no way of knowing in advance what method will actually be called. B.3.5 File outputs in imported functions. Do not call file_out() inside imported functions that you write. Only targets in your drake plan should have file outputs. ## toally_fine() will depend on the imported data.csv file. ## But make sure data.csv is an imported file and not a file target. totally_okay &lt;- function(x, y, z){ read.csv(file_in(&quot;data.csv&quot;)) } ## file_out() is for file targets, ## so `drake` will ignore it in imported functions. avoid_this &lt;- function(x, y, z){ read.csv(file_out(&quot;data.csv&quot;)) } B.3.6 Functions produced by Vectorize() With functions produced by Vectorize(), detecting dependencies is especially hard because the body of every such function is args &lt;- lapply(as.list(match.call())[-1L], eval, parent.frame()) names &lt;- if (is.null(names(args))) character(length(args)) else names(args) dovec &lt;- names %in% vectorize.args do.call(&quot;mapply&quot;, c(FUN = FUN, args[dovec], MoreArgs = list(args[!dovec]), SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES)) Thus, if f is constructed with Vectorize(g, ...), drake searches g() for dependencies, not f(). In fact, if drake sees that environment(f)[[&quot;FUN&quot;]] exists and is a function, then environment(f)[[&quot;FUN&quot;]] will be analyzed instead of f(). Furthermore, if f() is the output of Vectorize(), then drake reproducibly tracks environment(f)[[&quot;FUN&quot;]] rather than f() itself. Thus, if the configuration settings of vectorization change (such as which arguments are vectorized), but the core element-wise functionality remains the same, then make() will not react. Also, if you hover over the f node in vis_drake_graph(hover = TRUE), then you will see the body of environment(f)[[&quot;FUN&quot;]], not the body of f(). B.3.7 Compiled code is not reproducibly tracked. Some R functions use .Call() to run compiled code in the backend. The R code in these functions is tracked, but not the compiled object called with .Call(), nor its C/C++/Fortran source. B.3.8 Directories (folders) are not reproducibly tracked. In your drake plan, you can use file_in(), file_out(), and knitr_in() to assert that some targets/imports are external files. However, entire directories (i.e. folders) cannot be reproducibly tracked this way. Please see issue 12 for a discussion. B.3.9 Packages are not tracked as dependencies. drake may import functions from packages, but the packages themselves are not tracked as dependencies. For this, you will need other tools that support reproducibility beyond the scope of drake. Packrat creates a tightly-controlled local library of packages to extend the shelf life of your project. And with Docker, you can execute your project on a virtual machine to ensure platform independence. Together, packrat and Docker can help others reproduce your work even if they have different software and hardware. B.4 High-performance computing B.4.1 Calling mclapply() within targets The following workflow fails because make() locks your environment and mclapply() tries to add new variables to it. plan &lt;- drake_plan(parallel::mclapply(1:8, sqrt, mc.cores = 2)) make(plan) But there are plenty of workarounds, including make(plan, lock_envir = FALSE) and other parallel computing functions like parLapply() or furrr::future_map(). See this comment and the ensuing discussion. B.4.2 Zombie processes Some parallel backends, particularly make(parallelism = &quot;future&quot;) with future::multicore, may create zombie processes. Zombie children are not usually harmful, but you may wish to kill them yourself. The following function by Carl Boneri should work on Unix-like systems. For a discussion, see drake issue 116. fork_kill_zombies &lt;- function(){ require(inline) includes &lt;- &quot;#include &lt;sys/wait.h&gt;&quot; code &lt;- &quot;int wstat; while (waitpid(-1, &amp;wstat, WNOHANG) &gt; 0) {};&quot; wait &lt;- inline::cfunction( body = code, includes = includes, convention = &quot;.C&quot; ) invisible(wait()) } B.5 Storage B.5.1 Projects hosted on Dropbox and similar platforms If download a drake project from Dropbox, you may get an error like the one in issue 198: cache pathto/.drake connect 61 imports: ... connect 200 targets: ... Error in rawToChar(as.raw(x)) : embedded nul in string: 'initial_drake_version\\0\\0\\x9a\\x9d\\xdc\\0J\\xe9\\0\\0\\0(\\x9d\\xf9brם\\0\\xca)\\0\\0\\xb4\\xd7\\0\\0\\0\\0\\xb9' In addition: Warning message: In rawToChar(as.raw(x)) : out-of-range values treated as 0 in coercion to raw This is probably because Dropbox generates a bunch of “conflicted copy” files when file transfers do not go smoothly. This confuses storr, drake’s caching backend. keys/config/aG9vaw (Sandy Sum's conflicted copy 2018-01-31) keys/config/am9icw (Sandy Sum's conflicted copy 2018-01-31) keys/config/c2VlZA (Sandy Sum's conflicted copy 2018-01-31) Just remove these files using drake_gc() and proceed with your work. cache &lt;- get_cache() drake_gc(cache) B.5.2 Cache customization is limited The storage guide describes how storage works in drake. As explained near the end of that chapter, you can plug custom storr caches into make(). However, non-RDS caches such as storr_dbi() may not work with most forms of parallel computing. The storr::storr_dbi() cache and many others are not thread-safe. Either Set parallelism = &quot;clustermq_staged&quot; in make(), or Set parallelism = &quot;future&quot; with caching = &quot;master&quot; in make(), or Use no parallel computing at all. B.5.3 Runtime predictions In predict_runtime() and rate_limiting_times(), drake only accounts for the targets with logged build times. If some targets have not been timed, drake throws a warning and prints the untimed targets. "]
]
