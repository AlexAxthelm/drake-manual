[
["index.html", "The drake R Package User Manual Chapter 1 Introduction 1.1 The drake R package 1.2 Installation 1.3 Why drake? 1.4 Documentation 1.5 Help and troubleshooting 1.6 Similar work 1.7 Acknowledgements", " The drake R Package User Manual Will Landau, Kirill Müller, Alex Axthelm, Jasper Clarkberg, Lorenz Walthert Copyright Eli Lilly and Company Chapter 1 Introduction 1.1 The drake R package drake — or, Data Frames in R for Make — is a general-purpose workflow manager for data-driven tasks. It rebuilds intermediate data objects when their dependencies change, and it skips work when the results are already up to date. Not every runthrough starts from scratch, and completed workflows have tangible evidence of reproducibility. drake also helps scale up workflows and run steps in parallel. 1.2 Installation You can choose among different versions of drake. The latest CRAN release may be more convenient to install, but this manual is kept up to date with the GitHub version, so some features described here may not yet be available on CRAN. # Install the latest stable release from CRAN. install.packages(&quot;drake&quot;) # Alternatively, install the development version from GitHub. install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;ropensci/drake&quot;) 1.3 Why drake? 1.3.1 What gets done stays done. Too many data science projects follow a Sisyphean loop: Launch the code. Wait while it runs. Discover an issue. Restart from scratch. Ordinarily, it is hard to avoid restarting from scratch. But with drake, you can automatically Launch the parts that changed since last time. Skip the rest. 1.3.2 Reproducibility with confidence The R community emphasizes reproducibility. Traditional themes include scientific replicability, literate programming with knitr, and version control with git. But internal consistency is important too. Reproducibility carries the promise that your output matches the code and data you say you used. 1.3.2.1 Evidence Suppose you are reviewing someone else’s data analysis project for reproducibility. You scrutinize it carefully, checking that the datasets are available and the documentation is thorough. But could you re-create the results without the help of the original author? With drake, it is quick and easy to find out. make(plan) config &lt;- drake_config(plan) outdated(config) With everything already up to date, you have tangible evidence of reproducibility. Even though you did not re-create the results, you know the results are re-creatable. They faithfully show what the code is producing. Given the right package environment and system configuration, you have everything you need to reproduce all the output by yourself. 1.3.2.2 Ease When it comes time to actually rerun the entire project, you have much more confidence. Starting over from scratch is trivially easy. clean() # Remove the original author&#39;s results. make(plan) # Independently re-create the results from the code and input data. 1.3.2.3 Independent replication With even more evidence and confidence, you can invest the time to independently replicate the original code base if necessary. Up until this point, you relied on basic drake functions such as make(), so you may not have needed to peek at any substantive author-defined code in advance. In that case, you can stay usefully ignorant as you reimplement the original author’s methodology. In other words, drake could potentially improve the integrity of independent replication. 1.3.2.4 Readability and transparency Ideally, independent observers should be able to read your code and understand it. drake helps in several ways. The workflow plan data frame explicitly outlines the steps of the analysis, and vis_drake_graph() visualizes how those steps depend on each other. drake takes care of the parallel scheduling and high-performance computing (HPC) for you. That means the HPC code is no longer tangled up with the code that actually expresses your ideas. You can generate large collections of targets without necessarily changing your code base of imported functions, another nice separation between the concepts and the execution of your workflow 1.3.3 Aggressively scale up. Not every project can complete in a single R session on your laptop. Some projects need more speed or computing power. Some require a few local processor cores, and some need large high-performance computing systems. But parallel computing is hard. Your tables and figures depend on your analysis results, and your analyses depend on your datasets, so some tasks must finish before others even begin. drake knows what to do. Parallelism is implicit and automatic. See the high-performance computing guide for all the details. # Use the spare cores on your local machine. make(plan, jobs = 4) # Or scale up to a supercomputer. drake_batchtools_tmpl_file(&quot;slurm&quot;) # https://slurm.schedmd.com/ library(future.batchtools) future::plan(batchtools_slurm, template = &quot;batchtools.slurm.tmpl&quot;, workers = 100) make(plan, parallelism = &quot;future_lapply&quot;) 1.4 Documentation The main resources to learn drake are The user manual, which contains a friendly introduction and several long-form tutorials. The documentation website, which serves as a quicker reference. Kirill Müller’s drake workshop from March 5, 2018. 1.4.1 Cheat sheet Thanks to Kirill for preparing a drake cheat sheet for the workshop. 1.4.2 Frequently asked questions The FAQ page is an index of links to appropriately-labeled issues on GitHub. To contribute, please submit a new issue and ask that it be labeled as a frequently asked question. 1.4.3 Function reference The reference section lists all the available functions. Here are the most important ones. drake_plan(): create a workflow data frame (like my_plan). make(): build your project. loadd(): load one or more built targets into your R session. readd(): read and return a built target. drake_config(): create a master configuration list for other user-side functions. vis_drake_graph(): show an interactive visual network representation of your workflow. outdated(): see which targets will be built in the next make(). deps(): check the dependencies of a command or function. failed(): list the targets that failed to build in the last make(). diagnose(): return the full context of a build, including errors, warnings, and messages. 1.4.4 Tutorials Thanks to Kirill for constructing two interactive learnr tutorials: one supporting drake itself, and a prerequisite walkthrough of the cooking package. 1.4.5 Examples There are multiple drake-powered example projects available here, ranging from beginner-friendly stubs to demonstrations of high-performance computing. You can generate the files for a project with drake_example() (e.g. drake_example(&quot;gsp&quot;)), and you can list the available projects with drake_examples(). You can contribute your own example project with a fork and pull request. 1.4.6 Presentations Kirill’s drake pitch drake + cooking with Kirill drake + cooking exercises Christine Stawitz’s R-Ladies Seattle talk on June 25, 2018 Sina Rüeger’s Geneva R Users Group presentation on October 4, 2018 (example code). The talk broadly focused on tidy workflows and diversity in the R community. 1.4.7 Real example projects Here are some real-world applications of drake in the wild. efcaguab/demografia-del-voto efcaguab/great-white-shark-nsw IndianaCHE/Detailed-SSP-Reports tiernanmartin/home-and-hope If you have a project of your own, we would love to add it. Click here to edit the README.Rmd file. 1.4.8 Context and history For context and history, check out this post on the rOpenSci blog and episode 22 of the R Podcast. 1.5 Help and troubleshooting The following resources document many known issues and challenges. Frequently-asked questions. Cautionary notes and edge cases Debugging and testing drake projects Other known issues (please search both open and closed ones). If you are still having trouble, please submit a new issue with a bug report or feature request, along with a minimal reproducible example where appropriate. The GitHub issue tracker is mainly intended for bug reports and feature requests. While questions about usage etc. are also highly encouraged, you may alternatively wish to post to Stack Overflow and use the drake-r-package tag. 1.6 Similar work 1.6.1 GNU Make The original idea of a time-saving reproducible build system extends back at least as far as GNU Make, which still aids the work of data scientists as well as the original user base of complied language programmers. In fact, the name “drake” stands for “Data Frames in R for Make”. Make is used widely in reproducible research. Below are some examples from Karl Broman’s website. Bostock, Mike (2013). “A map of flowlines from NHDPlus.” https://github.com/mbostock/us-rivers. Powered by the Makefile at https://github.com/mbostock/us-rivers/blob/master/Makefile. Broman, Karl W (2012). “Halotype Probabilities in Advanced Intercross Populations.” G3 2(2), 199-202.Powered by the Makefile at https://github.com/kbroman/ailProbPaper/blob/master/Makefile. Broman, Karl W (2012). “Genotype Probabilities at Intermediate Generations in the Construction of Recombinant Inbred Lines.” *Genetics 190(2), 403-412. Powered by the Makefile at https://github.com/kbroman/preCCProbPaper/blob/master/Makefile. Broman, Karl W and Kim, Sungjin and Sen, Saunak and Ane, Cecile and Payseur, Bret A (2012). “Mapping Quantitative Trait Loci onto a Phylogenetic Tree.” Genetics 192(2), 267-279. Powered by the Makefile at https://github.com/kbroman/phyloQTLpaper/blob/master/Makefile. There are several reasons for R users to prefer drake instead. drake already has a Make-powered parallel backend. Just run make(..., parallelism = &quot;Makefile&quot;, jobs = 2) to enjoy most of the original benefits of Make itself. Improved scalability. With Make, you must write a potentially large and cumbersome Makefile by hand. But with drake, you can use wildcard templating to automatically generate massive collections of targets with minimal code. Lower overhead for light-weight tasks. For each Make target that uses R, a brand new R session must spawn. For projects with thousands of small targets, that means more time may be spent loading R sessions than doing the actual work. With make(..., parallelism = &quot;mclapply, jobs = 4&quot;), drake launches 4 persistent workers up front and efficiently processes the targets in R. Convenient organization of output. With Make, the user must save each target as a file. drake saves all the results for you automatically in a storr cache so you do not have to micromanage the results. 1.6.2 Remake Drake overlaps with its direct predecessor, remake. In fact, drake owes its core ideas to remake and Rich Fitzjohn. Remake’s development repository lists several real-world applications. Drake surpasses remake in several important ways, including but not limited to the following. High-performance computing. Remake has no native parallel computing support. Drake, on the other hand, has a thorough selection of parallel computing technologies and scheduling algorithms. Thanks to future, future.batchtools, and batchtools, it is straightforward to configure a drake project for most popular job schedulers, such as SLURM, TORQUE, and the Grid Engine, as well as systems contained in Docker images. A friendly interface. In remake, the user must manually write a YAML configuration file to arrange the steps of a workflow, which leads to some of the same scalability problems as Make. Drake’s data-frame-based interface and wildcard templating functionality easily generate workflows at scale. Thorough documentation. Drake contains thorough user manual, a reference website, a comprehensive README, examples in the help files of user-side functions, and accessible example code that users can write with drake::example_drake(). Active maintenance. Drake is actively developed and maintained, and issues are usually addressed promptly. Presence on CRAN. At the time of writing, drake is available on CRAN, but remake is not. 1.6.3 Memoise Memoization is the strategic caching of the return values of functions. Every time a memoized function is called with a new set of arguments, the return value is saved for future use. Later, whenever the same function is called with the same arguments, the previous return value is salvaged, and the function call is skipped to save time. The memoise package is an excellent implementation of memoization in R. However, memoization does not go far enough. In reality, the return value of a function depends not only on the function body and the arguments, but also on any nested functions and global variables, the dependencies of those dependencies, and so on upstream. drake surpasses memoise because it uses the entire dependency network graph of a project to decide which pieces need to be rebuilt and which ones can be skipped. 1.6.4 Knitr and R Markdown Much of the R community uses knitr and R Markdown for reproducible research. The idea is to intersperse code chunks in an R Markdown or *.Rnw file and then generate a dynamic report that weaves together code, output, and prose. Knitr is not designed to be a serious pipeline toolkit, and it should not be the primary computational engine for medium to large data analysis projects. Knitr scales far worse than Make or remake. The whole point is to consolidate output and prose, so it deliberately lacks the essential modularity. There is no obvious high-performance computing support. While there is a way to skip chunks that are already up to date (with code chunk options cache and autodep), this functionality is not the focus of knitr. It is deactivated by default, and remake and drake are more dependable ways to skip work that is already up to date. drake was designed to manage the entire workflow with knitr reports as targets. The strategy is analogous for knitr reports within remake projects. 1.6.5 Factual’s Drake Factual’s Drake is similar in concept, but the development effort is completely unrelated to the drake R package. 1.6.6 Other pipeline toolkits There are countless other successful pipeline toolkits. The drake package distinguishes itself with its R-focused approach, Tidyverse-friendly interface, and a thorough selection of parallel computing technologies and scheduling algorithms. 1.7 Acknowledgements Special thanks to Jarad Niemi, my advisor from graduate school, for first introducing me to the idea of Makefiles for research. He originally set me down the path that led to drake. Many thanks to Julia Lowndes, Ben Marwick, and Peter Slaughter for reviewing drake for rOpenSci, and to Maëlle Salmon for such active involvement as the editor. Thanks also to the following people for contributing early in development. Alex Axthelm Chan-Yub Park Daniel Falster Eric Nantz Henrik Bengtsson Ian Watson Jasper Clarkberg Kendon Bell Kirill Müller Credit for images is attributed here. "],
["main.html", "Chapter 2 The main example 2.1 Set the stage. 2.2 Make your results. 2.3 Go back and fix things. 2.4 Try it yourself!", " Chapter 2 The main example A typical data analysis workflow is a sequence of data transformations. Raw data becomes tidy data, then turns into fitted models, summaries, and reports. Other analyses are usually variations of this pattern, and drake can easily accommodate them. 2.1 Set the stage. To set up a project, load your packages, library(drake) library(dplyr) library(ggplot2) load your custom functions, create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram() } check any supporting files (optional), ## Get the files with drake_example(&quot;main&quot;). file.exists(&quot;raw_data.xlsx&quot;) ## [1] TRUE file.exists(&quot;report.Rmd&quot;) ## [1] TRUE and plan what you are going to do. plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) %&gt;% select(-X__1), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan ## # A tibble: 5 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 raw_data &quot;readxl::read_excel(file_in(\\&quot;raw_data.xlsx\\&quot;))&quot; ## 2 data &quot;raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … ## 3 hist create_plot(data) ## 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) ## 5 report &quot;rmarkdown::render(knitr_in(\\&quot;report.Rmd\\&quot;), output_file = fil… Optionally, visualize your workflow to make sure you set it up correctly. The graph is interactive, so you can click, drag, hover, zoom, and explore. config &lt;- drake_config(plan) vis_drake_graph(config) 2.2 Make your results. So far, we have just been setting the stage. Use make() to do the real work. Targets are built in the correct order regardless of the row order of plan. make(plan) ## target raw_data ## target data ## target fit ## target hist ## target report Except for output files like report.html, your output is stored in a hidden .drake/ folder. Reading it back is easy. readd(data) # See also loadd(). ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows The graph shows everything up to date. vis_drake_graph(config) 2.3 Go back and fix things. You may look back on your work and see room for improvement, but it’s all good! The whole point of drake is to help you go back and change things quickly and painlessly. For example, we forgot to give our histogram a bin width. readd(hist) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. So let’s fix the plotting function. create_plot &lt;- function(data) { ggplot(data, aes(x = Petal.Width, fill = Species)) + geom_histogram(binwidth = 0.25) + theme_gray(20) } drake knows which results are affected. vis_drake_graph(config) The next make() just builds hist and report. No point in wasting time on the data or model. make(plan) ## target hist ## target report loadd(hist) hist 2.4 Try it yourself! Use drake_example(&quot;main&quot;) to get all the materials. "],
["packages.html", "Chapter 3 An analysis of R package download trends 3.1 Get the code. 3.2 Overview 3.3 Analysis 3.4 Other ways to trigger downloads", " Chapter 3 An analysis of R package download trends This chapter explores R package download trends using the cranlogs package, and it shows how drake’s custom triggers can help with workflows with remote data sources. 3.1 Get the code. Write the code files to your workspace. drake_example(&quot;packages&quot;) The new packages folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 3.2 Overview This small data analysis project explores some trends in R package downloads over time. The datasets are downloaded using the cranlogs package. library(cranlogs) cran_downloads(packages = &quot;dplyr&quot;, when = &quot;last-week&quot;) ## date count package ## 1 2018-10-07 10896 dplyr ## 2 2018-10-08 23088 dplyr ## 3 2018-10-09 24870 dplyr ## 4 2018-10-10 24568 dplyr ## 5 2018-10-11 25679 dplyr ## 6 2018-10-12 20402 dplyr ## 7 2018-10-13 12350 dplyr Above, each count is the number of times dplyr was downloaded from the RStudio CRAN mirror on the given day. To stay up to date with the latest download statistics, we need to refresh the data frequently. With drake, we can bring all our work up to date without restarting everything from scratch. 3.3 Analysis First, we load the required packages. drake detects the packages you install and load. library(cranlogs) library(drake) library(dplyr) library(ggplot2) library(knitr) library(rvest) We want to explore the daily downloads from these packages. package_list &lt;- c( &quot;knitr&quot;, &quot;Rcpp&quot;, &quot;ggplot2&quot; ) We will use the cranlogs package to get daily logs of package downloads from RStudio’s CRAN mirror. In our drake_plan(), we declare targets older and recent to contain snapshots of the logs. data_plan &lt;- drake_plan( older = cran_downloads( packages = package_list, from = &quot;2016-11-01&quot;, to = &quot;2016-12-01&quot; ), recent = target( command = cran_downloads( packages = package_list, when = &quot;last-month&quot; ), trigger = trigger(change = latest_log_date()) ) ) data_plan ## # A tibble: 2 x 3 ## target command trigger ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 older &quot;cran_downloads(packages = package_list, f… &lt;NA&gt; ## 2 recent &quot;cran_downloads(packages = package_list, w… trigger(change = lat… Notice the custom trigger for the target recent. Here, we are telling drake to rebuild recent whenever a new day’s log is uploaded to http://cran-logs.rstudio.com. In other words, drake keeps track of the return value of latest_log_date() and recomputes recent (during make()) if that value changed since the last make(). Here, latest_log_date() is one of our custom imported functions. We use it to scrape http://cran-logs.rstudio.com using the rvest package. latest_log_date &lt;- function(){ read_html(&quot;http://cran-logs.rstudio.com/&quot;) %&gt;% html_nodes(&quot;li:last-of-type&quot;) %&gt;% html_nodes(&quot;a:last-of-type&quot;) %&gt;% html_text() %&gt;% max } Continuing with the setup, we want to summarize each set of download statistics a couple different ways. output_types &lt;- drake_plan( averages = make_my_table(dataset__), plot = make_my_plot(dataset__) ) output_types ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 averages make_my_table(dataset__) ## 2 plot make_my_plot(dataset__) We need to define functions to summarize and plot the data. make_my_table &lt;- function(downloads){ group_by(downloads, package) %&gt;% summarize(mean_downloads = mean(count)) } make_my_plot &lt;- function(downloads){ ggplot(downloads) + geom_line(aes(x = date, y = count, group = package, color = package)) } Below, the targets recent and older each take turns substituting the dataset__ wildcard. Thus, output_plan has four rows. output_plan &lt;- plan_analyses( plan = output_types, datasets = data_plan ) output_plan ## # A tibble: 4 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 averages_older make_my_table(older) ## 2 averages_recent make_my_table(recent) ## 3 plot_older make_my_plot(older) ## 4 plot_recent make_my_plot(recent) We plan to weave the results together in a dynamic knitr report. report_plan &lt;- drake_plan( report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) ) report_plan ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;report.md\\&quot;), quiet =… Because of the mention of knitr_in() above, make() will look dependencies inside report.Rmd (targets mentioned with loadd() or readd() in active code chunks). That way, whenever a dependency changes, drake will rebuild report.md when you call make(). For that to happen, we need report.Rmd to exist before the call to make(). For this example, you can find report.Rmd here. Now, we complete the workflow plan data frame by concatenating the results together. drake uses implicit dependency relationships to resolve execution order, so row order in the plan does not matter. whole_plan &lt;- bind_plans( data_plan, output_plan, report_plan ) whole_plan ## # A tibble: 7 x 3 ## target command trigger ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 older &quot;cran_downloads(packages = package_lis… &lt;NA&gt; ## 2 recent &quot;cran_downloads(packages = package_lis… trigger(change = la… ## 3 averages_o… make_my_table(older) &lt;NA&gt; ## 4 averages_r… make_my_table(recent) &lt;NA&gt; ## 5 plot_older make_my_plot(older) &lt;NA&gt; ## 6 plot_recent make_my_plot(recent) &lt;NA&gt; ## 7 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_o… &lt;NA&gt; Now, we run the project to download the data and analyze it. The results will be summarized in the knitted report, report.md, but you can also read the results directly from the cache. make(whole_plan) ## target older ## target recent ## target averages_older ## target plot_older ## target averages_recent ## target plot_recent ## target report readd(averages_recent) ## # A tibble: 3 x 2 ## package mean_downloads ## &lt;chr&gt; &lt;dbl&gt; ## 1 ggplot2 24255. ## 2 knitr 13846. ## 3 Rcpp 29703. readd(averages_older) ## # A tibble: 3 x 2 ## package mean_downloads ## &lt;chr&gt; &lt;dbl&gt; ## 1 ggplot2 14641. ## 2 knitr 9069. ## 3 Rcpp 14408. readd(plot_recent) readd(plot_older) If we run make() again right away, we see that everything is up to date. But if we wait until a new day’s log is uploaded, make() will update recent and everything that depends on it. make(whole_plan) ## All targets are already up to date. To visualize the build behavior, you can plot the dependency network. config &lt;- drake_config(whole_plan) vis_drake_graph(config) 3.4 Other ways to trigger downloads Sometimes, our remote data sources get revised, and web scraping may not be the best way to detect changes. We may want to look at our remote dataset’s modification time or HTTP ETag. To see how this works, consider the CRAN log file from February 9, 2018. url &lt;- &quot;http://cran-logs.rstudio.com/2018/2018-02-09-r.csv.gz&quot; We can track the modification date using the httr package. library(httr) # For querying websites. HEAD(url)$headers[[&quot;last-modified&quot;]] ## [1] &quot;Mon, 12 Feb 2018 16:34:48 GMT&quot; In our workflow plan, we can track this timestamp and trigger a download whenever it changes. plan &lt;- drake_plan( logs = target( get_logs(url), trigger(change = HEAD(url)$headers[[&quot;last-modified&quot;]]) ) ) plan ## # A tibble: 1 x 3 ## target command trigger ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 logs get_logs(url) &quot;trigger(change = HEAD(url)$headers[[\\&quot;last-modifi… where library(R.utils) # For unzipping the files we download. library(curl) # For downloading data. get_logs &lt;- function(url){ curl_download(url, &quot;logs.csv.gz&quot;) # Get a big file. gunzip(&quot;logs.csv.gz&quot;, overwrite = TRUE) # Unzip it. out &lt;- read.csv(&quot;logs.csv&quot;, nrows = 4) # Extract the data you need. unlink(c(&quot;logs.csv.gz&quot;, &quot;logs.csv&quot;)) # Remove the big files out # Value of the target. } When we are ready, we run the workflow. make(plan) ## target logs readd(logs) ## date time size version os country ip_id ## 1 2018-02-09 13:01:13 82375220 3.4.3 win RO 1 ## 2 2018-02-09 13:02:06 74286541 3.3.3 win US 2 ## 3 2018-02-09 13:02:10 82375216 3.4.3 win US 3 ## 4 2018-02-09 13:03:30 82375220 3.4.3 win IS 4 If the log file at the url ever changes, the timestamp will update remotely, and make() will download the file again. "],
["gsp.html", "Chapter 4 Finding the best model of gross state product 4.1 Get the code. 4.2 Objective and methods 4.3 Data 4.4 Analysis 4.5 Results 4.6 Comparison with GNU Make 4.7 References", " Chapter 4 Finding the best model of gross state product The following data analysis workflow shows off drake’s ability to generate lots of reproducibly-tracked tasks with ease. The same technique would be cumbersome, even intractable, with GNU Make. 4.1 Get the code. Write the code files to your workspace. drake_example(&quot;gsp&quot;) The new gsp folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 4.2 Objective and methods The goal is to search for factors closely associated with the productivity of states in the USA around the 1970s and 1980s. For the sake of simplicity, we use gross state product as a metric of productivity, and we restrict ourselves to multiple linear regression models with three variables. For each of the 84 possible models, we fit the data and then evaluate the root mean squared prediction error (RMSPE). \\[ \\begin{aligned} \\text{RMSPE} = \\sqrt{(\\text{y} - \\widehat{y})^T(y - \\widehat{y})} \\end{aligned} \\] Here, \\(y\\) is the vector of observed gross state products in the data, and \\(\\widehat{y}\\) is the vector of predicted gross state products under one of the models. We take the best variables to be the triplet in the model with the lowest RMSPE. 4.3 Data The Produc dataset from the Ecdat package contains data on the Gross State Product from 1970 to 1986. Each row is a single observation on a single state for a single year. The dataset has the following variables as columns. See the references later in this report for more details. gsp: gross state product. state: the state. year: the year. pcap: private capital stock. hwy: highway and streets. water: water and sewer facilities. util: other public buildings and structures. pc: public capital. emp: labor input measured by the employment in non-agricultural payrolls. unemp: state unemployment rate. library(Ecdat) data(Produc) head(Produc) ## state year pcap hwy water util pc gsp emp ## 1 ALABAMA 1970 15032.67 7325.80 1655.68 6051.20 35793.80 28418 1010.5 ## 2 ALABAMA 1971 15501.94 7525.94 1721.02 6254.98 37299.91 29375 1021.9 ## 3 ALABAMA 1972 15972.41 7765.42 1764.75 6442.23 38670.30 31303 1072.3 ## 4 ALABAMA 1973 16406.26 7907.66 1742.41 6756.19 40084.01 33430 1135.5 ## 5 ALABAMA 1974 16762.67 8025.52 1734.85 7002.29 42057.31 33749 1169.8 ## 6 ALABAMA 1975 17316.26 8158.23 1752.27 7405.76 43971.71 33604 1155.4 ## unemp ## 1 4.7 ## 2 5.2 ## 3 4.7 ## 4 3.9 ## 5 5.5 ## 6 7.7 4.4 Analysis First, we load the required packages. drake is aware of all the packages you load with library() or require(). library(drake) library(Ecdat) # econometrics datasets library(ggplot2) library(knitr) library(purrr) library(tidyverse) Next, we write our custom supporting functions. Each model we fit will be a call to fit_gsp_model(). fit_gsp_model &lt;- function(V1, V2, V3, data){ lm(as.formula(paste(&quot;gsp ~&quot;, V1, &quot;+&quot;, V2, &quot;+&quot;, V3)), data = data) } fit_gsp_model(&quot;unemp&quot;, &quot;year&quot;, &quot;pcap&quot;, data = Produc) %&gt;% summary() ## ## Call: ## lm(formula = as.formula(paste(&quot;gsp ~&quot;, V1, &quot;+&quot;, V2, &quot;+&quot;, V3)), ## data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -79469 -5772 -298 4548 117119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.625e+06 2.548e+05 -6.376 3.04e-10 *** ## unemp -1.575e+03 2.879e+02 -5.470 5.99e-08 *** ## year 8.264e+02 1.292e+02 6.397 2.68e-10 *** ## pcap 2.456e+00 2.133e-02 115.123 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16630 on 812 degrees of freedom ## Multiple R-squared: 0.9437, Adjusted R-squared: 0.9435 ## F-statistic: 4541 on 3 and 812 DF, p-value: &lt; 2.2e-16 Ultimately, we will apply a 3-covariate model to each triplet of predictors in the data. predictors &lt;- setdiff(colnames(Produc), &quot;gsp&quot;) combos &lt;- tibble::as_tibble(t(combn(predictors, 3))) combos ## # A tibble: 84 x 3 ## V1 V2 V3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 state year pcap ## 2 state year hwy ## 3 state year water ## 4 state year util ## 5 state year pc ## 6 state year emp ## 7 state year unemp ## 8 state pcap hwy ## 9 state pcap water ## 10 state pcap util ## # ... with 74 more rows We want a combos data frame with all the arguments to fit_gsp_model() # The `data` argument to fit_gsp_model() is a symbol, # which could stand for a generic dataset or an upstream target. combos$data &lt;- rlang::syms(rep(&quot;Produc&quot;, nrow(combos))) combos ## # A tibble: 84 x 4 ## V1 V2 V3 data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 state year pcap &lt;symbol&gt; ## 2 state year hwy &lt;symbol&gt; ## 3 state year water &lt;symbol&gt; ## 4 state year util &lt;symbol&gt; ## 5 state year pc &lt;symbol&gt; ## 6 state year emp &lt;symbol&gt; ## 7 state year unemp &lt;symbol&gt; ## 8 state pcap hwy &lt;symbol&gt; ## 9 state pcap water &lt;symbol&gt; ## 10 state pcap util &lt;symbol&gt; ## # ... with 74 more rows Plus nice target names. combos$id &lt;- apply(combos, 1, paste, collapse = &quot;_&quot;) Now, we encode all the model-fitting code in a drake plan. model_plan &lt;- map_plan(combos, fit_gsp_model) model_plan ## # A tibble: 84 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 state_year_pcap_P… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = … ## 2 state_year_hwy_Pr… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = … ## 3 state_year_water_… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = … ## 4 state_year_util_P… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = … ## 5 state_year_pc_Pro… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = … ## 6 state_year_emp_Pr… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = … ## 7 state_year_unemp_… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = … ## 8 state_pcap_hwy_Pr… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;pcap\\&quot;, V3 = … ## 9 state_pcap_water_… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;pcap\\&quot;, V3 = … ## 10 state_pcap_util_P… &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;pcap\\&quot;, V3 = … ## # ... with 74 more rows model_plan$target[1] ## [1] &quot;state_year_pcap_Produc&quot; model_plan$command[1] ## [1] &quot;fit_gsp_model(V1 = \\&quot;state\\&quot;, V2 = \\&quot;year\\&quot;, V3 = \\&quot;pcap\\&quot;, data = Produc)&quot; We need to define a function to get the RMSPE for each model. get_rmspe &lt;- function(lm_fit, data){ y &lt;- data$gsp yhat &lt;- predict(lm_fit, data = data) terms &lt;- attr(summary(lm_fit)$terms, &quot;term.labels&quot;) data.frame( rmspe = sqrt(mean((y - yhat)^2)), # nolint X1 = terms[1], X2 = terms[2], X3 = terms[3] ) } Next, we make a plan to judge each model based on its root mean squared prediction error (RMSPE). rmspe_args &lt;- tibble::tibble( lm_fit = rlang::syms(model_plan$target), data = rlang::syms(combos$data), id = paste0(&quot;rmspe_&quot;, model_plan$target) ) rmspe_plan &lt;- map_plan(rmspe_args, get_rmspe) rmspe_plan ## # A tibble: 84 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 rmspe_state_year_pcap_P… get_rmspe(lm_fit = state_year_pcap_Produc, da… ## 2 rmspe_state_year_hwy_Pr… get_rmspe(lm_fit = state_year_hwy_Produc, dat… ## 3 rmspe_state_year_water_… get_rmspe(lm_fit = state_year_water_Produc, d… ## 4 rmspe_state_year_util_P… get_rmspe(lm_fit = state_year_util_Produc, da… ## 5 rmspe_state_year_pc_Pro… get_rmspe(lm_fit = state_year_pc_Produc, data… ## 6 rmspe_state_year_emp_Pr… get_rmspe(lm_fit = state_year_emp_Produc, dat… ## 7 rmspe_state_year_unemp_… get_rmspe(lm_fit = state_year_unemp_Produc, d… ## 8 rmspe_state_pcap_hwy_Pr… get_rmspe(lm_fit = state_pcap_hwy_Produc, dat… ## 9 rmspe_state_pcap_water_… get_rmspe(lm_fit = state_pcap_water_Produc, d… ## 10 rmspe_state_pcap_util_P… get_rmspe(lm_fit = state_pcap_util_Produc, da… ## # ... with 74 more rows rmspe_plan$target[1] ## [1] &quot;rmspe_state_year_pcap_Produc&quot; rmspe_plan$command[1] ## [1] &quot;get_rmspe(lm_fit = state_year_pcap_Produc, data = Produc)&quot; In our current plan, RMSPE is distributed over 84 targets (one for each model). Let’s plan to combine them all together in a single data frame. rmspe_results_plan &lt;- gather_plan( plan = rmspe_plan, target = &quot;rmspe&quot;, gather = &quot;rbind&quot; ) At the end, let’s generate a pdf plot of the RMSPE scores and a knitr report. output_plan &lt;- drake_plan( plot = ggsave( filename = file_out(&quot;rmspe.pdf&quot;), plot = plot_rmspe(rmspe), width = 7, height = 7 ), report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) ) head(output_plan) ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 plot &quot;ggsave(filename = file_out(\\&quot;rmspe.pdf\\&quot;), plot = plot_rmspe(rm… ## 2 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;report.md\\&quot;), quiet =… We see warnings above because our R Markdown report report.Rmd does not exist yet. You can find it here and download it with the code below. drake_example(&quot;gsp&quot;) file.copy(from = &quot;gsp/report.Rmd&quot;, to = &quot;.&quot;, overwrite = TRUE) ## [1] TRUE At this point, we can gather together the whole workflow plan. whole_plan &lt;- bind_plans( model_plan, rmspe_plan, rmspe_results_plan, output_plan ) Before we run the project, we need to define the plot_rmspe() function. plot_rmspe &lt;- function(rmspe){ ggplot(rmspe) + geom_histogram(aes(x = rmspe), bins = 15) } Now, we can run the project make(whole_plan, verbose = FALSE) 4.5 Results Here are the root mean squared prediction errors of all the models. results &lt;- readd(rmspe) loadd(plot_rmspe) library(ggplot2) plot_rmspe(rmspe = results) And here are the best models. The best variables are in the top row under X1, X2, and X3. head(results[order(results$rmspe, decreasing = FALSE), ]) ## rmspe X1 X2 X3 ## rmspe_state_hwy_emp_Produc 2613.669 state hwy emp ## rmspe_state_water_emp_Produc 2664.842 state water emp ## rmspe_state_util_emp_Produc 2665.744 state util emp ## rmspe_state_pc_emp_Produc 2666.058 state pc emp ## rmspe_state_pcap_emp_Produc 2675.336 state pcap emp ## rmspe_state_emp_unemp_Produc 2692.687 state emp unemp 4.6 Comparison with GNU Make If we were using Make instead of drake with the same set of targets, the analogous Makefile would look something like this pseudo-code sketch. models = model_state_year_pcap.rds model_state_year_hwy.rds ... # 84 of these model_% Rscript -e 'saveRDS(lm(...), ...)' rmspe_%: model_% Rscript -e 'saveRDS(get_rmspe(...), ...)' rmspe.rds: rmspe_% Rscript -e 'saveRDS(rbind(...), ...)' rmspe.pdf: rmspe.rds Rscript -e 'ggplot2::ggsave(plot_rmspe(readRDS(\"rmspe.rds\")), \"rmspe.pdf\")' report.md: report.Rmd Rscript -e 'knitr::knit(\"report.Rmd\")' There are three main disadvantages to this approach. Every target requires a new call to Rscript, which means that more time is spent initializing R sessions than doing the actual work. The user must micromanage nearly one hundred output files (in this case, *.rds files), which is cumbersome, messy, and inconvenient. drake, on the other hand, automatically manages storage using a storr cache. The user needs to write the names of the 84 models near the top of the Makefile, which is less convenient than maintaining a data frame in R. 4.7 References Baltagi, Badi H (2003). Econometric analysis of panel data, John Wiley and sons, http://www.wiley.com/legacy/wileychi/baltagi/. Baltagi, B. H. and N. Pinnoi (1995). “Public capital stock and state productivity growth: further evidence”, Empirical Economics, 20, 351-359. Munnell, A. (1990). “Why has productivity growth declined? Productivity and public investment”&quot;, New England Economic Review, 3-22. Yves Croissant (2016). Ecdat: Data Sets for Econometrics. R package version 0.3-1. https://CRAN.R-project.org/package=Ecdat. "],
["mtcars.html", "Chapter 5 The mtcars example and workflow plan generation 5.1 Get the code. 5.2 Quick examples 5.3 The motivation of the mtcars example 5.4 Set up the mtcars example 5.5 The workflow plan data frame 5.6 Generate the workflow plan 5.7 Flexible workflow plan generation 5.8 Run the workflow 5.9 Automatic watching for changed dependencies 5.10 A note on tidy evaluation 5.11 Need more speed?", " Chapter 5 The mtcars example and workflow plan generation This chapter is a walkthrough of drake’s main functionality based on the mtcars example. It sets up the project and runs it repeatedly to demonstrate drake’s most important functionality. 5.1 Get the code. Write the code files to your workspace. drake_example(&quot;mtcars&quot;) The new mtcars folder now includes a file structure of a serious drake project, plus an interactive-tutorial.R to narrate the example. The code is also online here. 5.2 Quick examples Inspect and run your project. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). config &lt;- drake_config(my_plan) # Master configuration list vis_drake_graph(config) # Hover, click, drag, zoom, pan. make(my_plan) # Run the workflow. outdated(config) # Everything is up to date. Debug errors. failed() # Targets that failed in the most recent `make()` context &lt;- diagnose(large) # Diagnostic metadata: errors, warnings, etc. error &lt;- context$error str(error) # Object of class &quot;error&quot; error$message error$call error$calls # Full traceback of nested calls leading up to the error. # nolint Dive deeper into the built-in examples. drake_example(&quot;mtcars&quot;) # Write the code files. drake_examples() # List the other examples. 5.3 The motivation of the mtcars example Is there an association between the weight and the fuel efficiency of cars? To find out, we use the mtcars dataset from the datasets package. The mtcars dataset originally came from the 1974 Motor Trend US magazine, and it contains design and performance data on 32 models of automobile. # ?mtcars # more info head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Here, wt is weight in tons, and mpg is fuel efficiency in miles per gallon. We want to figure out if there is an association between wt and mpg. The mtcars dataset itself only has 32 rows, so we generate two larger bootstrapped datasets and then analyze them with regression models. We summarize the regression models to see if there is an association. 5.4 Set up the mtcars example Before you run your project, you need to set up the workspace. In other words, you need to gather the “imports”: functions, pre-loaded data objects, and saved files that you want to be available before the real work begins. library(knitr) # Drake knows which packages you load. library(drake) We need a function to bootstrap larger datasets from mtcars. # Pick a random subset of n rows from a dataset random_rows &lt;- function(data, n){ data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } # Bootstrapped datasets from mtcars. simulate &lt;- function(n){ # Pick a random set of cars to bootstrap from the mtcars data. data &lt;- random_rows(data = mtcars, n = n) # x is the car&#39;s weight, and y is the fuel efficiency. data.frame( x = data$wt, y = data$mpg ) } We also need functions to apply the regression models we need for detecting associations. # Is fuel efficiency linearly related to weight? reg1 &lt;- function(d){ lm(y ~ + x, data = d) } # Is fuel efficiency related to the SQUARE of the weight? reg2 &lt;- function(d){ d$x2 &lt;- d$x ^ 2 lm(y ~ x2, data = d) } We want to summarize the final results in an R Markdown report, so we need the the report.Rmd source file. You can get it with drake_example(&quot;mtcars&quot;) or load_mtcars_example(). drake_example(&quot;mtcars&quot;, overwrite = TRUE) file.copy(&quot;mtcars/report.Rmd&quot;, &quot;.&quot;, overwrite = TRUE) ## [1] TRUE Here are the contents of the report. It will serve as a final summary of our work, and we will process it at the very end. Admittedly, some of the text spoils the punch line. cat(readLines(&quot;report.Rmd&quot;), sep = &quot;\\n&quot;) ## --- ## title: &quot;Final results report for the mtcars example&quot; ## author: You ## output: html_document ## --- ## ## # The weight and fuel efficiency of cars ## ## Is there an association between the weight and the fuel efficiency of cars? To find out, we use the `mtcars` dataset from the `datasets` package. The `mtcars` data originally came from the 1974 Motor Trend US magazine, and it contains design and performance data on 32 models of automobile. ## ## ```{r showmtcars} ## # ?mtcars # more info ## head(mtcars) ## ``` ## ## Here, `wt` is weight in tons, and `mpg` is fuel efficiency in miles per gallon. We want to figure out if there is an association between `wt` and `mpg`. The `mtcars` dataset itself only has 32 rows, so we generated two larger bootstrapped datasets. We called them `small` and `large`. ## ## ```{r example_chunk} ## library(drake) ## head(readd(small)) # 48 rows ## loadd(large) # 64 rows ## head(large) ## ``` ## ## Then, we fit a couple regression models to the `small` and `large` to try to detect an association between `wt` and `mpg`. Here are the coefficients and p-values from one of the model fits. ## ## ```{r second_example_chunk} ## readd(coef_regression2_small) ## ``` ## ## Since the p-value on `x2` is so small, there may be an association between weight and fuel efficiency after all. ## ## # A note on knitr reports in drake projects. ## ## Because of the calls to `readd()` and `loadd()`, `drake` knows that `small`, `large`, and `coef_regression2_small` are dependencies of this R Markdown report. This dependency relationship is what causes the report to be processed at the very end. Now, all our imports are set up. When the real work begins, drake will import functions and data objects from your R session environment ls() ## [1] &quot;combos&quot; &quot;config&quot; &quot;create_plot&quot; ## [4] &quot;data_plan&quot; &quot;fit_gsp_model&quot; &quot;get_logs&quot; ## [7] &quot;get_rmspe&quot; &quot;hist&quot; &quot;latest_log_date&quot; ## [10] &quot;make_my_plot&quot; &quot;make_my_table&quot; &quot;model_plan&quot; ## [13] &quot;output_plan&quot; &quot;output_types&quot; &quot;package_list&quot; ## [16] &quot;plan&quot; &quot;plot_rmspe&quot; &quot;predictors&quot; ## [19] &quot;Produc&quot; &quot;random_rows&quot; &quot;reg1&quot; ## [22] &quot;reg2&quot; &quot;report_plan&quot; &quot;results&quot; ## [25] &quot;rmspe_args&quot; &quot;rmspe_plan&quot; &quot;rmspe_results_plan&quot; ## [28] &quot;simulate&quot; &quot;url&quot; &quot;whole_plan&quot; and saved files from your file system. list.files() ## [1] &quot;_book&quot; &quot;_bookdown.yml&quot; &quot;appendix.Rmd&quot; ## [4] &quot;build.R&quot; &quot;caution.Rmd&quot; &quot;CONDUCT.md&quot; ## [7] &quot;debug.Rmd&quot; &quot;deploy.sh&quot; &quot;DESCRIPTION&quot; ## [10] &quot;drake-manual_files&quot; &quot;drake-manual.Rmd&quot; &quot;drake-manual.Rproj&quot; ## [13] &quot;examples.Rmd&quot; &quot;faq.R&quot; &quot;faq.Rmd&quot; ## [16] &quot;footer.html&quot; &quot;functionality.Rmd&quot; &quot;gsp.Rmd&quot; ## [19] &quot;hpc.Rmd&quot; &quot;images&quot; &quot;index.Rmd&quot; ## [22] &quot;LICENSE&quot; &quot;main.Rmd&quot; &quot;mtcars&quot; ## [25] &quot;mtcars.Rmd&quot; &quot;NEWS.md&quot; &quot;organize.Rmd&quot; ## [28] &quot;packages.Rmd&quot; &quot;plans.Rmd&quot; &quot;README.md&quot; ## [31] &quot;report.Rmd&quot; &quot;store.Rmd&quot; &quot;time.Rmd&quot; ## [34] &quot;triggers.Rmd&quot; &quot;visuals.Rmd&quot; 5.5 The workflow plan data frame Now that your workspace of imports is prepared, we can outline the real work step by step in a workflow plan data frame. load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). my_plan ## # A tibble: 15 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;report.m… ## 2 small simulate(48) ## 3 large simulate(64) ## 4 regression1_small reg1(small) ## 5 regression1_large reg1(large) ## 6 regression2_small reg2(small) ## 7 regression2_large reg2(large) ## 8 summ_regression1_… suppressWarnings(summary(regression1_small$residual… ## 9 summ_regression1_… suppressWarnings(summary(regression1_large$residual… ## 10 summ_regression2_… suppressWarnings(summary(regression2_small$residual… ## 11 summ_regression2_… suppressWarnings(summary(regression2_large$residual… ## 12 coef_regression1_… suppressWarnings(summary(regression1_small))$coeffi… ## 13 coef_regression1_… suppressWarnings(summary(regression1_large))$coeffi… ## 14 coef_regression2_… suppressWarnings(summary(regression2_small))$coeffi… ## 15 coef_regression2_… suppressWarnings(summary(regression2_large))$coeffi… Each row is an intermediate step, and each command generates a single target. A target is an output R object (cached when generated) or an output file (specified with single quotes), and a command just an ordinary piece of R code (not necessarily a single function call). Commands make use of targets generated by other commands, objects your environment, input files, and namespaced objects/functions from packages (referenced with :: or :::). These dependencies give your project an underlying network representation. # Hover, click, drag, zoom, and pan. config &lt;- drake_config(my_plan) vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) # Also drake_graph() You can also check the dependencies of individual targets and imported functions. deps_code(reg2) ## $globals ## [1] &quot;lm&quot; deps_code(my_plan$command[1]) # Files like report.Rmd are single-quoted. ## $globals ## [1] &quot;knit&quot; ## ## $loadd ## [1] &quot;large&quot; ## ## $readd ## [1] &quot;small&quot; &quot;coef_regression2_small&quot; ## ## $knitr_in ## [1] &quot;\\&quot;report.Rmd\\&quot;&quot; ## ## $file_out ## [1] &quot;\\&quot;report.md\\&quot;&quot; deps_code(my_plan$command[nrow(my_plan)]) ## $globals ## [1] &quot;suppressWarnings&quot; &quot;summary&quot; &quot;regression2_large&quot; List all the reproducibly-tracked objects and files. tracked(config) ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [7] &quot;large&quot; &quot;random_rows&quot; ## [9] &quot;reg1&quot; &quot;reg2&quot; ## [11] &quot;regression1_large&quot; &quot;regression1_small&quot; ## [13] &quot;regression2_large&quot; &quot;regression2_small&quot; ## [15] &quot;report&quot; &quot;simulate&quot; ## [17] &quot;small&quot; &quot;summ_regression1_large&quot; ## [19] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [21] &quot;summ_regression2_small&quot; Check for circular reasoning, missing input files, and other pitfalls. check_plan(my_plan) 5.6 Generate the workflow plan The workflow plan data frame my_plan would be a pain to write by hand, so drake has functions to help you. Here are the commands to generate the bootstrapped datasets. my_datasets &lt;- drake_plan( small = simulate(48), large = simulate(64)) my_datasets ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 small simulate(48) ## 2 large simulate(64) For multiple replicates: expand_plan(my_datasets, values = c(&quot;rep1&quot;, &quot;rep2&quot;)) ## # A tibble: 4 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 small_rep1 simulate(48) ## 2 small_rep2 simulate(48) ## 3 large_rep1 simulate(64) ## 4 large_rep2 simulate(64) Here is a template for applying our regression models to our bootstrapped datasets. methods &lt;- drake_plan( regression1 = reg1(dataset__), regression2 = reg2(dataset__)) methods ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 regression1 reg1(dataset__) ## 2 regression2 reg2(dataset__) We evaluate the dataset__ wildcard to generate all the regression commands we need. my_analyses &lt;- plan_analyses(methods, data = my_datasets) my_analyses ## # A tibble: 4 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 regression1_small reg1(small) ## 2 regression1_large reg1(large) ## 3 regression2_small reg2(small) ## 4 regression2_large reg2(large) Next, we summarize each analysis of each dataset. We calculate descriptive statistics on the residuals, and we collect the regression coefficients and their p-values. summary_types &lt;- drake_plan( summ = suppressWarnings(summary(analysis__$residuals)), coef = suppressWarnings(summary(analysis__))$coefficients ) summary_types ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 summ suppressWarnings(summary(analysis__$residuals)) ## 2 coef suppressWarnings(summary(analysis__))$coefficients results &lt;- plan_summaries(summary_types, analyses = my_analyses, datasets = my_datasets, gather = NULL) results ## # A tibble: 8 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 summ_regression1_sma… suppressWarnings(summary(regression1_small$residu… ## 2 summ_regression1_lar… suppressWarnings(summary(regression1_large$residu… ## 3 summ_regression2_sma… suppressWarnings(summary(regression2_small$residu… ## 4 summ_regression2_lar… suppressWarnings(summary(regression2_large$residu… ## 5 coef_regression1_sma… suppressWarnings(summary(regression1_small))$coef… ## 6 coef_regression1_lar… suppressWarnings(summary(regression1_large))$coef… ## 7 coef_regression2_sma… suppressWarnings(summary(regression2_small))$coef… ## 8 coef_regression2_lar… suppressWarnings(summary(regression2_large))$coef… The gather feature reduces a collection of targets to a single target. The resulting commands are long, so gathering is deactivated for the sake of readability. For your knitr reports, use knitr_in() in your commands so that report.Rmd is a dependency and targets loaded with loadd() and readd() in active code chunks are also dependencies. Use file_out() to tell drake that the target is a file output. If the file is an output, you do not need to name the target. The target name will be the name of the output file in quotes. report &lt;- drake_plan( report = knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) ) report ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;report.md\\&quot;), quiet =… Finally, consolidate your workflow using rbind(). Row order does not matter. my_plan &lt;- rbind(report, my_datasets, my_analyses, results) my_plan ## # A tibble: 15 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;report.m… ## 2 small simulate(48) ## 3 large simulate(64) ## 4 regression1_small reg1(small) ## 5 regression1_large reg1(large) ## 6 regression2_small reg2(small) ## 7 regression2_large reg2(large) ## 8 summ_regression1_… suppressWarnings(summary(regression1_small$residual… ## 9 summ_regression1_… suppressWarnings(summary(regression1_large$residual… ## 10 summ_regression2_… suppressWarnings(summary(regression2_small$residual… ## 11 summ_regression2_… suppressWarnings(summary(regression2_large$residual… ## 12 coef_regression1_… suppressWarnings(summary(regression1_small))$coeffi… ## 13 coef_regression1_… suppressWarnings(summary(regression1_large))$coeffi… ## 14 coef_regression2_… suppressWarnings(summary(regression2_small))$coeffi… ## 15 coef_regression2_… suppressWarnings(summary(regression2_large))$coeffi… 5.7 Flexible workflow plan generation If your workflow does not fit the rigid datasets/analyses/summaries framework, consider using functions expand_plan(), evaluate_plan(), gather_plan(), and reduce_plan(). df &lt;- drake_plan(data = simulate(center = MU, scale = SIGMA)) df ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data simulate(center = MU, scale = SIGMA) df &lt;- expand_plan(df, values = c(&quot;rep1&quot;, &quot;rep2&quot;)) df ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data_rep1 simulate(center = MU, scale = SIGMA) ## 2 data_rep2 simulate(center = MU, scale = SIGMA) evaluate_plan(df, wildcard = &quot;MU&quot;, values = 1:2) ## # A tibble: 4 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data_rep1_1 simulate(center = 1, scale = SIGMA) ## 2 data_rep1_2 simulate(center = 2, scale = SIGMA) ## 3 data_rep2_1 simulate(center = 1, scale = SIGMA) ## 4 data_rep2_2 simulate(center = 2, scale = SIGMA) evaluate_plan(df, wildcard = &quot;MU&quot;, values = 1:2, expand = FALSE) ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data_rep1 simulate(center = 1, scale = SIGMA) ## 2 data_rep2 simulate(center = 2, scale = SIGMA) evaluate_plan(df, rules = list(MU = 1:2, SIGMA = c(0.1, 1)), expand = FALSE) ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data_rep1 simulate(center = 1, scale = 0.1) ## 2 data_rep2 simulate(center = 2, scale = 1) evaluate_plan(df, rules = list(MU = 1:2, SIGMA = c(0.1, 1, 10))) ## # A tibble: 12 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data_rep1_1_0.1 simulate(center = 1, scale = 0.1) ## 2 data_rep1_1_1 simulate(center = 1, scale = 1) ## 3 data_rep1_1_10 simulate(center = 1, scale = 10) ## 4 data_rep1_2_0.1 simulate(center = 2, scale = 0.1) ## 5 data_rep1_2_1 simulate(center = 2, scale = 1) ## 6 data_rep1_2_10 simulate(center = 2, scale = 10) ## 7 data_rep2_1_0.1 simulate(center = 1, scale = 0.1) ## 8 data_rep2_1_1 simulate(center = 1, scale = 1) ## 9 data_rep2_1_10 simulate(center = 1, scale = 10) ## 10 data_rep2_2_0.1 simulate(center = 2, scale = 0.1) ## 11 data_rep2_2_1 simulate(center = 2, scale = 1) ## 12 data_rep2_2_10 simulate(center = 2, scale = 10) gather_plan(df) ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 target list(data_rep1 = data_rep1, data_rep2 = data_rep2) gather_plan(df, target = &quot;my_summaries&quot;, gather = &quot;rbind&quot;) ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 my_summaries rbind(data_rep1 = data_rep1, data_rep2 = data_rep2) x_plan &lt;- evaluate_plan( drake_plan(x = VALUE), wildcard = &quot;VALUE&quot;, values = 1:8 ) x_plan ## # A tibble: 8 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 x_1 1 ## 2 x_2 2 ## 3 x_3 3 ## 4 x_4 4 ## 5 x_5 5 ## 6 x_6 6 ## 7 x_7 7 ## 8 x_8 8 x_plan ## # A tibble: 8 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 x_1 1 ## 2 x_2 2 ## 3 x_3 3 ## 4 x_4 4 ## 5 x_5 5 ## 6 x_6 6 ## 7 x_7 7 ## 8 x_8 8 reduce_plan( x_plan, target = &quot;x_sum&quot;, pairwise = TRUE, begin = &quot;fun(&quot;, op = &quot;, &quot;, end = &quot;)&quot; ) ## # A tibble: 7 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 x_sum_1 fun(x_1, x_2) ## 2 x_sum_2 fun(x_3, x_4) ## 3 x_sum_3 fun(x_5, x_6) ## 4 x_sum_4 fun(x_7, x_8) ## 5 x_sum_5 fun(x_sum_1, x_sum_2) ## 6 x_sum_6 fun(x_sum_3, x_sum_4) ## 7 x_sum fun(x_sum_5, x_sum_6) 5.8 Run the workflow You may want to check for outdated or missing targets/imports first. config &lt;- drake_config(my_plan, verbose = FALSE) outdated(config) # Targets that need to be (re)built. ## [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [5] &quot;large&quot; &quot;regression1_large&quot; ## [7] &quot;regression1_small&quot; &quot;regression2_large&quot; ## [9] &quot;regression2_small&quot; &quot;report&quot; ## [11] &quot;small&quot; &quot;summ_regression1_large&quot; ## [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [15] &quot;summ_regression2_small&quot; missed(config) # Checks your workspace. ## character(0) Then just make(my_plan). make(my_plan) ## target large ## target small ## target regression1_large ## target regression2_large ## target regression1_small ## target regression2_small ## target summ_regression1_large ## target coef_regression1_large ## target summ_regression2_large ## target coef_regression2_large ## target summ_regression1_small ## target coef_regression1_small ## target coef_regression2_small ## target summ_regression2_small ## target report For the reg2() model on the small dataset, the p-value on x2 is so small that there may be an association between weight and fuel efficiency after all. readd(coef_regression2_small) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.8369441 1.07966324 25.782988 5.443436e-29 ## x2 -0.6359335 0.07138918 -8.907981 1.408597e-11 The non-file dependencies of your last target are already loaded in your workspace. ls() ## [1] &quot;combos&quot; &quot;config&quot; &quot;create_plot&quot; ## [4] &quot;data_plan&quot; &quot;df&quot; &quot;fit_gsp_model&quot; ## [7] &quot;get_logs&quot; &quot;get_rmspe&quot; &quot;hist&quot; ## [10] &quot;latest_log_date&quot; &quot;make_my_plot&quot; &quot;make_my_table&quot; ## [13] &quot;methods&quot; &quot;model_plan&quot; &quot;my_analyses&quot; ## [16] &quot;my_datasets&quot; &quot;my_plan&quot; &quot;output_plan&quot; ## [19] &quot;output_types&quot; &quot;package_list&quot; &quot;plan&quot; ## [22] &quot;plot_rmspe&quot; &quot;predictors&quot; &quot;Produc&quot; ## [25] &quot;random_rows&quot; &quot;reg1&quot; &quot;reg2&quot; ## [28] &quot;report_plan&quot; &quot;results&quot; &quot;rmspe_args&quot; ## [31] &quot;rmspe_plan&quot; &quot;rmspe_results_plan&quot; &quot;simulate&quot; ## [34] &quot;summary_types&quot; &quot;url&quot; &quot;whole_plan&quot; ## [37] &quot;x_plan&quot; outdated(config) # Everything is up to date. ## character(0) build_times(digits = 4) # How long did it take to make each target? ## # A tibble: 21 x 5 ## item type elapsed user system ## * &lt;chr&gt; &lt;chr&gt; &lt;S4: Duration&gt; &lt;S4: Duratio&gt; &lt;S4: Duratio&gt; ## 1 &quot;\\&quot;report.md\\&quot;&quot; target 0.074s 0.068s 0.008s ## 2 &quot;\\&quot;report.Rmd\\&quot;&quot; import 0.005s 0.004s 0s ## 3 coef_regression1_lar… target 0.009s 0.008s 0s ## 4 coef_regression1_sma… target 0.009s 0.008s 0s ## 5 coef_regression2_lar… target 0.009s 0.012s 0s ## 6 coef_regression2_sma… target 0.009s 0.008s 0s ## 7 large target 0.009s 0.012s 0s ## 8 random_rows import 0.004s 0.004s 0s ## 9 reg1 import 0.003s 0.004s 0s ## 10 reg2 import 0.004s 0s 0s ## # ... with 11 more rows See also predict_runtime() and rate_limiting_times(). In the new graph, the black nodes from before are now green. # Hover, click, drag, zoom, and explore. vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) Optionally, get visNetwork nodes and edges so you can make your own plot with visNetwork() or render_drake_graph(). drake_graph_info(config) Use readd() and loadd() to load targets into your workspace. (They are cached in the hidden .drake/ folder using storr). There are many more functions for interacting with the cache. readd(coef_regression2_large) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.7764504 0.85871209 32.34664 1.569624e-40 ## x2 -0.7056179 0.06942808 -10.16329 7.950479e-15 loadd(small) head(small) ## x y ## 1 5.424 10.4 ## 2 3.440 17.8 ## 3 3.440 19.2 ## 4 3.170 15.8 ## 5 3.730 17.3 ## 6 3.845 19.2 rm(small) cached(small, large) ## small large ## TRUE TRUE cached() ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [7] &quot;large&quot; &quot;random_rows&quot; ## [9] &quot;reg1&quot; &quot;reg2&quot; ## [11] &quot;regression1_large&quot; &quot;regression1_small&quot; ## [13] &quot;regression2_large&quot; &quot;regression2_small&quot; ## [15] &quot;report&quot; &quot;simulate&quot; ## [17] &quot;small&quot; &quot;summ_regression1_large&quot; ## [19] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [21] &quot;summ_regression2_small&quot; built() ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;coef_regression1_large&quot; ## [3] &quot;coef_regression1_small&quot; &quot;coef_regression2_large&quot; ## [5] &quot;coef_regression2_small&quot; &quot;large&quot; ## [7] &quot;regression1_large&quot; &quot;regression1_small&quot; ## [9] &quot;regression2_large&quot; &quot;regression2_small&quot; ## [11] &quot;report&quot; &quot;small&quot; ## [13] &quot;summ_regression1_large&quot; &quot;summ_regression1_small&quot; ## [15] &quot;summ_regression2_large&quot; &quot;summ_regression2_small&quot; imported() ## [1] &quot;\\&quot;report.Rmd\\&quot;&quot; &quot;random_rows&quot; &quot;reg1&quot; &quot;reg2&quot; ## [5] &quot;simulate&quot; head(read_drake_plan()) ## # A tibble: 6 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;report.md\\&quot;)… ## 2 small simulate(48) ## 3 large simulate(64) ## 4 regression1_sm… reg1(small) ## 5 regression1_la… reg1(large) ## 6 regression2_sm… reg2(small) head(progress()) # See also in_progress() ## $method ## NULL ## ## $url ## NULL ## ## $headers ## NULL ## ## $fields ## NULL ## ## $options ## $options$noprogress ## [1] FALSE ## ## $options$progressfunction ## function (down, up) ## { ## if (type == &quot;down&quot;) { ## total &lt;- down[[1]] ## now &lt;- down[[2]] ## } ## else { ## total &lt;- up[[1]] ## now &lt;- up[[2]] ## } ## if (total == 0 &amp;&amp; now == 0) { ## bar &lt;&lt;- NULL ## } ## else if (total == 0) { ## cat(&quot;\\rDownloading: &quot;, bytes(now, digits = 2), &quot; &quot;, ## sep = &quot;&quot;, file = con) ## utils::flush.console() ## } ## else { ## if (is.null(bar)) { ## bar &lt;&lt;- utils::txtProgressBar(max = total, style = 3, ## file = con) ## } ## utils::setTxtProgressBar(bar, now) ## if (now == total) ## close(bar) ## } ## TRUE ## } ## &lt;bytecode: 0xfecf278&gt; ## &lt;environment: 0xfed1198&gt; ## ## ## $auth_token ## NULL progress(large) ## Error in match.arg(type): object &#39;large&#39; not found # drake_session() # sessionInfo() of the last make() # nolint The next time you run make(my_plan), nothing will build because drake knows everything is already up to date. config &lt;- make(my_plan) # Will use config later. See also drake_config(). ## All targets are already up to date. But if you change one of your functions, commands, or other dependencies, drake will update the affected targets. Suppose we change the quadratic term to a cubic term in reg2(). We might want to do this if we suspect a cubic relationship between tons and miles per gallon. reg2 &lt;- function(d) { d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } The targets that depend on reg2() need to be rebuilt. outdated(config) ## [1] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [3] &quot;regression2_large&quot; &quot;regression2_small&quot; ## [5] &quot;report&quot; &quot;summ_regression2_large&quot; ## [7] &quot;summ_regression2_small&quot; Advanced: To get a rough idea of why a target is out of date, you can use dependency_profile(). It will tell you if any of the following changed since the last make(): The command in the workflow plan data frame. At least one non-file dependency. (For this, the imports have to be up to date and cached, either with make(), make(skip_targets = TRUE), outdated(), or similar.) At least one input file declared with file_in() or knitr_in(). At least one output file declared with file_out(). dependency_profile(target = regression2_small, config = config) ## # A tibble: 4 x 4 ## hash changed old_hash new_hash ## &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 command FALSE 9d241f1270a356091daacc2e82c… 9d241f1270a356091daacc2e82… ## 2 depend TRUE 46eb742feaf391c73a7c38b19ef… d329d343cb18f2d87c9e90a69e… ## 3 file_in FALSE e3b0c44298fc1c149afbf4c8996… e3b0c44298fc1c149afbf4c899… ## 4 file_o… FALSE e3b0c44298fc1c149afbf4c8996… e3b0c44298fc1c149afbf4c899… # Hover, click, drag, zoom, and explore. vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) The next make() will rebuild the targets depending on reg2() and leave everything else alone. make(my_plan) ## target regression2_large ## target regression2_small ## target summ_regression2_large ## target coef_regression2_large ## target coef_regression2_small ## target summ_regression2_small ## target report Trivial changes to whitespace and comments are totally ignored. reg2 &lt;- function(d) { d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) # I indented here. } outdated(config) # Everything is up to date. ## character(0) Drake cares about nested functions too: nontrivial changes to random_rows() will propagate to simulate() and all the downstream targets. random_rows &lt;- function(data, n){ n &lt;- n + 1 data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } outdated(config) ## [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [5] &quot;large&quot; &quot;regression1_large&quot; ## [7] &quot;regression1_small&quot; &quot;regression2_large&quot; ## [9] &quot;regression2_small&quot; &quot;report&quot; ## [11] &quot;small&quot; &quot;summ_regression1_large&quot; ## [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [15] &quot;summ_regression2_small&quot; make(my_plan) ## target large ## target small ## target regression1_large ## target regression2_large ## target regression1_small ## target regression2_small ## target summ_regression1_large ## target coef_regression1_large ## target summ_regression2_large ## target coef_regression2_large ## target summ_regression1_small ## target coef_regression1_small ## target coef_regression2_small ## target summ_regression2_small ## target report Need to add new work on the fly? Just append rows to the workflow plan. If the rest of your workflow is up to date, only the new work is run. new_simulation &lt;- function(n){ data.frame(x = rnorm(n), y = rnorm(n)) } additions &lt;- drake_plan( new_data = new_simulation(36) + sqrt(10)) additions ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 new_data new_simulation(36) + sqrt(10) my_plan &lt;- rbind(my_plan, additions) my_plan ## # A tibble: 16 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;report.m… ## 2 small simulate(48) ## 3 large simulate(64) ## 4 regression1_small reg1(small) ## 5 regression1_large reg1(large) ## 6 regression2_small reg2(small) ## 7 regression2_large reg2(large) ## 8 summ_regression1_… suppressWarnings(summary(regression1_small$residual… ## 9 summ_regression1_… suppressWarnings(summary(regression1_large$residual… ## 10 summ_regression2_… suppressWarnings(summary(regression2_small$residual… ## 11 summ_regression2_… suppressWarnings(summary(regression2_large$residual… ## 12 coef_regression1_… suppressWarnings(summary(regression1_small))$coeffi… ## 13 coef_regression1_… suppressWarnings(summary(regression1_large))$coeffi… ## 14 coef_regression2_… suppressWarnings(summary(regression2_small))$coeffi… ## 15 coef_regression2_… suppressWarnings(summary(regression2_large))$coeffi… ## 16 new_data new_simulation(36) + sqrt(10) make(my_plan) ## target new_data If you ever need to erase your work, use clean(). The next make() will rebuild any cleaned targets, so be careful. You may notice that by default, the size of the cache does not go down very much. To purge old data, you could use clean(garbage_collection = TRUE, purge = TRUE). To do garbage collection without removing any important targets, use drake_gc(). # Uncaches individual targets and imported objects. clean(small, reg1, verbose = FALSE) clean(verbose = FALSE) # Cleans all targets out of the cache. drake_gc(verbose = FALSE) # Just garbage collection. clean(destroy = TRUE, verbose = FALSE) # removes the cache entirely 5.9 Automatic watching for changed dependencies As you have seen with reg2(), drake reacts to changes in dependencies. In other words, make() notices when your dependencies are different from last time, rebuilds any affected targets, and continues downstream. In particular, drake watches for nontrivial changes to the following items as long as they are connected to your workflow. The output values of targets in your workflow plan. The commands themselves. External files, if their names are enclosed in single quotes in commands. R objects mentioned in the commands, including but not limited to user-defined functions and functions from packages. R objects (but not files) nested inside user-defined functions. For packages exposed with expose_imports(), R objects (but not files) nested inside package functions. Files declared with file_in() inside your commands or custom functions. knitr reports declared with knitr_in() in your commands, along with any targets explicitly loaded in active code chunks with loadd() or readd(). Do not use knitr_in() inside your imported functions. Files declared with file_out() in your commands. Do not use file_out() inside your imported functions. To enhance reproducibility beyond the scope of drake, you might consider packrat and a container tool (such as Singularity or Docker. Packrat creates a tightly-controlled local library of packages to extend the shelf life of your project. And with containerization, you can execute your project on a virtual machine to ensure platform independence. Together, packrat and containers can help others reproduce your work even if they have different software and hardware. 5.10 A note on tidy evaluation Running commands in your R console is not always exactly like running them with make(). That’s because make() uses tidy evaluation as implemented in the rlang package. # This workflow plan uses rlang&#39;s quasiquotation operator `!!`. my_plan &lt;- drake_plan(list = c( little_b = &quot;\\&quot;b\\&quot;&quot;, letter = &quot;!!little_b&quot; )) my_plan ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 little_b &quot;\\&quot;b\\&quot;&quot; ## 2 letter !!little_b make(my_plan) ## target little_b ## target letter readd(letter) ## [1] &quot;b&quot; For the commands you specify the free-form ... argument, drake_plan() also supports tidy evaluation. For example, it supports quasiquotation with the !! argument. Use tidy_evaluation = FALSE or the list argument to suppress this behavior. my_variable &lt;- 5 drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;) ) ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 a 5 ## 2 b 5 + 1 ## 3 d !!my_variable drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;), tidy_evaluation = FALSE ) ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 a !!my_variable ## 2 b !!my_variable + 1 ## 3 d !!my_variable For instances of !! that remain in the workflow plan, make() will run these commands in tidy fashion, evaluating the !! operator using the environment you provided. 5.11 Need more speed? drake has extensive high-performance computing support, from local multicore processing to serious distributed computing across multiple nodes of a cluster. See the high-performance computing chapter for detailed instructions. "],
["plans.html", "Chapter 6 Workflow plan data frames 6.1 What is a workflow plan data frame? 6.2 Plans are like R scripts. 6.3 So why do we use plans? 6.4 Automatic dependency detection 6.5 Automatically generating workflow plans 6.6 Optional columns in your plan.", " Chapter 6 Workflow plan data frames 6.1 What is a workflow plan data frame? Your workflow plan data frame is the object where you declare all the objects and files you are going to produce when you run your project. It enumerates each output R object, or target, and the command that will produce it. Here is the workflow plan from our previous example. plan &lt;- drake_plan( raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) %&gt;% select(-X__1), hist = create_plot(data), fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) ) plan ## # A tibble: 5 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 raw_data &quot;readxl::read_excel(file_in(\\&quot;raw_data.xlsx\\&quot;))&quot; ## 2 data &quot;raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … ## 3 hist create_plot(data) ## 4 fit lm(Sepal.Width ~ Petal.Width + Species, data) ## 5 report &quot;rmarkdown::render(knitr_in(\\&quot;report.Rmd\\&quot;), output_file = fil… When you run make(plan), drake will produce targets raw_data, data, hist, fit, and report. 6.2 Plans are like R scripts. Your workflow plan data frame is like your top-level “run everything” script in a project. In fact, you can convert back and forth between plans and scripts using functions plan_to_code() and code_to_plan() (please note the caveats here). plan_to_code(plan, &quot;new_script.R&quot;) ## Loading required namespace: styler cat(readLines(&quot;new_script.R&quot;), sep = &quot;\\n&quot;) ## raw_data &lt;- readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) ## data &lt;- raw_data %&gt;% ## mutate(Species = forcats::fct_inorder(Species)) %&gt;% ## select(-X__1) ## fit &lt;- lm(Sepal.Width ~ Petal.Width + Species, data) ## hist &lt;- create_plot(data) ## report &lt;- rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), ## output_file = file_out(&quot;report.html&quot;), ## quiet = TRUE ## ) code_to_plan(&quot;new_script.R&quot;) ## # A tibble: 5 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 raw_data &quot;readxl::read_excel(file_in(\\&quot;raw_data.xlsx\\&quot;))&quot; ## 2 data &quot;raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … ## 3 fit lm(Sepal.Width ~ Petal.Width + Species, data) ## 4 hist create_plot(data) ## 5 report &quot;rmarkdown::render(knitr_in(\\&quot;report.Rmd\\&quot;), output_file = fil… And plan_to_notebook() turns plans into R notebooks. plan_to_notebook(plan, &quot;new_notebook.Rmd&quot;) cat(readLines(&quot;new_notebook.Rmd&quot;), sep = &quot;\\n&quot;) ## --- ## title: &quot;My Notebook&quot; ## output: html_notebook ## --- ## ## ```{r my_code} ## raw_data &lt;- readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) ## data &lt;- raw_data %&gt;% ## mutate(Species = forcats::fct_inorder(Species)) %&gt;% ## select(-X__1) ## fit &lt;- lm(Sepal.Width ~ Petal.Width + Species, data) ## hist &lt;- create_plot(data) ## report &lt;- rmarkdown::render(knitr_in(&quot;report.Rmd&quot;), ## output_file = file_out(&quot;report.html&quot;), ## quiet = TRUE ## ) ## ``` code_to_plan(&quot;new_notebook.Rmd&quot;) ## # A tibble: 5 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 raw_data &quot;readxl::read_excel(file_in(\\&quot;raw_data.xlsx\\&quot;))&quot; ## 2 data &quot;raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … ## 3 fit lm(Sepal.Width ~ Petal.Width + Species, data) ## 4 hist create_plot(data) ## 5 report &quot;rmarkdown::render(knitr_in(\\&quot;report.Rmd\\&quot;), output_file = fil… 6.3 So why do we use plans? The workflow plan may seem like a burden to set up, and the use of data frames may seem counterintuitive at first, but the rewards are worth the effort. 6.3.1 You can skip up-to-date work. As we saw in our previous example, subsequent make()s skip work that is already up to date. To skip steps of the workflow, we need to know what those steps actaully are. Workflow plan data frames formally define skippable steps, whereas scripts and notebooks on their own do not. This general approach of declaring targets in advance has stood the test of time. The idea dates at least as far back as GNU Make, which uses Makefiles to declare targets and dependencies. drake’s predecessor remake uses YAML files in a similar way. 6.3.2 Data frames scale well. Makefiles are successful for Make because they accommodate software written in multiple languages. However, such external configuration files are not the best solution for R. Maintaining a Makefile or a remake YAML file requires a lot of manual typing. But with drake plans, you can use the usual data frame manipulation tools to expand, generate, and piece together large projets. The gsp example shows how expand.grid() and rbind() to automatically create plans with hundreds of targets. In addition, drake has a wildcard templating mechanism to generate large plans. 6.3.3 You do not need to worry about which targets run first. When you call make() on the plan above, drake takes care of &quot;raw_data.xlsx&quot;, then raw_data, and then data in sequence. Once data completes, fit and hist can start in any order, and then report begins once everything else is done. The execution does not depend on the order of the rows in your plan. In other words, the following plan is equivalent. drake_plan( fit = lm(Sepal.Width ~ Petal.Width + Species, data), report = rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ), hist = create_plot(data), data = raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) %&gt;% select(-X__1), raw_data = readxl::read_excel(file_in(&quot;raw_data.xlsx&quot;)) ) ## # A tibble: 5 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 fit lm(Sepal.Width ~ Petal.Width + Species, data) ## 2 report &quot;rmarkdown::render(knitr_in(\\&quot;report.Rmd\\&quot;), output_file = fil… ## 3 hist create_plot(data) ## 4 data &quot;raw_data %&gt;% mutate(Species = forcats::fct_inorder(Species)) … ## 5 raw_data &quot;readxl::read_excel(file_in(\\&quot;raw_data.xlsx\\&quot;))&quot; 6.4 Automatic dependency detection Why can you safely scramble the rows of a drake plan? Why is row order irrelevant to execution order? Because drake analyzes commands for dependencies, and make() processes those dependencies before moving on to downstream targets. To detect dependencies, drake walks through the abstract syntax tree of every piece of code to find the objects and files relevant to the workflow pipeline. create_plot &lt;- function(data) { ggplot(data, aes_string(x = &quot;Petal.Width&quot;, fill = &quot;Species&quot;)) + geom_histogram() } deps_code(create_plot) ## $globals ## [1] &quot;ggplot&quot; &quot;aes_string&quot; &quot;geom_histogram&quot; deps_code( quote({ some_function_i_wrote(data) rmarkdown::render( knitr_in(&quot;report.Rmd&quot;), output_file = file_out(&quot;report.html&quot;), quiet = TRUE ) }) ) ## $globals ## [1] &quot;some_function_i_wrote&quot; &quot;data&quot; ## ## $namespaced ## [1] &quot;rmarkdown::render&quot; ## ## $loadd ## [1] &quot;fit&quot; ## ## $readd ## [1] &quot;hist&quot; ## ## $knitr_in ## [1] &quot;\\&quot;report.Rmd\\&quot;&quot; ## ## $file_out ## [1] &quot;\\&quot;report.html\\&quot;&quot; drake detects dependencies without actually running the command. file.exists(&quot;report.html&quot;) ## [1] FALSE Automatically detected dependencies include: Other targets in the plan. Objects and functions in your environment. Objects and functions from packages that you reference with :: or ::: (namespaced objects). Input and output files declared in your commands with file_in(), knitr_in(), or file_out(). Input files declared in imported functions ((2) or (3)) with file_in() or knitr_in(). For knitr or R Markdown reports declared with knitr_in() (example here), drake scans active code chunks for objects mentioned with loadd() and readd(). So when fit or hist change, drake rebuilds the report target to produce the file report.html. Targets can depend on one another through file_in()/file_out() connections. saveRDS(1, &quot;start.rds&quot;) write_files &lt;- function(){ x &lt;- readRDS(file_in(&quot;start.rds&quot;)) for (file in letters[1:3]){ saveRDS(x, file) } } small_plan &lt;- drake_plan( x = { write_files() file_out(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) }, y = readRDS(file_in(&quot;a&quot;)) ) config &lt;- drake_config(small_plan) vis_drake_graph(config) So when target x changes the output for files &quot;a&quot;, &quot;b&quot;, or &quot;c&quot;, drake knows to rebuild target y. In addition, if you accidentally modify any of these output files by hand, drake will run the command of x to restore the files to a reproducible state. 6.5 Automatically generating workflow plans drake provides many more utilites that increase the flexibility of workflow plan generation beyond expand.grid(). drake_plan() map_plan() evaluate_plan() plan_analyses() plan_summaries() expand_plan() gather_by() reduce_by() gather_plan() reduce_plan() 6.5.1 map_plan() purrr-like functional programming is like looping, but cleaner. The idea is to iterate the same computation over multiple different data points. You write a function to do something once, and a map()-like helper invokes it on each point in your dataset. drake’s version of map() — or more precisely, pmap_df() — is map_plan(). In the following example, we want to know how well each pair covariates in the mtcars dataset can predict fuel efficiency (in miles per gallon). We will try multiple pairs of covariates using the same statistical analysis, so it is a great time for drake-flavored functional programming with map_plan(). As with its cousin, pmap_df(), map_plan() needs A function. A grid of function arguments. Our function fits a fuel efficiency model given a single pair of covariate names x1 and x2. my_model_fit &lt;- function(x1, x2, data){ lm(as.formula(paste(&quot;mpg ~&quot;, x1, &quot;+&quot;, x2)), data = data) } Our grid of function arguments is a data frame of possible values for x1, x2, and data. covariates &lt;- setdiff(colnames(mtcars), &quot;mpg&quot;) # Exclude the response variable. args &lt;- combn(covariates, 2) %&gt;% # Take all possible pairs. t() %&gt;% # Take the transpose so each row is a single pair. %&gt;% tibble::as_tibble() # Tibbles are so nice. colnames(args) &lt;- c(&quot;x1&quot;, &quot;x2&quot;) # The column names must be the argument names of my_model_fit() args$data &lt;- &quot;mtcars&quot; args ## # A tibble: 45 x 3 ## x1 x2 data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 cyl disp mtcars ## 2 cyl hp mtcars ## 3 cyl drat mtcars ## 4 cyl wt mtcars ## 5 cyl qsec mtcars ## 6 cyl vs mtcars ## 7 cyl am mtcars ## 8 cyl gear mtcars ## 9 cyl carb mtcars ## 10 disp hp mtcars ## # ... with 35 more rows Each row of args corresponds to a call to my_model_fit(). To actually write out all those function calls, we use map_plan(). map_plan(args, my_model_fit) ## # A tibble: 45 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 my_model_fit_501e0… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;disp\\&quot;, data = … ## 2 my_model_fit_d5de1… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;hp\\&quot;, data = \\&quot;… ## 3 my_model_fit_eac6c… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;drat\\&quot;, data = … ## 4 my_model_fit_3900e… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;wt\\&quot;, data = \\&quot;… ## 5 my_model_fit_a2d79… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;qsec\\&quot;, data = … ## 6 my_model_fit_f5c0a… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;vs\\&quot;, data = \\&quot;… ## 7 my_model_fit_507d2… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;am\\&quot;, data = \\&quot;… ## 8 my_model_fit_b5f9a… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;gear\\&quot;, data = … ## 9 my_model_fit_8c4c5… &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;carb\\&quot;, data = … ## 10 my_model_fit_f9bb9… &quot;my_model_fit(x1 = \\&quot;disp\\&quot;, x2 = \\&quot;hp\\&quot;, data = \\… ## # ... with 35 more rows We now have a plan, but it has a couple issues. The data argument should be a symbol. In other words, we want my_model_fit(data = mtcars), not my_model_fit(data = &quot;mtcars&quot;). So we use the syms() function from the rlang package turn args$data into a list of symbols. The default argument names are ugly, so we can add a new &quot;id&quot; column to args (or select one with the id argument of map_plan()). # Fixes (1) args$data &lt;- rlang::syms(args$data) # Alternative if each element of `args$data` is code with multiple symbols: # args$data &lt;- purrr::map(args$data, rlang::parse_expr) # Fixes (2) args$id &lt;- paste0(&quot;fit_&quot;, args$x1, &quot;_&quot;, args$x2) args ## # A tibble: 45 x 4 ## x1 x2 data id ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;chr&gt; ## 1 cyl disp &lt;symbol&gt; fit_cyl_disp ## 2 cyl hp &lt;symbol&gt; fit_cyl_hp ## 3 cyl drat &lt;symbol&gt; fit_cyl_drat ## 4 cyl wt &lt;symbol&gt; fit_cyl_wt ## 5 cyl qsec &lt;symbol&gt; fit_cyl_qsec ## 6 cyl vs &lt;symbol&gt; fit_cyl_vs ## 7 cyl am &lt;symbol&gt; fit_cyl_am ## 8 cyl gear &lt;symbol&gt; fit_cyl_gear ## 9 cyl carb &lt;symbol&gt; fit_cyl_carb ## 10 disp hp &lt;symbol&gt; fit_disp_hp ## # ... with 35 more rows Much better. Now, we can create the plan we will actually use. plan &lt;- map_plan(args, my_model_fit) plan ## # A tibble: 45 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 fit_cyl_disp &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;disp\\&quot;, data = mtcars)&quot; ## 2 fit_cyl_hp &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;hp\\&quot;, data = mtcars)&quot; ## 3 fit_cyl_drat &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;drat\\&quot;, data = mtcars)&quot; ## 4 fit_cyl_wt &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;wt\\&quot;, data = mtcars)&quot; ## 5 fit_cyl_qsec &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;qsec\\&quot;, data = mtcars)&quot; ## 6 fit_cyl_vs &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;vs\\&quot;, data = mtcars)&quot; ## 7 fit_cyl_am &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;am\\&quot;, data = mtcars)&quot; ## 8 fit_cyl_gear &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;gear\\&quot;, data = mtcars)&quot; ## 9 fit_cyl_carb &quot;my_model_fit(x1 = \\&quot;cyl\\&quot;, x2 = \\&quot;carb\\&quot;, data = mtcars)&quot; ## 10 fit_disp_hp &quot;my_model_fit(x1 = \\&quot;disp\\&quot;, x2 = \\&quot;hp\\&quot;, data = mtcars)&quot; ## # ... with 35 more rows We can now fit our models. make(plan, verbose = FALSE) And inspect the output. readd(fit_cyl_disp) ## ## Call: ## lm(formula = as.formula(paste(&quot;mpg ~&quot;, x1, &quot;+&quot;, x2)), data = data) ## ## Coefficients: ## (Intercept) cyl disp ## 34.66099 -1.58728 -0.02058 6.5.2 Wildcard templating In drake, you can write plans with wildcards. These wilrdards are placeholders for text in commands. By iterating over the possible values of a wildcard, you can easily generate plans with thousands of targets. Let’s say you are running a simulation study, and you need to generate sets of random numbers from different distributions. plan &lt;- drake_plan( t = rt(1000, df = 5), normal = runif(1000, mean = 0, sd = 1) ) If you need to generate many datasets with different means, you may wish to write out each target individually. drake_plan( t = rt(1000, df = 5), normal_0 = runif(1000, mean = 0, sd = 1), normal_1 = runif(1000, mean = 1, sd = 1), normal_2 = runif(1000, mean = 2, sd = 1), normal_3 = runif(1000, mean = 3, sd = 1), normal_4 = runif(1000, mean = 4, sd = 1), normal_5 = runif(1000, mean = 5, sd = 1), normal_6 = runif(1000, mean = 6, sd = 1), normal_7 = runif(1000, mean = 7, sd = 1), normal_8 = runif(1000, mean = 8, sd = 1), normal_9 = runif(1000, mean = 9, sd = 1) ) But writing all that code manually is a pain and prone to human error. Instead, use evaluate_plan() plan &lt;- drake_plan( t = rt(1000, df = 5), normal = runif(1000, mean = mean__, sd = 1) ) evaluate_plan(plan, wildcard = &quot;mean__&quot;, values = 0:9) ## # A tibble: 11 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 t rt(1000, df = 5) ## 2 normal_0 runif(1000, mean = 0, sd = 1) ## 3 normal_1 runif(1000, mean = 1, sd = 1) ## 4 normal_2 runif(1000, mean = 2, sd = 1) ## 5 normal_3 runif(1000, mean = 3, sd = 1) ## 6 normal_4 runif(1000, mean = 4, sd = 1) ## 7 normal_5 runif(1000, mean = 5, sd = 1) ## 8 normal_6 runif(1000, mean = 6, sd = 1) ## 9 normal_7 runif(1000, mean = 7, sd = 1) ## 10 normal_8 runif(1000, mean = 8, sd = 1) ## 11 normal_9 runif(1000, mean = 9, sd = 1) You can specify multiple wildcards at once. If multiple wildcards appear in the same command, you will get a new target for each unique combination of values. plan &lt;- drake_plan( t = rt(1000, df = df__), normal = runif(1000, mean = mean__, sd = sd__) ) evaluate_plan( plan, rules = list( mean__ = c(0, 1), sd__ = c(3, 4), df__ = 5:7 ) ) ## # A tibble: 7 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 t_5 rt(1000, df = 5) ## 2 t_6 rt(1000, df = 6) ## 3 t_7 rt(1000, df = 7) ## 4 normal_0_3 runif(1000, mean = 0, sd = 3) ## 5 normal_0_4 runif(1000, mean = 0, sd = 4) ## 6 normal_1_3 runif(1000, mean = 1, sd = 3) ## 7 normal_1_4 runif(1000, mean = 1, sd = 4) Set expand to FALSE to disable expansion. plan &lt;- drake_plan( t = rpois(samples__, lambda = mean__), normal = runif(samples__, mean = mean__) ) evaluate_plan( plan, rules = list( samples__ = c(50, 100), mean__ = c(1, 5) ), expand = FALSE ) ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 t rpois(50, lambda = 1) ## 2 normal runif(100, mean = 5) Wildcard templating can sometimes be tricky. For example, suppose your project is to analyze school data, and your workflow checks several metrics of several schools. The idea is to write a workflow plan with your metrics and let the wildcard templating expand over the available schools. hard_plan &lt;- drake_plan( credits = check_credit_hours(school__), students = check_students(school__), grads = check_graduations(school__), public_funds = check_public_funding(school__) ) evaluate_plan( hard_plan, rules = list(school__ = c(&quot;schoolA&quot;, &quot;schoolB&quot;, &quot;schoolC&quot;)) ) ## # A tibble: 12 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 credits_schoolA check_credit_hours(schoolA) ## 2 credits_schoolB check_credit_hours(schoolB) ## 3 credits_schoolC check_credit_hours(schoolC) ## 4 students_schoolA check_students(schoolA) ## 5 students_schoolB check_students(schoolB) ## 6 students_schoolC check_students(schoolC) ## 7 grads_schoolA check_graduations(schoolA) ## 8 grads_schoolB check_graduations(schoolB) ## 9 grads_schoolC check_graduations(schoolC) ## 10 public_funds_schoolA check_public_funding(schoolA) ## 11 public_funds_schoolB check_public_funding(schoolB) ## 12 public_funds_schoolC check_public_funding(schoolC) But what if some metrics do not make sense? For example, what if schoolC is a completely privately-funded school? With no public funds, check_public_funds(schoolC) may quit in error if we are not careful. This is where setting up workflow plans requires a little creativity. In this case, we recommend that you use two wildcards: one for all the schools and another for just the public schools. The new plan has no twelfth row. plan_template &lt;- drake_plan( school = get_school_data(&quot;school__&quot;), credits = check_credit_hours(all_schools__), students = check_students(all_schools__), grads = check_graduations(all_schools__), public_funds = check_public_funding(public_schools__) ) evaluate_plan( plan = plan_template, rules = list( school__ = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), all_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;), public_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;) ) ) ## # A tibble: 14 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 school_A &quot;get_school_data(\\&quot;A\\&quot;)&quot; ## 2 school_B &quot;get_school_data(\\&quot;B\\&quot;)&quot; ## 3 school_C &quot;get_school_data(\\&quot;C\\&quot;)&quot; ## 4 credits_school_A check_credit_hours(school_A) ## 5 credits_school_B check_credit_hours(school_B) ## 6 credits_school_C check_credit_hours(school_C) ## 7 students_school_A check_students(school_A) ## 8 students_school_B check_students(school_B) ## 9 students_school_C check_students(school_C) ## 10 grads_school_A check_graduations(school_A) ## 11 grads_school_B check_graduations(school_B) ## 12 grads_school_C check_graduations(school_C) ## 13 public_funds_school_A check_public_funding(school_A) ## 14 public_funds_school_B check_public_funding(school_B) Thanks to Alex Axthelm for this use case in issue 235. 6.5.3 Wildcard clusters With evaluate_plan(trace = TRUE), you can generate columns that show how the targets were generated from the wildcards. plan_template &lt;- drake_plan( school = get_school_data(&quot;school__&quot;), credits = check_credit_hours(all_schools__), students = check_students(all_schools__), grads = check_graduations(all_schools__), public_funds = check_public_funding(public_schools__) ) plan &lt;- evaluate_plan( plan = plan_template, rules = list( school__ = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), all_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;), public_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;) ), trace = TRUE ) plan ## # A tibble: 14 x 8 ## target command school__ school___from all_schools__ all_schools___f… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 schoo… &quot;get_s… A school &lt;NA&gt; &lt;NA&gt; ## 2 schoo… &quot;get_s… B school &lt;NA&gt; &lt;NA&gt; ## 3 schoo… &quot;get_s… C school &lt;NA&gt; &lt;NA&gt; ## 4 credi… check_… &lt;NA&gt; &lt;NA&gt; school_A credits ## 5 credi… check_… &lt;NA&gt; &lt;NA&gt; school_B credits ## 6 credi… check_… &lt;NA&gt; &lt;NA&gt; school_C credits ## 7 stude… check_… &lt;NA&gt; &lt;NA&gt; school_A students ## 8 stude… check_… &lt;NA&gt; &lt;NA&gt; school_B students ## 9 stude… check_… &lt;NA&gt; &lt;NA&gt; school_C students ## 10 grads… check_… &lt;NA&gt; &lt;NA&gt; school_A grads ## 11 grads… check_… &lt;NA&gt; &lt;NA&gt; school_B grads ## 12 grads… check_… &lt;NA&gt; &lt;NA&gt; school_C grads ## 13 publi… check_… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 publi… check_… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # ... with 2 more variables: public_schools__ &lt;chr&gt;, ## # public_schools___from &lt;chr&gt; And then when you visualize the dependency graph, you can cluster nodes based on the wildcard info. config &lt;- drake_config(plan) vis_drake_graph( config, group = &quot;all_schools__&quot;, clusters = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;) ) See the visualization guide for more details. 6.5.4 Specialized wildcard functionality In the mtcars example, we will analyze bootstrapped versions of the mtcars dataset to look for an association between the weight and the fuel efficiency of cars. This example uses plan_analyses() and plan_summaries(), two specialized applications of evaluate_plan(). First, we generate the plan for the bootstrapped datasets. my_datasets &lt;- drake_plan( small = simulate(48), large = simulate(64)) my_datasets ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 small simulate(48) ## 2 large simulate(64) We want to analyze each dataset with one of two regression models. methods &lt;- drake_plan( regression1 = reg1(dataset__), regression2 = reg2(dataset__)) methods ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 regression1 reg1(dataset__) ## 2 regression2 reg2(dataset__) We evaluate the dataset__ wildcard to generate all the regression commands we will need. my_analyses &lt;- plan_analyses(methods, datasets = my_datasets) my_analyses ## # A tibble: 4 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 regression1_small reg1(small) ## 2 regression1_large reg1(large) ## 3 regression2_small reg2(small) ## 4 regression2_large reg2(large) Next, we summarize each analysis of each dataset. We calculate descriptive statistics on the residuals, and we collect the regression coefficients and their p-values. summary_types &lt;- drake_plan( summ = suppressWarnings(summary(analysis__$residuals)), coef = suppressWarnings(summary(analysis__))$coefficients ) summary_types ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 summ suppressWarnings(summary(analysis__$residuals)) ## 2 coef suppressWarnings(summary(analysis__))$coefficients results &lt;- plan_summaries(summary_types, analyses = my_analyses, datasets = my_datasets, gather = NULL) # Gathering is suppressed here. results ## # A tibble: 8 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 summ_regression1_sma… suppressWarnings(summary(regression1_small$residu… ## 2 summ_regression1_lar… suppressWarnings(summary(regression1_large$residu… ## 3 summ_regression2_sma… suppressWarnings(summary(regression2_small$residu… ## 4 summ_regression2_lar… suppressWarnings(summary(regression2_large$residu… ## 5 coef_regression1_sma… suppressWarnings(summary(regression1_small))$coef… ## 6 coef_regression1_lar… suppressWarnings(summary(regression1_large))$coef… ## 7 coef_regression2_sma… suppressWarnings(summary(regression2_small))$coef… ## 8 coef_regression2_lar… suppressWarnings(summary(regression2_large))$coef… Next, we bind all the rows together for a single plan that we can later supply to make(). my_plan &lt;- rbind(my_datasets, my_analyses, results) my_plan ## # A tibble: 14 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 small simulate(48) ## 2 large simulate(64) ## 3 regression1_small reg1(small) ## 4 regression1_large reg1(large) ## 5 regression2_small reg2(small) ## 6 regression2_large reg2(large) ## 7 summ_regression1_sm… suppressWarnings(summary(regression1_small$residu… ## 8 summ_regression1_la… suppressWarnings(summary(regression1_large$residu… ## 9 summ_regression2_sm… suppressWarnings(summary(regression2_small$residu… ## 10 summ_regression2_la… suppressWarnings(summary(regression2_large$residu… ## 11 coef_regression1_sm… suppressWarnings(summary(regression1_small))$coef… ## 12 coef_regression1_la… suppressWarnings(summary(regression1_large))$coef… ## 13 coef_regression2_sm… suppressWarnings(summary(regression2_small))$coef… ## 14 coef_regression2_la… suppressWarnings(summary(regression2_large))$coef… 6.5.5 Non-wildcard functions 6.5.5.1 expand_plan() Sometimes, you just want multiple replicates of the same targets. plan &lt;- drake_plan( fake_data = simulate_from_model(), bootstrapped_data = bootstrap_from_real_data(real_data) ) expand_plan(plan, values = 1:3) ## # A tibble: 6 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 fake_data_1 simulate_from_model() ## 2 fake_data_2 simulate_from_model() ## 3 fake_data_3 simulate_from_model() ## 4 bootstrapped_data_1 bootstrap_from_real_data(real_data) ## 5 bootstrapped_data_2 bootstrap_from_real_data(real_data) ## 6 bootstrapped_data_3 bootstrap_from_real_data(real_data) 6.5.5.2 gather_plan() and gather_by() Other times, you want to combine multiple targets into one. plan &lt;- drake_plan( small = data.frame(type = &quot;small&quot;, x = rnorm(25), y = rnorm(25)), large = data.frame(type = &quot;large&quot;, x = rnorm(1000), y = rnorm(1000)) ) gather_plan(plan, target = &quot;combined&quot;) ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 combined list(small = small, large = large) In this case, small and large are data frames, so it may be more convenient to combine the rows together. gather_plan(plan, target = &quot;combined&quot;, gather = &quot;rbind&quot;) ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 combined rbind(small = small, large = large) See also gather_by() to gather multiple groups of targets based on other columns in the plan (e.g. from evaluate_plan(trace = TRUE)). 6.5.5.3 reduce_plan() and reduce_by() reduce_plan() is similar to gather_plan(), but it allows you to combine multiple targets together in pairs. This is useful if combining everything at once requires too much time or computer memory, or if you want to parallelize the aggregation. plan &lt;- drake_plan( a = 1, b = 2, c = 3, d = 4 ) reduce_plan(plan) ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 target_1 a + b ## 2 target_2 c + d ## 3 target target_1 + target_2 You can control how each pair of targets gets combined. reduce_plan(plan, begin = &quot;c(&quot;, op = &quot;, &quot;, end = &quot;)&quot;) ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 target_1 c(a, b) ## 2 target_2 c(c, d) ## 3 target c(target_1, target_2) See also reduce_by() to do reductions on multiple groups of targets based on other columns in the plan (e.g. from evaluate_plan(trace = TRUE)). 6.5.6 Custom metaprogramming The workflow plan is just a data frame. There is nothing magic about it, and you can create it any way you want. With your own custom metaprogramming, you don’t even need the drake_plan() function. The following example could more easily be implemented with map_plan(), but we use other techniques to demonstrate the versatility of custom metaprogramming. Let’s consider a file-based example workflow. Here, our targets execute Linux commands to process input files and create output files. cat in1.txt > out1.txt cat in2.txt > out2.txt The glue package can automatically generate these Linux commands. library(glue) glue_data( list( inputs = c(&quot;in1.txt&quot;, &quot;in2.txt&quot;), outputs = c(&quot;out1.txt&quot;, &quot;out2.txt&quot;) ), &quot;cat {inputs} &gt; {outputs}&quot; ) ## cat in1.txt &gt; out1.txt ## cat in2.txt &gt; out2.txt Our drake commands will use system() to execute the Linux commands that glue generates. Technically, we could use drake_plan() if we wanted. library(tidyverse) drake_plan( glue_data( list( inputs = file_in(c(&quot;in1.txt&quot;, &quot;in2.txt&quot;)), outputs = file_out(c(&quot;out1.txt&quot;, &quot;out2.txt&quot;)) ), &quot;cat {inputs} &gt; {outputs}&quot; ) %&gt;% lapply(FUN = system) ) ## # A tibble: 1 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 drake_target… &quot;glue_data(list(inputs = file_in(c(\\&quot;in1.txt\\&quot;, \\&quot;in2.txt… But what if we want to generate these glue commands instead of writing them literally in our plan? This is a job for custom metaprogramming with tidy evaluation. First, we create a function to generate the drake command of an arbitrary target. library(rlang) # for tidy evaluation write_command &lt;- function(cmd, inputs = NULL , outputs = NULL){ inputs &lt;- enexpr(inputs) outputs &lt;- enexpr(outputs) expr({ glue_data( list( inputs = file_in(!!inputs), outputs = file_out(!!outputs) ), !!cmd ) %&gt;% lapply(FUN = system) }) %&gt;% expr_text } write_command( cmd = &quot;cat {inputs} &gt; {outputs}&quot;, inputs = c(&quot;in1.txt&quot;, &quot;in2.txt&quot;), outputs = c(&quot;out1.txt&quot;, &quot;out2.txt&quot;) ) %&gt;% cat ## { ## glue_data(list(inputs = file_in(c(&quot;in1.txt&quot;, &quot;in2.txt&quot;)), ## outputs = file_out(c(&quot;out1.txt&quot;, &quot;out2.txt&quot;))), &quot;cat {inputs} &gt; {outputs}&quot;) %&gt;% ## lapply(FUN = system) ## } Then, we lay out all the arguments we will pass to write_command(). Here, each row corresponds to a separate target. meta_plan &lt;- tribble( ~cmd, ~inputs, ~outputs, &quot;cat {inputs} &gt; {outputs}&quot;, c(&quot;in1.txt&quot;, &quot;in2.txt&quot;), c(&quot;out1.txt&quot;, &quot;out2.txt&quot;), &quot;cat {inputs} {inputs} &gt; {outputs}&quot;, c(&quot;out1.txt&quot;, &quot;out2.txt&quot;), c(&quot;out3.txt&quot;, &quot;out4.txt&quot;) ) %&gt;% print ## # A tibble: 2 x 3 ## cmd inputs outputs ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 cat {inputs} &gt; {outputs} &lt;chr [2]&gt; &lt;chr [2]&gt; ## 2 cat {inputs} {inputs} &gt; {outputs} &lt;chr [2]&gt; &lt;chr [2]&gt; Finally, we create our workflow plan without any built-in drake functions. plan &lt;- tibble( target = paste0(&quot;target_&quot;, seq_len(nrow(meta_plan))), command = pmap_chr(meta_plan, write_command) ) %&gt;% print ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 target_1 &quot;{\\n glue_data(list(inputs = file_in(c(\\&quot;in1.txt\\&quot;, \\&quot;in2.t… ## 2 target_2 &quot;{\\n glue_data(list(inputs = file_in(c(\\&quot;out1.txt\\&quot;, \\&quot;out2… writeLines(&quot;in1&quot;, &quot;in1.txt&quot;) writeLines(&quot;in2&quot;, &quot;in2.txt&quot;) vis_drake_graph(drake_config(plan)) Alternatively, you could use as.call() instead of tidy evaluation to generate your plan. Use as.call() to construct calls to file_in(), file_out(), and custom functions in your commands. library(purrr) # pmap_chr() is particularly useful here. # A function that will be called in your commands. command_function &lt;- function(cmd, inputs, outputs){ glue_data( list( inputs = inputs, outputs = outputs ), cmd ) %&gt;% purrr::walk(system) } # A function to generate quoted calls to command_function(), # which in turn contain quoted calls to file_in() and file_out(). write_command &lt;- function(...){ args &lt;- list(...) args$inputs &lt;- as.call(list(quote(file_in), args$inputs)) args$outputs &lt;- as.call(list(quote(file_out), args$outputs)) c(quote(command_function), args) %&gt;% as.call() %&gt;% rlang::expr_text() } plan &lt;- tibble( target = paste0(&quot;target_&quot;, seq_len(nrow(meta_plan))), command = pmap_chr(meta_plan, write_command) ) %&gt;% print ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 target_1 &quot;command_function(cmd = \\&quot;cat {inputs} &gt; {outputs}\\&quot;, inputs =… ## 2 target_2 &quot;command_function(cmd = \\&quot;cat {inputs} {inputs} &gt; {outputs}\\&quot;,… Metaprogramming gets much simpler if you do not need to construct literal calls to file_in(), file_out(), etc. in your commands. The construction of model_plan in the gross state product exmaple is an example. Thanks to Chris Hammill for presenting this scenario and contributing to the solution. 6.6 Optional columns in your plan. Besides the usual columns target and command, there are other columns you can add. cpu, elapsed, and timeout: number of seconds to wait for the target to build before timing out (timeout for a general upper bound, cpu for CPU time, and elapsed for elapsed time). priority: for paralllel computing, optionally rank the targets according to priority. That way, when two targets become ready to build at the same time, drake will pick the one with the dominant priority first. retries: number of times to retry building a target in the event of an error. trigger: choose the criterion that drake uses to decide whether to build the target. See ?trigger or read the trigger chapter to learn more. worker: for paralllel computing, optionally name the preferred worker to assign to each target. "],
["organize.html", "Chapter 7 How to organize the files of drake projects 7.1 Examples 7.2 Where do you put your code? 7.3 Your commands are code chunks, not R scripts 7.4 Workflows as R packages", " Chapter 7 How to organize the files of drake projects Unlike most workflow managers, drake focuses on your R session, and it does not care how you organize your files. This flexibility is great in the long run, but it leaves many new users wondering how to structure their projects. This chapter provides guidance, advice, and recommendations on structure and organization. 7.1 Examples For examples of how to structure your code files, see the beginner oriented example projects: mtcars gsp packages Write the code directly with the drake_example() function. drake_example(&quot;mtcars&quot;) drake_example(&quot;gsp&quot;) drake_example(&quot;packages&quot;) In practice, you do not need to organize your files the way the examples do, but it does happen to be a reasonable way of doing things. 7.2 Where do you put your code? It is best to write your code as a bunch of functions. You can save those functions in R scripts and then source() them before doing anything else. # Load functions get_data(), analyze_data, and summarize_results() source(&quot;my_functions.R&quot;) Then, set up your workflow plan data frame. good_plan &lt;- drake_plan( my_data = get_data(file_in(&quot;data.csv&quot;)), # External files need to be in commands explicitly. # nolint my_analysis = analyze_data(my_data), my_summaries = summarize_results(my_data, my_analysis) ) good_plan ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 my_data &quot;get_data(file_in(\\&quot;data.csv\\&quot;))&quot; ## 2 my_analysis analyze_data(my_data) ## 3 my_summaries summarize_results(my_data, my_analysis) drake knows that my_analysis depends on my_data because my_data is an argument to analyze_data(), which is part of the command for my_analysis. config &lt;- drake_config(good_plan) vis_drake_graph(config) Now, you can call make() to build the targets. make(good_plan) If your commands are really long, just put them in larger functions. drake analyzes imported functions for non-file dependencies. 7.3 Your commands are code chunks, not R scripts Some people are accustomed to dividing their work into R scripts and then calling source() to run each step of the analysis. For example you might have the following files. get_data.R analyze_data.R summarize_results.R If you migrate to drake, you may be tempted to set up a workflow plan like this. bad_plan &lt;- drake_plan( my_data = source(file_in(&quot;get_data.R&quot;)), my_analysis = source(file_in(&quot;analyze_data.R&quot;)), my_summaries = source(file_in(&quot;summarize_data.R&quot;)) ) bad_plan ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 my_data &quot;source(file_in(\\&quot;get_data.R\\&quot;))&quot; ## 2 my_analysis &quot;source(file_in(\\&quot;analyze_data.R\\&quot;))&quot; ## 3 my_summaries &quot;source(file_in(\\&quot;summarize_data.R\\&quot;))&quot; But now, the dependency structure of your work is broken. Your R script files are dependencies, but since my_data is not mentioned in a function or command, drake does not know that my_analysis depends on it. ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] TRUE config &lt;- drake_config(bad_plan) vis_drake_graph(config) ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] TRUE ## ## [[3]] ## [1] TRUE Dangers: In the first make(bad_plan, jobs = 2), drake will try to build my_data and my_analysis at the same time even though my_data must finish before my_analysis begins. drake is oblivious to data.csv since it is not explicitly mentioned in a workflow plan command. So when data.csv changes, make(bad_plan) will not rebuild my_data. my_analysis will not update when my_data changes. The return value of source() is formatted counter-intuitively. If source(file_in(&quot;get_data.R&quot;)) is the command for my_data, then my_data will always be a list with elements &quot;value&quot; and &quot;visible&quot;. In other words, source(file_in(&quot;get_data.R&quot;))$value is really what you would want. In addition, this source()-based approach is simply inconvenient. drake rebuilds my_data every time get_data.R changes, even when those changes are just extra comments or blank lines. On the other hand, in the previous plan that uses my_data = get_data(), drake does not trigger rebuilds when comments or whitespace in get_data() are modified. drake is R-focused, not file-focused. If you embrace this viewpoint, your work will be easier. 7.4 Workflows as R packages The R package structure is a great way to organize the files of your project. Writing your own package to contain your data science workflow is a good idea, but you will need to Use expose_imports() to properly account for all your nested function dependencies, and If you load the package with devtools::load_all(), set the prework argument of make(): e.g. make(prework = &quot;devtools::load_all()&quot;). Thanks to Jasper Clarkberg for the workaround behind expose_imports(). 7.4.1 Advantages of putting workflows in R packages The file organization of R packages is a well-understood community standard. If you follow it, your work may be more readable and thus reproducible. R package installation is a standard process. The system makes it easier for others to obtain and run your code. You get development and quality control tools for free: helpers for loading code and creating files, unit testing, package checks, code coverage, and continuous integration. 7.4.2 The problem For drake, there is one problem: nested functions. drake always looks for imported functions nested in other imported functions, but only in your environment. When it sees a function from a package, it does not look in its body for other imports. To see this, consider the digest() function from the digest package. Digest package is a utility for computing hashes, not a data science workflow, but I will use it to demonstrate how drake treats imports from packages. library(digest) g &lt;- function(x){ digest(x) } f &lt;- function(x){ g(x) } plan &lt;- drake_plan(x = f(1)) # Here are the reproducibly tracked objects in the workflow. config &lt;- drake_config(plan) tracked(config) ## [1] &quot;f&quot; &quot;g&quot; &quot;x&quot; # But the `digest()` function has dependencies too. # Because `drake` knows `digest()` is from a package, # it ignores these dependencies by default. head(deps_code(digest), 10) ## $globals ## [1] &quot;match.arg&quot; &quot;stop&quot; ## [3] &quot;warning&quot; &quot;return&quot; ## [5] &quot;invisible&quot; &quot;is.infinite&quot; ## [7] &quot;is.character&quot; &quot;missing&quot; ## [9] &quot;names&quot; &quot;formals&quot; ## [11] &quot;any&quot; &quot;is.na&quot; ## [13] &quot;pmatch&quot; &quot;which&quot; ## [15] &quot;as.raw&quot; &quot;inherits&quot; ## [17] &quot;paste&quot; &quot;switch&quot; ## [19] &quot;path.expand&quot; &quot;file.exists&quot; ## [21] &quot;isTRUE&quot; &quot;file.info&quot; ## [23] &quot;file.access&quot; &quot;.Call&quot; ## [25] &quot;digest_impl&quot; &quot;as.integer&quot; ## [27] &quot;.getCRC32PreferOldOutput&quot; &quot;sub&quot; ## ## $namespaced ## [1] &quot;base::serialize&quot; 7.4.3 The solution To force drake to dive deeper into the nested functions in a package, you must use expose_imports(). Again, I demonstrate with the digest package package, but you should really only do this with a package you write yourself to contain your workflow. For external packages, packrat is a much better solution for package reproducibility. expose_imports(digest) ## &lt;environment: R_GlobalEnv&gt; config &lt;- drake_config(plan) new_objects &lt;- tracked(config) head(new_objects, 10) ## [1] &quot;.getCRC32PreferOldOutput&quot; &quot;base::serialize&quot; ## [3] &quot;digest&quot; &quot;digest_impl&quot; ## [5] &quot;f&quot; &quot;g&quot; ## [7] &quot;x&quot; length(new_objects) ## [1] 7 # Now when you call `make()`, `drake` will dive into `digest` # to import dependencies. cache &lt;- storr::storr_environment() # just for examples make(plan, cache = cache) ## target x head(cached(cache = cache), 10) ## [1] &quot;base::serialize&quot; &quot;digest&quot; &quot;digest_impl&quot; &quot;f&quot; ## [5] &quot;g&quot; &quot;x&quot; length(cached(cache = cache)) ## [1] 6 "],
["vis.html", "Chapter 8 Visualization with drake 8.1 Underlying graph data: node and edge data frames 8.2 Visualizing target status 8.3 Subgraphs 8.4 Control the vis_drake_graph() legend. 8.5 Clusters 8.6 Output files", " Chapter 8 Visualization with drake Data analysis projects have complicated networks of dependencies, and drake can help you visualize them with vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() (note the two g’s). 8.0.1 vis_drake_graph() Powered by visNetwork. Colors represent target status, and shapes represent data type. These graphs are interactive, so you can click, drag, zoom, and and pan to adjust the size and position. Double-click on nodes to contract neighborhoods into clusters or expand them back out again. If you hover over a node, you will see text in a tooltip showing the first few lines of The command of a target, or The body of an imported function, or The content of an imported text file. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). config &lt;- drake_config(my_plan) vis_drake_graph(config) To save this interactive widget for later, just supply the name of an HTML file. vis_drake_graph(config, file = &quot;graph.html&quot;) To save a static image file, supply a file name that ends in &quot;.png&quot;, &quot;.pdf&quot;, &quot;.jpeg&quot;, or &quot;.jpg&quot;. vis_drake_graph(config, file = &quot;graph.png&quot;) 8.0.2 sankey_drake_graph() These interactive networkD3 Sankey diagrams have more nuance: the height of each node is proportional to its number of connections. Nodes with many incoming connnections tend to fall out of date more often, and nodes with many outgoing connections can invalidate bigger chunks of the downstream pipeline. sankey_drake_graph(config) Saving the graphs is the same as before. sankey_drake_graph(config, file = &quot;graph.html&quot;) # Interactive HTML widget sankey_drake_graph(config, file = &quot;graph.png&quot;) # Static image file Unfortunately, a legend is not yet available for Sankey diagrams, but drake exposes a separate legend for the colors and shapes. library(visNetwork) legend_nodes() ## # A tibble: 10 x 6 ## label color shape font.color font.size id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Up to date #228B22 dot black 20 1 ## 2 Outdated #000000 dot black 20 2 ## 3 In progress #FF7221 dot black 20 3 ## 4 Failed #AA0000 dot black 20 4 ## 5 Imported #1874CD dot black 20 5 ## 6 Missing #9A32CD dot black 20 6 ## 7 Object #888888 dot black 20 7 ## 8 Function #888888 triangle black 20 8 ## 9 File #888888 square black 20 9 ## 10 Cluster #888888 diamond black 20 10 visNetwork(nodes = legend_nodes()) 8.0.3 drake_ggraph() Powered by ggraph, these graphs are static ggplot2 objects, and you can save them with ggsave(). plan &lt;- drake_plan(data = get_data(), model = data, plot = data) plan ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data get_data() ## 2 model data ## 3 plot data get_data &lt;- function(){} config &lt;- drake_config(plan) drake_ggraph(config) 8.1 Underlying graph data: node and edge data frames drake_graph_info() is used behind the scenes in vis_drake_graph(), sankey_drake_graph(), and drake_ggraph() to get the graph information ready for rendering. To save time, you can call drake_graph_info() to get these internals and then call render_drake_graph(), render_sankey_drake_graph(), or render_drake_ggraph(). str(drake_graph_info(config)) ## List of 4 ## $ nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 12 variables: ## ..$ id : chr [1:4] &quot;data&quot; &quot;get_data&quot; &quot;model&quot; &quot;plot&quot; ## ..$ deps :List of 4 ## .. ..$ :&lt;environment: 0xe213c68&gt; ## .. ..$ :&lt;environment: 0xfda2620&gt; ## .. ..$ :&lt;environment: 0xe3a1248&gt; ## .. ..$ :&lt;environment: 0xe4938a8&gt; ## ..$ trigger :List of 4 ## .. ..$ :&lt;environment: 0xe5d65a0&gt; ## .. ..$ : NULL ## .. ..$ :&lt;environment: 0xe5d5c00&gt; ## .. ..$ :&lt;environment: 0xe5d5180&gt; ## ..$ label : chr [1:4] &quot;data&quot; &quot;get_data\\n0.003s&quot; &quot;model&quot; &quot;plot&quot; ## ..$ command : chr [1:4] &quot;get_data()&quot; NA &quot;data&quot; &quot;data&quot; ## ..$ status : chr [1:4] &quot;outdated&quot; &quot;imported&quot; &quot;outdated&quot; &quot;outdated&quot; ## ..$ type : chr [1:4] &quot;object&quot; &quot;function&quot; &quot;object&quot; &quot;object&quot; ## ..$ font.size: num [1:4] 20 20 20 20 ## ..$ color : chr [1:4] &quot;#000000&quot; &quot;#1874CD&quot; &quot;#000000&quot; &quot;#000000&quot; ## ..$ shape : chr [1:4] &quot;dot&quot; &quot;triangle&quot; &quot;dot&quot; &quot;dot&quot; ## ..$ level : int [1:4] 2 1 3 3 ## ..$ title : chr [1:4] &quot;get_data()&quot; &quot;function&amp;nbsp;()&amp;nbsp;&lt;br&gt;{&lt;br&gt;}&quot; &quot;data&quot; &quot;data&quot; ## $ edges :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3 obs. of 5 variables: ## ..$ from : chr [1:3] &quot;get_data&quot; &quot;data&quot; &quot;data&quot; ## ..$ to : chr [1:3] &quot;data&quot; &quot;model&quot; &quot;plot&quot; ## ..$ file : int [1:3] 0 0 0 ## ..$ arrows: chr [1:3] &quot;to&quot; &quot;to&quot; &quot;to&quot; ## ..$ smooth: logi [1:3] TRUE TRUE TRUE ## $ legend_nodes :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 6 variables: ## ..$ label : chr [1:4] &quot;Outdated&quot; &quot;Imported&quot; &quot;Object&quot; &quot;Function&quot; ## ..$ color : chr [1:4] &quot;#000000&quot; &quot;#1874CD&quot; &quot;#888888&quot; &quot;#888888&quot; ## ..$ shape : chr [1:4] &quot;dot&quot; &quot;dot&quot; &quot;dot&quot; &quot;triangle&quot; ## ..$ font.color: chr [1:4] &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ## ..$ font.size : num [1:4] 20 20 20 20 ## ..$ id : int [1:4] 2 5 7 8 ## $ default_title: chr &quot;Dependency graph&quot; 8.2 Visualizing target status drake’s visuals tell you which targets are up to date and which are outdated. config &lt;- make(my_plan, jobs = 2, verbose = FALSE) ## Error: package txtq not installed. Please install it with install.packages(&quot;txtq&quot;). outdated(config) ## [1] &quot;data&quot; &quot;model&quot; &quot;plot&quot; sankey_drake_graph(config) When you change a dependency, some targets fall out of date (black nodes). reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } sankey_drake_graph(config) 8.3 Subgraphs Graphs can grow enormous for serious projects, so there are multiple ways to focus on a manageable subgraph. The most brute-force way is to just pick a manual subset of nodes. However, with the subset argument, the graphing functions can drop intermediate nodes and edges. vis_drake_graph( config, subset = c(&quot;regression2_small&quot;, &quot;large&quot;) ) ## Error in `$&lt;-.data.frame`(`*tmp*`, status, value = &quot;imported&quot;): replacement has 1 row, data has 0 The rest of the subgraph functionality preserves connectedness. Use targets_only to ignore the imports. vis_drake_graph(config, targets_only = TRUE) Similarly, you can just show downstream nodes. vis_drake_graph(config, from = c(&quot;regression2_small&quot;, &quot;regression2_large&quot;)) ## Error: All import/target names are invalid in argument &#39;targets&#39;, &#39;from&#39;, or &#39;subset&#39; for make() or similar function. Or upstream ones. vis_drake_graph(config, from = &quot;small&quot;, mode = &quot;in&quot;) ## Error: All import/target names are invalid in argument &#39;targets&#39;, &#39;from&#39;, or &#39;subset&#39; for make() or similar function. In fact, let us just take a small neighborhood around a target in both directions. For the graph below, given order is 1, but all the custom file_out() output files of the neighborhood’s targets appear as well. This ensures consistent behavior between show_output_files = TRUE and show_output_files = FALSE (more on that later). vis_drake_graph(config, from = &quot;small&quot;, mode = &quot;all&quot;, order = 1) ## Error: All import/target names are invalid in argument &#39;targets&#39;, &#39;from&#39;, or &#39;subset&#39; for make() or similar function. 8.4 Control the vis_drake_graph() legend. Some arguments to vis_drake_graph() control the legend. vis_drake_graph(config, full_legend = TRUE, ncol_legend = 2) To remove the legend altogether, set the ncol_legend argument to 0. vis_drake_graph(config, ncol_legend = 0) 8.5 Clusters With the group and clusters arguments to the graphing functions, you can condense nodes into clusters. This is handy for workflows with lots of targets. Take the schools scenario from the workflow plan guide. Our plan was generated with evaluate_plan(trace = TRUE), so it has wildcard columns that group nodes into natural clusters already. You can manually add such columns if you wish. plan_template &lt;- drake_plan( school = get_school_data(&quot;school__&quot;), credits = check_credit_hours(all_schools__), students = check_students(all_schools__), grads = check_graduations(all_schools__), public_funds = check_public_funding(public_schools__) ) plan &lt;- evaluate_plan( plan = plan_template, rules = list( school__ = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), all_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;), public_schools__ = c(&quot;school_A&quot;, &quot;school_B&quot;) ), trace = TRUE ) plan ## # A tibble: 14 x 8 ## target command school__ school___from all_schools__ all_schools___f… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 schoo… &quot;get_s… A school &lt;NA&gt; &lt;NA&gt; ## 2 schoo… &quot;get_s… B school &lt;NA&gt; &lt;NA&gt; ## 3 schoo… &quot;get_s… C school &lt;NA&gt; &lt;NA&gt; ## 4 credi… check_… &lt;NA&gt; &lt;NA&gt; school_A credits ## 5 credi… check_… &lt;NA&gt; &lt;NA&gt; school_B credits ## 6 credi… check_… &lt;NA&gt; &lt;NA&gt; school_C credits ## 7 stude… check_… &lt;NA&gt; &lt;NA&gt; school_A students ## 8 stude… check_… &lt;NA&gt; &lt;NA&gt; school_B students ## 9 stude… check_… &lt;NA&gt; &lt;NA&gt; school_C students ## 10 grads… check_… &lt;NA&gt; &lt;NA&gt; school_A grads ## 11 grads… check_… &lt;NA&gt; &lt;NA&gt; school_B grads ## 12 grads… check_… &lt;NA&gt; &lt;NA&gt; school_C grads ## 13 publi… check_… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 14 publi… check_… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # ... with 2 more variables: public_schools__ &lt;chr&gt;, ## # public_schools___from &lt;chr&gt; Ordinarily, the workflow graph gives a separate node to each individual import object or target. config &lt;- drake_config(plan) vis_drake_graph(config) For large projects with hundreds of nodes, this can get quite cumbersome. But here, we can choose a wildcard column (or any other column in the plan, even custom columns) to condense nodes into natural clusters. For the group argument to the graphing functions, choose the name of a column in plan or a column you know will be in drake_graph_info(config)$nodes. Then for clusters, choose the values in your group column that correspond to nodes you want to bunch together. The new graph is not as cumbersome. config &lt;- drake_config(plan) vis_drake_graph( config, group = &quot;all_schools__&quot;, clusters = c(&quot;school_A&quot;, &quot;school_B&quot;, &quot;school_C&quot;) ) As I mentioned, you can group on any column in drake_graph_info(config)$nodes. Let’s return to the mtcars project for demonstration. config &lt;- drake_config(my_plan) vis_drake_graph(config) Let’s condense all the imports into one node and all the up-to-date targets into another. That way, the outdated targets stand out. vis_drake_graph( config, group = &quot;status&quot;, clusters = c(&quot;imported&quot;, &quot;up to date&quot;) ) 8.6 Output files drake can reproducibly track multiple output files per target and show them in the graph. plan &lt;- drake_plan( target1 = { file.copy(file_in(&quot;in1.txt&quot;), file_out(&quot;out1.txt&quot;)) file.copy(file_in(&quot;in2.txt&quot;), file_out(&quot;out2.txt&quot;)) }, target2 = { file.copy(file_in(&quot;out1.txt&quot;), file_out(&quot;out3.txt&quot;)) file.copy(file_in(&quot;out2.txt&quot;), file_out(&quot;out4.txt&quot;)) } ) writeLines(&quot;in1&quot;, &quot;in1.txt&quot;) writeLines(&quot;in2&quot;, &quot;in2.txt&quot;) config &lt;- make(plan) ## target target1 ## target target2 writeLines(&quot;abcdefg&quot;, &quot;out3.txt&quot;) vis_drake_graph(config, targets_only = TRUE) If your graph is too busy, you can hide the output files with show_output_files = FALSE. drake preserves dependency relationships induced by files, so there is now an edge from target1 to target2. vis_drake_graph(config, show_output_files = FALSE, targets_only = TRUE) If target2 depends on the value of target1 in addition to its output files, the graph adds another edge. plan &lt;- drake_plan( target1 = { file.copy(file_in(&quot;in1.txt&quot;), file_out(&quot;out1.txt&quot;)) file.copy(file_in(&quot;in2.txt&quot;), file_out(&quot;out2.txt&quot;)) rnorm(5) }, target2 = { file.copy(file_in(&quot;out1.txt&quot;), file_out(&quot;out3.txt&quot;)) file.copy(file_in(&quot;out2.txt&quot;), file_out(&quot;out4.txt&quot;)) target1 + 5 } ) config &lt;- drake_config(plan) vis_drake_graph(config, targets_only = TRUE) "],
["debug.html", "Chapter 9 Debugging and testing drake projects 9.1 Dependencies 9.2 Diagnose failures. 9.3 Timeouts and retries 9.4 More help", " Chapter 9 Debugging and testing drake projects This chapter is a guide to debugging and testing drake projects. 9.1 Dependencies drake automatically detects dependency relationships among your targets and imports. While this is convenient most of the time, it can lead to some pitfalls. This section describes techniques to understand you project’s dependency structure and diagnose and debug issues. 9.1.1 Visualize your dependency graph. To avoid frustration early on, please use drake’s dependency graph visualizations to see how the steps of your workflow fit together. drake resolves the dependency relationships in the graph by analyzing the code in your commands and the functions in your environment. load_mtcars_example() config &lt;- drake_config(my_plan) # Hover, click, drag, zoom, and pan. See args &#39;from&#39; and &#39;to&#39;. vis_drake_graph(config, width = &quot;100%&quot;, height = &quot;500px&quot;) 9.1.2 Check specific dependency information. With the deps_code() function, you can see for yourself how drake detects first-order dependencies from code. print(simulate) ## function (n) ## { ## data &lt;- random_rows(data = mtcars, n = n) ## data.frame(x = data$wt, y = data$mpg) ## } deps_code(simulate) ## $globals ## [1] &quot;random_rows&quot; &quot;mtcars&quot; &quot;data.frame&quot; # knitr_in() makes sure your target depends on `report.Rmd` # and any dependencies loaded with loadd() and readd() # in the report&#39;s active code chunks. cat(my_plan$command[1]) ## knit(knitr_in(&quot;report.Rmd&quot;), file_out(&quot;report.md&quot;), quiet = TRUE) deps_code(my_plan$command[1]) ## $globals ## [1] &quot;knit&quot; ## ## $loadd ## [1] &quot;large&quot; ## ## $readd ## [1] &quot;small&quot; &quot;coef_regression2_small&quot; ## ## $knitr_in ## [1] &quot;\\&quot;report.Rmd\\&quot;&quot; ## ## $file_out ## [1] &quot;\\&quot;report.md\\&quot;&quot; cat(my_plan$command[nrow(my_plan)]) ## suppressWarnings(summary(regression2_large))$coefficients deps_code(my_plan$command[nrow(my_plan)]) ## $globals ## [1] &quot;suppressWarnings&quot; &quot;summary&quot; &quot;regression2_large&quot; With deps_target(), you can see the dependencies that drake has already detected for your targets and imports. deps_target(&quot;simulate&quot;, config) ## $globals ## [1] &quot;random_rows&quot; deps_target(&quot;small&quot;, config) ## $globals ## [1] &quot;simulate&quot; deps_target(&quot;report&quot;, config) ## $knitr_in ## [1] &quot;\\&quot;report.Rmd\\&quot;&quot; ## ## $file_out ## [1] &quot;\\&quot;report.md\\&quot;&quot; ## ## $readd ## [1] &quot;small&quot; &quot;coef_regression2_small&quot; ## ## $loadd ## [1] &quot;large&quot; And with tracked(), you can list all the reproducibly tracked objects and files. tracked(config) ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [7] &quot;large&quot; &quot;random_rows&quot; ## [9] &quot;reg1&quot; &quot;reg2&quot; ## [11] &quot;regression1_large&quot; &quot;regression1_small&quot; ## [13] &quot;regression2_large&quot; &quot;regression2_small&quot; ## [15] &quot;report&quot; &quot;simulate&quot; ## [17] &quot;small&quot; &quot;summ_regression1_large&quot; ## [19] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [21] &quot;summ_regression2_small&quot; 9.1.3 Outdated targets and missing dependencies missed() shows any imports missing from your environment missed(config) # Nothing is missing right now. ## character(0) outdated() reports any targets that are outdated. outdated(config) ## [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [5] &quot;large&quot; &quot;regression1_large&quot; ## [7] &quot;regression1_small&quot; &quot;regression2_large&quot; ## [9] &quot;regression2_small&quot; &quot;report&quot; ## [11] &quot;small&quot; &quot;summ_regression1_large&quot; ## [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [15] &quot;summ_regression2_small&quot; make(my_plan) ## target large ## target small ## target regression1_large ## target regression2_large ## target regression1_small ## target regression2_small ## target summ_regression1_large ## target coef_regression1_large ## target summ_regression2_large ## target coef_regression2_large ## target summ_regression1_small ## target coef_regression1_small ## target coef_regression2_small ## target summ_regression2_small ## target report outdated(config) ## character(0) 9.1.4 But why are my targets out of date? drake has the option to produce a cache log with the fingerprint of every target and import. head(drake_cache_log()) ## # A tibble: 6 x 3 ## hash type name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 c7e850cdf32cafe0 target &quot;\\&quot;report.md\\&quot;&quot; ## 2 a2e1a7be9a4b1a24 import &quot;\\&quot;report.Rmd\\&quot;&quot; ## 3 4e6e4d4c0bcda263 target coef_regression1_large ## 4 5ad25785a84e0cac target coef_regression1_small ## 5 b2662785d55b28c1 target coef_regression2_large ## 6 23c66b36be56905b target coef_regression2_small We highly recommend that you automatically produce a cache log file on every make() and put it under version control with the rest of your project. make(my_plan, cache_log_file = &quot;cache_log.txt&quot;) ## All targets are already up to date. read.table(&quot;cache_log.txt&quot;, nrows = 6, header = TRUE) ## hash type name ## 1 c7e850cdf32cafe0 target report.md ## 2 a2e1a7be9a4b1a24 import report.Rmd ## 3 4e6e4d4c0bcda263 target coef_regression1_large ## 4 5ad25785a84e0cac target coef_regression1_small ## 5 b2662785d55b28c1 target coef_regression2_large ## 6 23c66b36be56905b target coef_regression2_small Suppose we go back and add input checking to one of our functions. print(random_rows) ## function (data, n) ## { ## data[sample.int(n = nrow(data), size = n, replace = TRUE), ## ] ## } ## &lt;bytecode: 0xf9184a8&gt; random_rows &lt;- function(data, n){ stopifnot(n &gt; 0) data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } Then, we forget to run make() again, and we leave the the project for several months. When we come back, all our targets are suddenly out of date. outdated(config) ## [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [5] &quot;large&quot; &quot;regression1_large&quot; ## [7] &quot;regression1_small&quot; &quot;regression2_large&quot; ## [9] &quot;regression2_small&quot; &quot;report&quot; ## [11] &quot;small&quot; &quot;summ_regression1_large&quot; ## [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [15] &quot;summ_regression2_small&quot; At first, we may not know why all our targets are outdated. But we can generate another cache log and check any hashes that changed. Our call to outdated() already re-cached the imports, so any changed imports will show up in the new log file. drake_cache_log_file(file = &quot;cache_log2.txt&quot;) system2(&quot;diff&quot;, &quot;cache_log.txt cache_log2.txt&quot;, stdout = TRUE) %&gt;% cat(sep = &quot;\\n&quot;) ## Warning in system2(&quot;diff&quot;, &quot;cache_log.txt cache_log2.txt&quot;, stdout = TRUE): ## running command &#39;&#39;diff&#39; cache_log.txt cache_log2.txt&#39; had status 1 ## 9c9 ## &lt; 44b898f3c687ee02 import random_rows ## --- ## &gt; a2ed0494ada81cff import random_rows ## 17c17 ## &lt; 4acbf8bf5b7452c9 import simulate ## --- ## &gt; 0f44eb1457517517 import simulate Now, we see that random_rows() has changed since last time, and we have a new dependency stopifnot(). simulate() shows up in the changes too, but only because random_rows() is nested in the body of simulate(). If we revert random_rows() to its original state, all our targets are up to date again. random_rows &lt;- function(data, n){ data[sample.int(n = nrow(data), size = n, replace = TRUE), ] } outdated(config) ## character(0) drake_cache_log_file(file = &quot;cache_log3.txt&quot;) system2(&quot;diff&quot;, &quot;cache_log.txt cache_log3.txt&quot;, stdout = TRUE) ## character(0) 9.2 Diagnose failures. drake records diagnostic metadata on all your targets, including the latest errors, warnings, messages, and other bits of context. f &lt;- function(x){ if (x &lt; 0){ stop(&quot;`x` cannot be negative.&quot;) } x } bad_plan &lt;- drake_plan( a = 12, b = -a, my_target = f(b) ) bad_plan ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 a 12 ## 2 b -a ## 3 my_target f(b) withr::with_message_sink( new = stdout(), make(bad_plan) ) ## target a ## target b ## target my_target ## fail my_target ## Error: Target `my_target` failed. Call `diagnose(my_target)` for details. Error message: ## `x` cannot be negative. ## Warning: No message sink to remove. failed(verbose = FALSE) # from the last make() only ## [1] &quot;my_target&quot; # See also warnings and messages. error &lt;- diagnose(my_target, verbose = FALSE)$error error$message ## [1] &quot;`x` cannot be negative.&quot; error$call ## f(b) str(error$calls) # View the traceback. ## List of 8 ## $ : language local({ f(b) ... ## $ : language eval.parent(substitute(eval(quote(expr), envir))) ## $ : language eval(expr, p) ## $ : language eval(expr, p) ## $ : language eval(quote({ f(b) ... ## $ : language eval(quote({ f(b) ... ## $ : language f(b) ## $ : language stop(&quot;`x` cannot be negative.&quot;) ## ..- attr(*, &quot;srcref&quot;)= &#39;srcref&#39; int [1:8] 3 5 3 35 5 35 3 3 ## .. ..- attr(*, &quot;srcfile&quot;)=Classes &#39;srcfilecopy&#39;, &#39;srcfile&#39; &lt;environment: 0x14e92d68&gt; To figure out what went wrong, you could try to build the failed target interactively. To do that, simply call drake_build() or drake_debug(). These functions first call loadd(deps = TRUE) to load any missing dependencies (see the replace argument here) and then build your target. drake_build() simply runs the command, and drake_debug() runs the command in debug mode using debugonce(). # Pretend we just opened a new R session. library(drake) # Unloads target `b`. config &lt;- drake_config(plan = bad_plan) ## Unloading targets from environment: ## b # my_target depends on b. &quot;b&quot; %in% ls() ## [1] FALSE # Try to build my_target until the error is fixed. # Skip all that pesky work checking dependencies. drake_build(my_target, config = config) # See also drake_debug(). ## target my_target ## fail my_target ## Error: Target `my_target` failed. Call `diagnose(my_target)` for details. Error message: ## `x` cannot be negative. # The target failed, but the dependency was loaded. &quot;b&quot; %in% ls() ## [1] TRUE # What was `b` again? b ## [1] -12 # How was `b` used? diagnose(my_target)$message ## NULL diagnose(my_target)$call ## NULL f ## function(x){ ## if (x &lt; 0){ ## stop(&quot;`x` cannot be negative.&quot;) ## } ## x ## } # Aha! The error was in f(). Let&#39;s fix it and try again. f &lt;- function(x){ x &lt;- abs(x) if (x &lt; 0){ stop(&quot;`x` cannot be negative.&quot;) } x } # Now it works! # Since you called make() previously, `config` is read from the cache # if you do not supply it. drake_build(my_target) # See also drake_debug(). ## target my_target readd(my_target) ## [1] 12 9.3 Timeouts and retries See the timeout, cpu, elapsed, and retries argument to make(). clean(verbose = FALSE) f &lt;- function(...){ Sys.sleep(1) } debug_plan &lt;- drake_plan(x = 1, y = f(x)) debug_plan ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 x 1 ## 2 y f(x) withr::with_message_sink( stdout(), make(debug_plan, timeout = 1e-3, retries = 2) ) ## target x ## target y ## retry y: 1 of 2 ## retry y: 2 of 2 ## fail y ## Error: Target `y` failed. Call `diagnose(y)` for details. Error message: ## reached elapsed time limit ## Warning: No message sink to remove. To tailor these settings to each individual target, create new timeout, cpu, elapsed, or retries columns in your workflow plan. These columns override the analogous arguments to make(). clean(verbose = FALSE) debug_plan$timeout &lt;- c(1e-3, 2e-3) debug_plan$retries &lt;- 1:2 debug_plan ## # A tibble: 2 x 4 ## target command timeout retries ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 x 1 0.001 1 ## 2 y f(x) 0.002 2 withr::with_message_sink( new = stdout(), make(debug_plan, timeout = Inf, retries = 0) ) ## Unloading targets from environment: ## x ## target x ## target y ## [2018-10-15 11:38:03] TimeoutException: reached CPU time limit [cpu=0.002s, ## elapsed=0.002s] ## Warning: No message sink to remove. 9.4 More help Please also see the compendium of cautionary notes, which addresses drake’s known edge cases, pitfalls, and weaknesses that may or may not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the issue tracker, where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version. "],
["hpc.html", "Chapter 10 High-performance computing with drake 10.1 Batch mode for long workflows 10.2 Let drake schedule your targets. 10.3 Parallel backends 10.4 Local workers 10.5 Remote workers 10.6 Scheduling algorithms 10.7 Final thoughts 10.8 Footnotes", " Chapter 10 High-performance computing with drake drake is not only a reproducibility tool, but also a high-performance computing engine. To activate parallel computing, just set the jobs argument of make() to a value greater than 1. Below, up to 2 targets can run simultaneously at any given time. library(drake) load_mtcars_example() make(my_plan, jobs = 2) 10.1 Batch mode for long workflows To deploy serious long workflows, we recommend putting the call to make() in a script (say, drake_work.R) and running it in an unobtrusive background process that persists after you log out. In the Linux command line, this is straightforward. nohup nice -19 R CMD BATCH drake_work.R & Or, you could call drake inside an overarching Makefile that chains multiple stages together in a larger reproducible pipeline. (See Karl Broman’s post on Makefiles for reproducible research.) all: final_output.pdf final_output.pdf: python_magic.py results_summary.csv python python_magic.py results_summary.csv: drake_work.R Rscript drake_work.R clean: rm -rf .drake Then, run your whole pipleine in a persistent background process. nohup nice -19 R CMD BATCH make & If you do write a custom Makefile at the root of your project and you plan to use make(parallelism = &quot;Makefile&quot;), please read about make(parallelism = &quot;Makefile&quot;) later in this document to avoid potential conflicts between your Makefile and the one drake writes. 10.2 Let drake schedule your targets. When you deploy your project, drake uses the dependency network to figure out how to run your work in parallel. You as the user do not have to micromanage when individual targets are built. load_mtcars_example() config &lt;- drake_config(my_plan) vis_drake_graph(config) 10.3 Parallel backends There are multiple ways to walk this graph and multiple ways to launch workers, and every project has its own needs. Thus, drake supports multiple parallel backends. Choose the backend with the parallelism argument. make(my_plan, parallelism = &quot;parLapply&quot;, jobs = 2) You can use a different backend for the imports than you select for the targets. If you do so, you force all the imports to be processed before any of the targets are built, but you might want to do so anyway. For example, staged scheduling could be great for imports even when it is not be the right choice for the targets (more on that later). make( my_plan, parallelism = c(imports = &quot;mclapply_staged&quot;, targets = &quot;mclapply&quot;), jobs = 2 ) List your options with parallelism_choices(). parallelism_choices() ## [1] &quot;mclapply&quot; &quot;parLapply&quot; &quot;mclapply_staged&quot; ## [4] &quot;parLapply_staged&quot; &quot;clustermq&quot; &quot;clustermq_staged&quot; ## [7] &quot;future&quot; &quot;future_lapply&quot; &quot;future_lapply_staged&quot; ## [10] &quot;Makefile&quot; The backends vary widely in terms of how the workers deploy and how they are scheduled. Deploy: local Deploy: remote Schedule: persistent “mclapply”, “parLapply” “clustermq”, “future_lapply” Schedule: transient “future”, “Makefile” Schedule: staged “mclapply_staged”, “parLapply_staged” “clustermq_staged”, “future_lapply_staged” The next sections describe how and when to use each scheduling algorithm and deployment strategy. 10.4 Local workers Local workers deploy as separate forks or processes to you computer. The &quot;mclapply&quot; and &quot;mclapply_staged&quot; backends uses the mclapply() function from the parallel package to launch workers. make(my_plan, parallelism = &quot;mclapply&quot;, jobs = 2) make(my_plan, parallelism = &quot;mclapply_staged&quot;, jobs = 2) Workers are quicker to launch than in any other drake backend, so these two choices are the lowest-overhead options. However, they have limitations: the mclapply() function is inefficient with respect to computer memory (see explanations here and here) and it cannot launch multiple workers on Windows. For this reason, drake supports platform agnostic backends &quot;parLapply&quot; and &quot;parLapply_staged&quot;, both of which are based on the parLapply() function from the parallel package. These options work on Windows, but each make() requires extra overhead to create a parallel socket (PSOCK) cluster. make(my_plan, parallelism = &quot;parLapply&quot;, jobs = 2) make(my_plan, parallelism = &quot;parLapply_staged&quot;, jobs = 2) The default parallelism is &quot;parLapply&quot; on Windows and &quot;mclapply&quot; everywhere else. default_parallelism() ## [1] &quot;mclapply&quot; 10.5 Remote workers The &quot;future_lapply&quot;, &quot;future&quot;, and &quot;Makefile&quot; backends have the option to launch workers to remote resources such as nodes on a computing cluster. parallelism_choices(distributed_only = TRUE) ## [1] &quot;clustermq&quot; &quot;clustermq_staged&quot; &quot;future&quot; ## [4] &quot;future_lapply&quot; &quot;future_lapply_staged&quot; &quot;Makefile&quot; Testing them out is straightforward. make(my_plan, parallelism = &quot;clustermq&quot;, jobs = 2) make(my_plan, parallelism = &quot;clustermq_staged&quot;, jobs = 2) make(my_plan, parallelism = &quot;future&quot;, jobs = 2) make(my_plan, parallelism = &quot;future_lapply&quot;, jobs = 2) make(my_plan, parallelism = &quot;future_lapply_staged&quot;, jobs = 2) make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 2) For remote workers, the all the imports are processed with one of the local worker backends before any of the targets start. You can use different numbers of workers for the imports and the targets. make(my_plan, parallelism = &quot;clustermq&quot;, jobs = c(imports = 2, targets = 4)) By default, these backends launch the workers on your local machine. It takes extra configuring to actually deploy them to a remote cluster. The next subsections have the details. 10.5.1 &quot;clustermq&quot; and &quot;clustermq_staged&quot; The clustermq R package is a fast, user-friendly tool for sending R jobs to clusters, and clustermq-powered drake workers suffer less overhead than other remote workers. To use drake’s clustermq backends, you must first install ZeroMQ (installation instructions here). Then, install clustermq explicitly (it does not come with drake). install.packages(&quot;clustermq&quot;) # CRAN release # Alternatively, install the GitHub development version. devtools::install_github(&quot;mschubert/clustermq&quot;, ref = &quot;develop&quot;) # &quot;develop&quot; is the name of a git branch. clustermq detects most clusters automatically. However, you may still want to configure resources, wall time limits, etc. If so, you will need a template file. Use drake_hpc_template_file() to write one of the available example template files for clustermq. (list with drake_hpc_template_files()). You will probably need to edit your tempalte file manually to match your resources and needs. drake_hpc_tempalte_file(&quot;slurm_clustermq.tmpl&quot;) # Write the file slurm_clustermq.tmpl. Next, configure clustermq to use your scheduler and template file. clustermq has a really nice wiki with more detailed directions. options(clustermq.scheduler = &quot;slurm&quot;, template = &quot;slurm_clustermq.tmpl&quot;) Tip: for a dry run on your local machine, use options(clustermq.scheduler = &quot;multicore&quot;) instead. Either way, once it is time to run your project, simply call make() with one of the clustermq parallelism backends. make(my_plan, parallelism = &quot;clustermq&quot;, jobs = 4) make(my_plan, parallelism = &quot;clustermq_staged&quot;, jobs = 4) # Different job scheduling algorithm. 10.5.1.1 Control how I/O is scheduled With the caching argument to make(), you have the option to either let the workers cache the targets. You can choose either caching = &quot;worker&quot; or caching = &quot;master&quot;. In the former case, the workers cache the targets directly to the file system in parallel. Target return values are transferred to the central cache over a network file system, not ZeroMQ sockets. In the latter case, target values are sent to the master process via fast ZeroMQ sockets, but then the master has to cache all the data by itself, which could create a bottleneck. caching = &quot;master&quot; also allows for alternative cache types such as storr::storr_environment() and storr::storr_dbi() caches. See the storr package documentation for details. 10.5.1.2 The template argument of make() For more control and flexibility in the clustermq-based backends, you can parameterize your template file and use the template argument of make() configure resource requirements. For example, suppose you want to programatically set the number of “slots” (basically cores) per job on an SGE system (clustermq guide to SGE setup here). We begin with a parameterized template file sge_clustermq.tmpl with a custom n_slots placeholder. # File: sge_clustermq.tmpl # Modified from https://github.com/mschubert/clustermq/wiki/SGE #$ -N {{ job_name }} # job name #$ -t 1-{{ n_jobs }} # submit jobs as array #$ -j y # combine stdout/error in one file #$ -o {{ log_file | /dev/null }} # output file #$ -cwd # use pwd as work dir #$ -V # use environment variable #$ -pe smp {{ n_slots | 1 }} # request n_slots cores per job module load R ulimit -v $(( 1024 * {{ memory | 4096 }} )) R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ master }}&quot;)&#39; Then when you run make(), use the template argument to set n_slots. options(clustermq.scheduler = &quot;sge&quot;, clustermq.template = &quot;sge_clustermq.tmpl&quot;) library(drake) load_mtcars_example() # Or set up your own workflow. make( my_plan, parallelism = &quot;clustermq&quot;, jobs = 16 # Number of jobs / nodes. template = list(n_slots = 4) # Request 4 cores per job. ) Custom placeholders like n_slots are processed with the infuser package. 10.5.2 &quot;future&quot;, &quot;future_lapply&quot;, and &quot;future_lapply_staged&quot; The plan() function from the future package configures how and where the workers will deploy on the next make(). For example, the following code uses future’s multisession backend, which is analogous to drake’s &quot;parLapply&quot; parallelism. library(future) future::plan(multisession) make(my_plan, parallelism = &quot;future&quot;, jobs = 2) # Same technology, different scheduling: make(my_plan, parallelism = &quot;future_lapply&quot;, jobs = 2) In the case of parallelism = &quot;future&quot;, the caching argument can be either &quot;worker&quot; or &quot;master&quot;, and it works the same as for &quot;clustermq&quot; parallelism. For &quot;future_lapply_staged&quot; parallelism, the jobs argument is ignored, and you must instead specify the number of workers in future::plan(). future::plan(multisession, workers = 2) make(my_plan, parallelism = &quot;future_lapply_staged&quot;) To deploy to a cluster (say, a SLURM cluster), you need the batchtools and future.batchtools packages. library(future.batchtools) You also need template file to configure batchtools with the cluster: memory specifications, wall time limits etc. Use drake_hpc_template_file() to write one of the available example template files for batchtools (list with drake_hpc_template_files()). You will probably need to edit your tempalte file manually to match your resources and needs. drake_hpc_tempalte_file(&quot;slurm_batchtools.tmpl&quot;) # Write the file slurm_batchtools.tmpl. Load the template file your future::plan() and call make() to run the project. future::plan(batchtools_slurm, template = &quot;slurm_batchtools.tmpl&quot;) make(my_plan, parallelism = &quot;future&quot;, jobs = 2) # Same technology, different scheduling options: make(my_plan, parallelism = &quot;future_lapply&quot;, jobs = 2) And as before, &quot;future_lapply_staged&quot; uses the workers argument from future::plan() rather than jobs in make(). future::plan(batchtools_slurm, template = &quot;slurm_batchtools.tmpl&quot;, workers = 2) make(my_plan, parallelism = &quot;future_lapply_staged&quot;) See packages future, future.batchtools, and batchtools for more options. For example, the alternatives for future::plan() are listed here and here. 10.5.3 &quot;Makefile&quot; Here, drake actually writes, configures, and runs a proper Makefile to run the targets. make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 2) You can configure both the Unix command that runs the Makefile and the command line arguments passed to it. make( my_plan, parallelism = &quot;Makefile&quot;, command = &quot;lsmake&quot;, args = c(&quot;--touch&quot;, &quot;--silent&quot;) ) If drake’s Makefile conflicts with a Makefile you already wrote yourself, drake does not overwrite your Makefile. Instead, make() tells you about the conflict and then stops running. To force drake to use a different Makefile that does not conflict with yours, pass the file path to the makefile_path argument and set the --file argument in args. make( my_plan, parallelism = &quot;Makefile&quot;, makefile_path = &quot;my_folder/my_makefile&quot;, args = &quot;--file=my_folder/my_makefile&quot; ) There are more customization options in make(), such as the recipe_command argument. make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 4, recipe_command = &quot;R -e &#39;R_RECIPE&#39; -q&quot;) See the help files of individual functions for details. default_Makefile_command() ## [1] &quot;make&quot; default_recipe_command() ## [1] &quot;Rscript -e &#39;R_RECIPE&#39;&quot; r_recipe_wildcard() ## [1] &quot;R_RECIPE&quot; Makefile_recipe( recipe_command = &quot;R -e &#39;R_RECIPE&#39; -q&quot;, target = &quot;this_target&quot;, cache_path = &quot;custom_cache&quot; ) ## R -e &#39;drake::mk(target = &quot;this_target&quot;, cache_path = &quot;custom_cache&quot;)&#39; -q To deploy workers to a cluster, you need to supply the Makefile with a custom shell script that launches cluster jobs. Use the shell_file() function to write an example compatible with the Univa Grid Engine. You will probably need to configure it manually. Suppose our file is shell.sh. #!/bin/bash shift echo \"module load R; $*\" | qsub -sync y -cwd -j y You will need to set permissions to allow execution. In the Linux command line, this is straightforward. $ chmod +x shell.sh When you actually call make(), use the prepend argument to write a line at the top of the Makefile to reference your shell file. make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 2, prepend = &quot;SHELL=./shell.sh&quot;) SLURM users may be able to invoke srun and dispense with shell.sh altogether, though success may vary depending on the SLURM system. You will probably also need to set resource allocation parameters governing memory, runtime, etc. See man srun for the possible .SHELLFLAGS. make( my_plan, parallelism = &quot;Makefile&quot;, jobs = 2, prepend = c( &quot;SHELL=srun&quot;, &quot;.SHELLFLAGS=-N1 -n1 bash -c&quot; ) ) 10.6 Scheduling algorithms 10.6.1 Persistent scheduling Backends “mclapply”, “parLapply”, and “future_lapply” launch persistent workers. make(my_plan, parallelism = &quot;mclapply&quot;, jobs = 2) make(my_plan, parallelism = &quot;parLapply&quot;, jobs = 2) future::plan(future::multisession) make(my_plan, parallelism = &quot;future_lapply&quot;, jobs = 2) In each of these calls to make(), three processes launch: two workers and one master. Whenever a worker is idle, the master assigns it the next available target (whose dependencies have been built). The workers keep running until there are no more targets to build. The following video demonstrates the concept. For staged scheduling, you can micromanage which workers can run which targets. This column can be an integer vector or a list of integer vectors. Simply set an optional workers column in your drake_plan(). Why would you wish to do this? Consider the mtcars example. load_mtcars_example() my_plan$workers &lt;- 1 my_plan$workers[grepl(&quot;large&quot;, my_plan$target)] &lt;- 2 my_plan ## # A tibble: 15 x 3 ## target command workers ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 report &quot;knit(knitr_in(\\&quot;report.Rmd\\&quot;), file_out(\\&quot;re… 1 ## 2 small simulate(48) 1 ## 3 large simulate(64) 2 ## 4 regression1_sma… reg1(small) 1 ## 5 regression1_lar… reg1(large) 2 ## 6 regression2_sma… reg2(small) 1 ## 7 regression2_lar… reg2(large) 2 ## 8 summ_regression… suppressWarnings(summary(regression1_small$re… 1 ## 9 summ_regression… suppressWarnings(summary(regression1_large$re… 2 ## 10 summ_regression… suppressWarnings(summary(regression2_small$re… 1 ## 11 summ_regression… suppressWarnings(summary(regression2_large$re… 2 ## 12 coef_regression… suppressWarnings(summary(regression1_small))$… 1 ## 13 coef_regression… suppressWarnings(summary(regression1_large))$… 2 ## 14 coef_regression… suppressWarnings(summary(regression2_small))$… 1 ## 15 coef_regression… suppressWarnings(summary(regression2_large))$… 2 Here, one of the workers is in charge of all the targets that have to do with the large dataset. That way, we do not need other workers to read large from disk. If reads from disk take a long time, this could speed up your workflow. On the other hand, delegating all the large targets to worker 2 could prevent worker 1 from sharing the computational load, which could slow things down. Ultimately, you as the user need to make these tradeoffs. Also, the workers column only applies to the persistent scheduling backends. Similarly, you can set an optional priority column for your drake_plan(). plan &lt;- drake_plan(A = build(), B = stuff()) plan$priority &lt;- c(1, 2) plan ## # A tibble: 2 x 3 ## target command priority ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A build() 1 ## 2 B stuff() 2 Because of the priority column, if targets A and B are both ready to build, then A will be assigned to a worker first. Custom priorities apply to the staged scheduling backends, plus the &quot;future&quot; backend. The predict_runtime() and predict_load_balancing() functions emulate persistent workers, and the predictions also apply to transient workers. See the timing guide for a demonstration. These functions also respond to the workers column. 10.6.2 Transient scheduling Persistent workers are great because they minimize overhead: all the workers are created at the beginning, and then you never have to create any more for the rest of the runthrough. Unfortunately, computing clusters usually limit the amount of time each worker can stay running. That is why drake also supports transient workers in backends &quot;future&quot; and &quot;Makefile&quot;. Here, the master process creates a new worker for each target individually, and the worker dies after it finishes its single target. For the &quot;future&quot; backend, the master is just the existing process calling make(). The following video demonstrates the concept. future::plan(future::multisession) make(my_plan, parallelism = &quot;future&quot;, jobs = 2) make(my_plan, parallelism = &quot;Makefile&quot;, jobs = 2) 10.6.3 Staged scheduling Backends &quot;mclapply_staged&quot; and &quot;parLapply_staged&quot; support staged scheduling. make(my_plan, parallelism = &quot;mclapply_staged&quot;, jobs = 2) make(my_plan, parallelism = &quot;parLapply_staged&quot;, jobs = 2) Here, the dependency network is divided into separate stages of conditionally independent targets. Within each stage, drake uses mclapply() or parLapply() to process the targets in parallel. Stages run one after the other, so the slowest target in the current stage needs to complete before the next stage begins. So we lose a lot of parallel efficiency. The following video demonstrates the major drawback.[1] However, because there is no formal master process in each stage, overhead is extremely low. This lack of overhead can make staged parallelism a great choice for projects with a small number of large stages: tall dependency graphs with most of the work in the tallest stages. library(dplyr) library(drake) N &lt;- 500 gen_data &lt;- function() { tibble(a = seq_len(N), b = 1, c = 2, d = 3) } plan_data &lt;- drake_plan( data = gen_data() ) plan_sub &lt;- gen_data() %&gt;% transmute( target = paste0(&quot;data&quot;, a), command = paste0(&quot;data[&quot;, a, &quot;, ]&quot;) ) plan &lt;- bind_rows(plan_data, plan_sub) plan ## # A tibble: 501 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 data gen_data() ## 2 data1 data[1, ] ## 3 data2 data[2, ] ## 4 data3 data[3, ] ## 5 data4 data[4, ] ## 6 data5 data[5, ] ## 7 data6 data[6, ] ## 8 data7 data[7, ] ## 9 data8 data[8, ] ## 10 data9 data[9, ] ## # ... with 491 more rows config &lt;- drake_config(plan) vis_drake_graph(config) 10.7 Final thoughts 10.7.1 Debugging For large workflows, downsizing and debugging tools become super important. See the guide to debugging and testing drake projects for help on diagnosing problems with a workflow. 10.7.2 Drake as an ordinary job scheduler If you do not care about reproducibility and you want drake to be an ordinary job scheduler, consider using alternative triggers (see ?trigger). load_mtcars_example() make(my_plan, trigger = trigger(condition = TRUE)) Above, drake only builds the missing targets. This skips much of the time-consuming hashing that ordinarily detects which targets are out of date. 10.7.3 More resources See the timing guide for explanations of functions predict_runtime() and predict_load_balancing(), which can help you plan and strategize deployment. 10.8 Footnotes [1] The video of staged parallelism is an oversimplification. It holds mostly true for make(parallelism = &quot;parLapply_staged&quot;), but make(parallelism = &quot;mclapply_staged&quot;) is a bit different. In the former case, each stage is a call to parLapply(), which recycles existing workers on a pre-built parallel socket (PSOCK) cluster. But in the latter, every stage is a new call to mclapply(), which launches a brand new batch of workers. In that sense, workers in make(parallelism = &quot;parLapply_staged&quot;) are sort of persistent, and workers in make(parallelism = &quot;mclapply_staged&quot;) are sort of transient for some projects. "],
["triggers.html", "Chapter 11 Triggers: decision rules for building targets 11.1 What are triggers? 11.2 Customization 11.3 Alternative trigger modes 11.4 A more practical example", " Chapter 11 Triggers: decision rules for building targets When you call make(), drake tries to skip as many targets as possible. If it thinks a command will return the same value as last time, it does not bother running it. In other words, drake is lazy, and laziness saves you time. 11.1 What are triggers? To figure out whether it can skip a target, drake goes through an intricate checklist of triggers: The missing trigger: Do we lack a return value from a previous make()? Maybe you are building the target for the first time or you removed it from the cache with clean(). The command trigger: did the command in the workflow plan data frame change nontrivially since the last make()? Changes to spacing, formatting, and comments are ignored. The depend trigger: did any non-file dependencies change since the last make()? These could be: Other targets. Imported objects. Imported functions (ignoring changes to spacing, formatting, and comments). Any dependencies of imported functions. Any dependencies of dependencies of imported functions, and so on. The file trigger: did any file inputs or file outputs change since the last make()? These files are the ones explicitly declared in the command with file_in(), knitr_in(), and file_out(). The condition trigger: an optional user-defined piece of code that evaluates to a TRUE/FALSE value. The target builds if the value is TRUE. The change trigger: an optional user-defined piece of code that evaluates to any value (preferably small and quick to compute). The target builds if the value changed since the last make(). If any trigger detects something wrong or different with the target or its dependencies, the next make() will run the command and (re)build the target. 11.2 Customization With the trigger() function, you can create your own customized checklist of triggers. Let’s run a simple workflow with just the missing trigger. We deactivate the command, depend, and file triggers by setting the respective command, depend, and file arguments to FALSE. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2, psi_2 = (sqrt(5) - 1) / 2 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) ## target psi_1 ## target psi_2 ## Used non-default triggers. Some targets may not be up to date. Now, even if you wreck all the commands, nothing rebuilds. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = (sqrt(5) - 1) / 2 - 9999999999999 ) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) ## Used non-default triggers. Some targets may not be up to date. You can also give different targets to different triggers. Triggers in the workflow plan data frame override the trigger argument to make(). Below, psi_2 always builds, but psi_1 only builds if it has never been built before. plan &lt;- drake_plan( psi_1 = (sqrt(5) + 1) / 2 + 9999999999999, psi_2 = target( command = (sqrt(5) - 1) / 2 - 9999999999999, trigger = trigger(condition = psi_1 &gt; 0) ) ) plan ## # A tibble: 2 x 3 ## target command trigger ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 psi_1 (sqrt(5) + 1)/2 + 9999999999999 &lt;NA&gt; ## 2 psi_2 (sqrt(5) - 1)/2 - 9999999999999 trigger(condition = psi_1 &gt; 0) make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) ## target psi_2 ## Used non-default triggers. Some targets may not be up to date. make(plan, trigger = trigger(command = FALSE, depend = FALSE, file = FALSE)) ## target psi_2 ## Used non-default triggers. Some targets may not be up to date. Interestingly, psi_2 now depends on psi_1. Since psi_1 is part of the because of the condition trigger, it needs to be up to date before we attempt psi_2. However, since psi_1 is not part of the command, changing it will not trip the other triggers such as depend. vis_drake_graph(drake_config(plan)) In the next toy example below, drake flips a coin to decide whether to build x. Try it out. set.seed(0) plan &lt;- drake_plan( x = target( 1 + 1, trigger(condition = runif(1) &gt; 0.5) ) ) make(plan) ## Unloading targets from environment: ## x ## target x make(plan) ## target x make(plan) ## All targets are already up to date. make(plan) ## All targets are already up to date. make(plan) ## target x In a real project with remote data sources, you may want to use the condition trigger to limit your builds to times when enough bandwidth is available for a large download. For example, drake_plan( x = target( command = download_large_dataset(), trigger = trigger(condition = is_enough_bandwidth()) ) ) Since the change trigger can return any value, it is often easier to use than the condition trigger. plan &lt;- drake_plan( x = target( command = 1 + 1, trigger = trigger(change = sqrt(y)) ) ) y &lt;- 1 make(plan) ## All targets are already up to date. make(plan) ## All targets are already up to date. y &lt;- 2 make(plan) ## All targets are already up to date. In practice, you may want to use the change trigger to check a large remote before downloading it. drake_plan( x = target( command = download_large_dataset(), trigger = trigger( condition = is_enough_bandwidth(), change = date_last_modified() ) ) ) A word of caution: every non-NULL change trigger is always evaluated, and its value is carried around in memory throughout make(). So if you are not careful, heavy use of the change trigger could slow down your workflow and consume extra resources. The change trigger should return small values (and should ideally be quick to evaluate). To reduce memory consumption, you may want to return a fingerprint of your trigger value rather than the value itself. See the digest package for more information on computing hashes/fingerprints. library(digest) drake_plan( x = target( command = download_large_dataset(), trigger = trigger( change = digest(download_medium_dataset()) ) ) ) 11.3 Alternative trigger modes Sometimes, you may want to suppress a target without having to worry about turning off every single trigger. That is why the trigger() function has a mode argument, which controls the role of the condition trigger in the decision to build or skip a target. The available trigger modes are &quot;whitelist&quot; (default), &quot;blacklist&quot;, and &quot;condition&quot;. trigger(mode = &quot;whitelist&quot;): we rebuild the target whenever condition evaluates to TRUE. Otherwise, we defer to the other triggers. This is the default behaviro described above in this chapter. trigger(mode = &quot;blacklist&quot;): we skip the target whenever condition evaluates to FALSE. Otherwise, we defer to the other triggers. trigger(mode = &quot;condition&quot;): here, the condition trigger is the only decider, and we ignore all the other triggers. We rebuild target whenever condition evaluates to TRUE and skip it whenever condition evaluates to FALSE. 11.4 A more practical example See the “packages” example for a more practical demonstration of triggers and their usefulness. "],
["time.html", "Chapter 12 Time: logging, prediction, and strategy 12.1 Predict total runtime 12.2 Strategize your high-performance computing", " Chapter 12 Time: logging, prediction, and strategy Thanks to Jasper Clarkberg, drake records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan, jobs = 2) ## Error: package txtq not installed. Please install it with install.packages(&quot;txtq&quot;). build_times(digits = 8) # From the cache. ## # A tibble: 0 x 5 ## # ... with 5 variables: item &lt;chr&gt;, type &lt;chr&gt;, elapsed &lt;S4: Duration&gt;, ## # user &lt;S4: Duration&gt;, system &lt;S4: Duration&gt; ## `dplyr`-style `tidyselect` commands build_times(starts_with(&quot;coef&quot;), digits = 8) ## # A tibble: 0 x 5 ## # ... with 5 variables: item &lt;chr&gt;, type &lt;chr&gt;, elapsed &lt;S4: Duration&gt;, ## # user &lt;S4: Duration&gt;, system &lt;S4: Duration&gt; build_times(digits = 8, targets_only = TRUE) ## # A tibble: 0 x 5 ## # ... with 5 variables: item &lt;chr&gt;, type &lt;chr&gt;, elapsed &lt;S4: Duration&gt;, ## # user &lt;S4: Duration&gt;, system &lt;S4: Duration&gt; For drake version 4.1.0 and earlier, build_times() just measures the elapsed runtime of each command in my_plan$command. For later versions, the build times also account for all the internal operations in drake:::build(), such as storage and hashing. 12.1 Predict total runtime Drake uses these times to predict the runtime of the next make(). At this moment, everything is up to date in the current example, so the next make() should be fast. Here, we only factor in the times of the formal targets in the workflow plan, excluding any imports. config &lt;- drake_config(my_plan, verbose = FALSE) predict_runtime(config, targets_only = TRUE) ## Warning: Some targets were never actually timed, And no hypothetical time was specified in `known_times`. Assuming a runtime of 0 for these targets: ## coef_regression2_small ## large ## small ## regression1_small ## regression1_large ## regression2_small ## regression2_large ## report ## summ_regression1_small ## summ_regression1_large ## summ_regression2_small ## summ_regression2_large ## coef_regression1_small ## coef_regression1_large ## coef_regression2_large ## Error in loadNamespace(name): there is no package called &#39;txtq&#39; Suppose we change a dependency to make some targets out of date. Now, even though, the next make() should take a little longer. reg2 &lt;- function(d){ d$x3 &lt;- d$x ^ 3 lm(y ~ x3, data = d) } predict_runtime(config, targets_only = TRUE) ## Warning: Some targets were never actually timed, And no hypothetical time was specified in `known_times`. Assuming a runtime of 0 for these targets: ## coef_regression2_small ## large ## small ## regression1_small ## regression1_large ## regression2_small ## regression2_large ## report ## summ_regression1_small ## summ_regression1_large ## summ_regression2_small ## summ_regression2_large ## coef_regression1_small ## coef_regression1_large ## coef_regression2_large ## Error in loadNamespace(name): there is no package called &#39;txtq&#39; But what if you plan on starting from scratch next time, either after clean() or with make(trigger = trigger(condition = TRUE))? predict_runtime(config, from_scratch = TRUE, targets_only = TRUE) ## Warning: Some targets were never actually timed, And no hypothetical time was specified in `known_times`. Assuming a runtime of 0 for these targets: ## coef_regression2_small ## large ## small ## regression1_small ## regression1_large ## regression2_small ## regression2_large ## report ## summ_regression1_small ## summ_regression1_large ## summ_regression2_small ## summ_regression2_large ## coef_regression1_small ## coef_regression1_large ## coef_regression2_large ## Error in loadNamespace(name): there is no package called &#39;txtq&#39; 12.2 Strategize your high-performance computing Let’s say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run make(), your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let’s write down these known times in seconds. known_times &lt;- c(5, rep(7200, nrow(my_plan) - 1)) names(known_times) &lt;- c(file_store(&quot;report.md&quot;), my_plan$target[-1]) known_times ## &quot;report.md&quot; small large ## 5 7200 7200 ## regression1_small regression1_large regression2_small ## 7200 7200 7200 ## regression2_large summ_regression1_small summ_regression1_large ## 7200 7200 7200 ## summ_regression2_small summ_regression2_large coef_regression1_small ## 7200 7200 7200 ## coef_regression1_large coef_regression2_small coef_regression2_large ## 7200 7200 7200 How many parallel jobs should you use in the next make()? The predict_runtime() function can help you decide. predict_runtime(jobs = n) simulates persistent parallel workers and reports the estimated total runtime of make(jobs = n). (See also predict_load_balancing().) time &lt;- c() for (jobs in 1:12){ time[jobs] &lt;- predict_runtime( config, jobs = jobs, from_scratch = TRUE, known_times = known_times ) } ## Warning: Some targets were never actually timed, And no hypothetical time was specified in `known_times`. Assuming a runtime of 0 for these targets: ## random_rows ## reg1 ## reg2 ## &quot;report.Rmd&quot; ## simulate ## report ## Error in loadNamespace(name): there is no package called &#39;txtq&#39; library(ggplot2) ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) + geom_line(aes(x = jobs, y = time, group = group)) + scale_y_continuous(breaks = 0:10 * 4, limits = c(0, 29)) + theme_gray(16) + xlab(&quot;jobs argument of make()&quot;) + ylab(&quot;Predicted runtime of make() (hours)&quot;) ## Error in data.frame(time = time/3600, jobs = ordered(1:12), group = 1): arguments imply differing number of rows: 0, 12, 1 We see serious potential speed gains up to 4 jobs, but beyond that point, we have to double the jobs to shave off another 2 hours. Your choice of jobs for make() ultimately depends on the runtime you can tolerate and the computing resources at your disposal. A final note on predicting runtime: the output of predict_runtime() and predict_load_balancing() also depends the optional workers column of your drake_plan(). If you micromanage which workers are allowed to build which targets, you may minimize reads from disk, but you could also slow down your workflow if you are not careful. See the high-performance computing guide for more. "],
["store.html", "Chapter 13 Storage 13.1 Caches 13.2 Hash algorithms 13.3 Which hash algorithm should you choose? 13.4 Select the hash algorithms of the cache 13.5 Using storr directly 13.6 Cleaning up", " Chapter 13 Storage drake’s make() function generates your project’s output, and drake takes storing this output seriously. This guide explains how drake caches and hashes its data, and describes customization options that can increase convenience and speed. 13.1 Caches When you run make(), drake stores your imports and targets in a hidden cache. library(drake) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). config &lt;- make(my_plan, verbose = FALSE) You can explore your cached data using functions loadd(), readd(), cached(), and others. head(cached()) ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; head(readd(small)) ## x y ## 1 5.424 10.4 ## 2 3.440 17.8 ## 3 3.440 19.2 ## 4 3.170 15.8 ## 5 3.730 17.3 ## 6 3.845 19.2 loadd(large) head(large) ## x y ## 1 3.170 15.8 ## 2 5.345 14.7 ## 3 4.070 16.4 ## 4 1.615 30.4 ## 5 3.170 15.8 ## 6 1.513 30.4 rm(large) # Does not remove `large` from the cache. By default, these objects live in a hidden .drake folder in your working directory. find_cache() ### [1] &quot;/home/you/project/.drake&quot; find_project() ### [1] &quot;/home/you/project&quot; drake (via storr) has an object-like interface to these caches. cache &lt;- get_cache() cache$list() ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [7] &quot;large&quot; &quot;random_rows&quot; ## [9] &quot;reg1&quot; &quot;reg2&quot; ## [11] &quot;regression1_large&quot; &quot;regression1_small&quot; ## [13] &quot;regression2_large&quot; &quot;regression2_small&quot; ## [15] &quot;report&quot; &quot;simulate&quot; ## [17] &quot;small&quot; &quot;summ_regression1_large&quot; ## [19] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [21] &quot;summ_regression2_small&quot; head(cache$get(&quot;small&quot;)) ## x y ## 1 5.424 10.4 ## 2 3.440 17.8 ## 3 3.440 19.2 ## 4 3.170 15.8 ## 5 3.730 17.3 ## 6 3.845 19.2 tail(cache$get(&quot;small&quot;, namespace = &quot;meta&quot;)) ## $dependency_hash ## [1] &quot;95f7dd7fc0839d9037de2690b378708acac1d60e1e10a268cbe861097ebb5a1e&quot; ## ## $input_file_hash ## [1] &quot;e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&quot; ## ## $output_file_hash ## [1] &quot;e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&quot; ## ## $start ## user system elapsed ## 85.732 4.368 94.842 ## ## $time_command ## item type elapsed user system ## 1 small target 0.001 0.004 0 ## ## $time_build ## item type elapsed user system ## 1 small target 0.009 0.012 0 cache$list_namespaces() ## [1] &quot;attempt&quot; &quot;common&quot; &quot;config&quot; &quot;kernels&quot; &quot;memoize&quot; &quot;meta&quot; ## [7] &quot;objects&quot; &quot;progress&quot; &quot;session&quot; Create a new cache of your own with new_cache(). cache2 &lt;- new_cache(path = &quot;cache2&quot;) file.exists(&quot;cache2&quot;) ## [1] TRUE You can use multiple caches simultaneously, default and non-default alike. config &lt;- drake_config(cache = cache) config2 &lt;- drake_config(cache = cache2) outdated(config) ## character(0) outdated(config2) ## [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [5] &quot;large&quot; &quot;regression1_large&quot; ## [7] &quot;regression1_small&quot; &quot;regression2_large&quot; ## [9] &quot;regression2_small&quot; &quot;report&quot; ## [11] &quot;small&quot; &quot;summ_regression1_large&quot; ## [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [15] &quot;summ_regression2_small&quot; make(my_plan, cache = cache) ## All targets are already up to date. make(my_plan, cache = cache2) ## target large ## target small ## target regression1_large ## target regression2_large ## target regression1_small ## target regression2_small ## target summ_regression1_large ## target coef_regression1_large ## target summ_regression2_large ## target coef_regression2_large ## target summ_regression1_small ## target coef_regression1_small ## target coef_regression2_small ## target summ_regression2_small ## target report There are a couple different ways to retrieve caches. get_cache(path = &quot;my_path&quot;) assumes my_path is a project root containing a .drake folder. If it does not find a .drake folder in my_path, it searches up through the ancestors of my_path until it finds one. this_cache(path = &quot;my_path&quot;) literally assumes my_path is the path to the cache, .drake folder or not. storr::storr_rds(&quot;my_path&quot;, mangle_key = TRUE) is analogous to this_cache(path = &quot;my_path&quot;). cache3 &lt;- get_cache(path = getwd()) # Finds the .drake folder in your directory. head(cache3$list()) ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; cache4 &lt;- this_cache(path = &quot;cache2&quot;) # The cache folder is literally called cache2. head(cache4$list()) ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; Destroy caches to remove them from your file system. cache4$destroy() cache2$list() # Same folder as cache4. ## character(0) See storr for more on drake’s caching infrastructure. 13.2 Hash algorithms The concept of hashing is central to storr’s internals. Storr uses hashes to label stored objects, and drake leverages these hashes to figure out which targets are up to date and which ones are outdated. A hash is like a target’s fingerprint, so the hash changes when the target changes. Regardless of the target’s size, the hash is always the same number of characters. library(digest) # package for hashing objects and files smaller_data &lt;- 12 larger_data &lt;- rnorm(1000) digest(smaller_data) # compute the hash ## [1] &quot;23c80a31c0713176016e6e18d76a5f31&quot; digest(larger_data) ## [1] &quot;4e0ccc0c347fc3e1f70ab60b9d99f555&quot; However, different hash algorithms vary in length. digest(larger_data, algo = &quot;sha512&quot;) ## [1] &quot;c086d140868e5596a322cc2513dd7744638b942fdf066edd6bd9b6ce50549493e7b7816d063926471c1c5b26fe744c57bd750229db7db3b77e1464087f91a2c4&quot; digest(larger_data, algo = &quot;md5&quot;) ## [1] &quot;4e0ccc0c347fc3e1f70ab60b9d99f555&quot; digest(larger_data, algo = &quot;xxhash64&quot;) ## [1] &quot;c298df1e94141c7f&quot; digest(larger_data, algo = &quot;murmur32&quot;) ## [1] &quot;5af98121&quot; 13.3 Which hash algorithm should you choose? Hashing is expensive, and unsurprisingly, shorter hashes are usually faster to compute. So why not always use murmur32? One reason is the risk of collisions: that is, when two different objects have the same hash. In general, shorter hashes have more frequent collisions. On the other hand, a longer hash is not always the answer. Besides the loss of speed, drake and storr sometimes use hash keys as file names, and long hashes could violate the 260-character cap on Windows file paths. That is why drake uses a shorter hash algorithm for internal cache-related file names and a longer hash algorithm for everything else. default_short_hash_algo() ## [1] &quot;xxhash64&quot; default_long_hash_algo() ## [1] &quot;sha256&quot; short_hash(cache) ## [1] &quot;xxhash64&quot; long_hash(cache) ## [1] &quot;sha256&quot; 13.4 Select the hash algorithms of the cache If you want to set the hash algorithms, do so right when the cache is first created. ## cache_path(cache) # Default cache from before. # nolint ## Start from scratch to reset both hash algorithms. clean(destroy = TRUE) tmp &lt;- new_cache( path = default_cache_path(), # The `.drake/` folder. short_hash_algo = &quot;crc32&quot;, long_hash_algo = &quot;sha1&quot; ) config &lt;- make(my_plan, verbose = FALSE) short_hash(config$cache) # xxhash64 is the default_short_hash_algo() ## [1] &quot;crc32&quot; long_hash(config$cache) # sha256 is the default_long_hash_algo() ## [1] &quot;sha1&quot; You can change the long hash algorithm without throwing away the cache, but your project will rebuild from scratch. As for the short hash, you are committed until you delete the cache and all its supporting files. outdated(config) # empty ## character(0) config$cache &lt;- configure_cache( config$cache, long_hash_algo = &quot;murmur32&quot;, overwrite_hash_algos = TRUE ) Below, the targets become outdated because the existing hash keys do not match the new hash algorithm. config &lt;- drake_config(my_plan, verbose = FALSE, cache = config$cache) outdated(config) ## [1] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [3] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [5] &quot;large&quot; &quot;regression1_large&quot; ## [7] &quot;regression1_small&quot; &quot;regression2_large&quot; ## [9] &quot;regression2_small&quot; &quot;report&quot; ## [11] &quot;small&quot; &quot;summ_regression1_large&quot; ## [13] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [15] &quot;summ_regression2_small&quot; config &lt;- make(my_plan, verbose = FALSE) short_hash(config$cache) # same as before ## [1] &quot;crc32&quot; long_hash(config$cache) # different from before ## [1] &quot;murmur32&quot; 13.5 Using storr directly If you want bypass drake and generate a cache directly from storr, it is best to do so right from the beginning. library(storr) my_storr &lt;- storr_rds(&quot;my_storr&quot;, mangle_key = TRUE) new_plan &lt;- drake_plan(simple = sqrt(4)) make(new_plan, cache = my_storr) ## target simple cached(cache = my_storr) ## [1] &quot;simple&quot; readd(simple, cache = my_storr) ## [1] 2 In addition to storr_rds(), drake supports in-memory caches created from storr_environment(). However, parallel computing is not supported these caches. The jobs argument must be 1, and the parallelism argument must be either &quot;mclapply&quot; or &quot;parLapply&quot;. (It is sufficient to leave the default values alone.) memory_cache &lt;- storr_environment() other_plan &lt;- drake_plan( some_data = rnorm(50), more_data = rpois(75, lambda = 10), result = mean(c(some_data, more_data)) ) make(other_plan, cache = memory_cache) ## target some_data ## target more_data ## target result cached(cache = memory_cache) ## [1] &quot;more_data&quot; &quot;result&quot; &quot;some_data&quot; readd(result, cache = memory_cache) ## [1] 6.247034 In theory, it should be possible to leverage serious databases using storr_dbi(). However, if you use such caches, please heed the following. Be sure you have storr version 1.1.3 or greater installed. Be careful about parallel computing. For example the storr::storr_dbi() cache is not thread-safe. To use these exotic cache types in parallelized make()s, select caching = &quot;master&quot; and set parallelism equal to &quot;clustermq&quot;, &quot;clustermq_staged&quot;, or &quot;future&quot;. In addition, make sure imports are not parallelized (i.e. use something like make(jobs = c(imports = 1, targets = 4)). The following example demonstrates the use of the DBI cahce, and it requires the DBI and RSQLite packages. mydb &lt;- DBI::dbConnect(RSQLite::SQLite(), &quot;my-db.sqlite&quot;) cache &lt;- storr::storr_dbi( tbl_data = &quot;data&quot;, tbl_keys = &quot;keys&quot;, con = mydb ) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). unlink(&quot;.drake&quot;, recursive = TRUE) make(my_plan, cache = cache) # Parallelized alternative: options(clustermq.scheduler = &quot;multicore&quot;) make( my_plan, parallelism = &quot;clustermq&quot;, jobs = c(imports = 1, targets = 4), cache = cache ) 13.6 Cleaning up If you want to start from scratch, you can clean() the cache. Use the destroy argument to remove it completely. cache$del() and cache$destroy() are also options, but they leave output file targets dangling. By contrast, clean(destroy = TRUE) removes file targets generated by drake::make(). drake_gc() and clean(..., garbage_collection = TRUE) do garbage collection, and clean(purge = TRUE) removes all target-level data, not just the final output values. clean(small, large) cached() # &#39;small&#39; and &#39;large&#39; are gone ## [1] &quot;\\&quot;report.md\\&quot;&quot; &quot;\\&quot;report.Rmd\\&quot;&quot; ## [3] &quot;coef_regression1_large&quot; &quot;coef_regression1_small&quot; ## [5] &quot;coef_regression2_large&quot; &quot;coef_regression2_small&quot; ## [7] &quot;random_rows&quot; &quot;reg1&quot; ## [9] &quot;reg2&quot; &quot;regression1_large&quot; ## [11] &quot;regression1_small&quot; &quot;regression2_large&quot; ## [13] &quot;regression2_small&quot; &quot;report&quot; ## [15] &quot;simulate&quot; &quot;summ_regression1_large&quot; ## [17] &quot;summ_regression1_small&quot; &quot;summ_regression2_large&quot; ## [19] &quot;summ_regression2_small&quot; clean(destroy = TRUE) clean(destroy = TRUE, cache = my_storr) "],
["faq.html", "A Frequently-asked questions", " A Frequently-asked questions This FAQ is a compendium of pedagogically useful issues tagged on GitHub. To contribute, please submit a new issue and ask that it be labeled a frequently asked question. function dependencies are missing: drake_config() in a magrittr pipe evaulate_plan() with multiple rules Best practices for including a drake workflow in a package Can you have multiple drake plans? Metaprogramming Difficulties (vignette potentially helpful) How to propagate wild cards into future steps How to speed up construction of a (very) large plan? evaluate file.path and variables in file_out and friends Working with HPC time limits How to debug file targets Gather without loading all dependencies at the same time Reproducibility with random numbers output file as target How should I mix non-R code (e.g. Python and shell scripts) in a large drake workflow? Helper function for creating file targets with multiple files Reproducible remote data sources Command that writes a file always runs Using strings as wildcards? Trouble with caches sent through Dropbox How to add .R files to drake_plan() evaluate_plan() with file targets "],
["caution.html", "B Cautionary notes B.1 Projects built with drake &lt;= 4.4.0 are not back compatible with drake &gt; 4.4.0. B.2 Workflow plans B.3 Execution B.4 Dependencies B.5 High-performance computing B.6 Storage", " B Cautionary notes This chapter addresses drake’s known edge cases, pitfalls, and weaknesses that might not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the issue tracker, where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version of drake. For a guide to debugging and testing drake projects, please refer to the separate guide to debugging and testing drake projects. B.1 Projects built with drake &lt;= 4.4.0 are not back compatible with drake &gt; 4.4.0. The cache internals changed between 4.4.0 and 5.0.0. Unfortunately, you will need to make() your project all over again. B.2 Workflow plans B.2.1 Externalizing commands in R script files It is common practice to divide the work of a project into multiple R files, but if you do this, you will not get the most out of drake. Please see the chapter on organizing your files for more details. B.2.2 Commands are NOT perfectly flexible. In your workflow plan data frame (produced by drake_plan() and accepted by make()), your commands can usually be flexible R expressions. drake_plan( target1 = 1 + 1 - sqrt(sqrt(3)), target2 = my_function(web_scraped_data) %&gt;% my_tidy ) ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 target1 1 + 1 - sqrt(sqrt(3)) ## 2 target2 my_function(web_scraped_data) %&gt;% my_tidy However, please try to avoid formulas and function definitions in your commands. You may be able to get away with drake_plan(f = function(x){x + 1}) or drake_plan(f = y ~ x) in some use cases, but be careful. It is generally to define functions and formulas in your workspace and then let make() import them. (Alternatively, use the envir argument to make() to tightly control which imported functions are available.) Use the check_plan() function to help screen and quality-control your workflow plan data frame, use tracked() to see the items that are reproducibly tracked, and use vis_drake_graph() and build_drake_graph() to see the dependency structure of your project. B.3 Execution B.3.1 Install drake properly. You must properly install drake using install.packages(), devtools::install_github(), or a similar approach. Functions like devtools::load_all() are insufficient, particularly for parallel computing functionality in which separate new R sessions try to require(drake). B.3.2 Install all your packages. Your workflow may depend on external packages such as ggplot2, dplyr, and MASS. Such packages must be formally installed with install.packages(), devtools::install_github(), devtools::install_local(), or a similar command. If you load uninstalled packages with devtools::load_all(), results may be unpredictable and incorrect. B.3.3 A note on tidy evaluation Running commands in your R console is not always exactly like running them with make(). That’s because make() uses tidy evaluation as implemented in the rlang package. ## This workflow plan uses rlang&#39;s quasiquotation operator `!!`. my_plan &lt;- drake_plan(list = c( little_b = &quot;\\&quot;b\\&quot;&quot;, letter = &quot;!!little_b&quot; )) my_plan ## # A tibble: 2 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 little_b &quot;\\&quot;b\\&quot;&quot; ## 2 letter !!little_b make(my_plan) ## target little_b ## target letter readd(letter) ## [1] &quot;b&quot; For the commands you specify the free-form ... argument, drake_plan() also supports tidy evaluation. For example, it supports quasiquotation with the !! argument. Use tidy_evaluation = FALSE or the list argument to suppress this behavior. my_variable &lt;- 5 drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;) ) ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 a 5 ## 2 b 5 + 1 ## 3 d !!my_variable drake_plan( a = !!my_variable, b = !!my_variable + 1, list = c(d = &quot;!!my_variable&quot;), tidy_evaluation = FALSE ) ## # A tibble: 3 x 2 ## target command ## &lt;chr&gt; &lt;chr&gt; ## 1 a !!my_variable ## 2 b !!my_variable + 1 ## 3 d !!my_variable For instances of !! that remain in the workflow plan, make() will run these commands in tidy fashion, evaluating the !! operator using the environment you provided. B.3.4 Find and diagnose your errors. When make() fails, use failed() and diagnose() to debug. Try the following out yourself. ## Targets with available diagnostic metadata, incluing errors, warnings, etc. diagnose() ## [1] &quot;letter&quot; &quot;little_b&quot; f &lt;- function(){ stop(&quot;unusual error&quot;) } bad_plan &lt;- drake_plan(target = f()) withr::with_message_sink( stdout(), make(bad_plan) ) ## target target ## fail target ## Error: Target `target` failed. Call `diagnose(target)` for details. Error message: ## unusual error ## Warning: No message sink to remove. failed() # From the last make() only ## [1] &quot;target&quot; error &lt;- diagnose(target)$error # See also warnings and messages. error$message ## [1] &quot;unusual error&quot; error$call ## f() error$calls # View the traceback. ## [[1]] ## local({ ## f() ## }) ## ## [[2]] ## eval.parent(substitute(eval(quote(expr), envir))) ## ## [[3]] ## eval(expr, p) ## ## [[4]] ## eval(expr, p) ## ## [[5]] ## eval(quote({ ## f() ## }), new.env()) ## ## [[6]] ## eval(quote({ ## f() ## }), new.env()) ## ## [[7]] ## f() ## ## [[8]] ## stop(&quot;unusual error&quot;) B.3.5 Refresh the drake_config() list early and often. The master configuration list returned by drake_config() is important to drake’s internals, and you will need it for functions like outdated() and vis_drake_graph(). The config list corresponds to a single call to make(), and you should not modify it by hand afterwards. For example, modifying the targets element post-hoc will have no effect because the graph element will remain the same. It is best to just call drake_config() again. B.3.6 Workflows as R packages. The R package structure is a great way to organize the files of your project. Writing your own package to contain your data science workflow is a good idea, but you will need to Use expose_imports() to properly account for all your nested function dependencies, and If you load the package with devtools::load_all(), set the prework argument of make(): e.g. make(prework = &quot;devtools::load_all()&quot;). See the file organization chapter and ?expose_imports for detailed explanations. Thanks to Jasper Clarkberg for the workaround. B.3.7 The lazy_load flag does not work with &quot;parLapply&quot; parallelism. Ordinarily, drake prunes the execution environment at every parallelizable stage. In other words, it loads all the dependencies and unloads anything superfluous for entire batches of targets. This approach may require too much memory for some use cases, so there is an option to delay the loading of dependencies using the lazy_load argument to make() (powered by delayedAssign()). There are two major risks. make(..., lazy_load = TRUE, parallelism = &quot;parLapply&quot;, jobs = 2) does not work. If you want to use local multisession parallelism with multiple jobs and lazy loading, try &quot;future_lapply&quot; parallelism instead. library(future) future::plan(multisession) load_mtcars_example() # Get the code with drake_example(&quot;mtcars&quot;). make(my_plan, lazy_load = TRUE, parallelism = &quot;future_lapply&quot;) Delayed evaluation may cause the same dependencies to be loaded multiple times, and these duplicated loads could be slow. B.3.8 Timeouts may be unreliable. You can call make(..., timeout = 10) to time out all each target after 10 seconds. However, timeouts rely on R.utils::withTimeout(), which in turn relies on setTimeLimit(). These functions are the best that R can offer right now, but they have known issues, and timeouts may fail to take effect for certain environments. B.4 Dependencies B.4.1 Objects that contain functions may rebuild too often For example, an R6 class changes whenever a new R6 object of that class is created. library(digest) library(R6) circle_class &lt;- R6Class( &quot;circle_class&quot;, private = list(radius = NULL), public = list( initialize = function(radius){ private$radius &lt;- radius }, area = function(){ pi * private$radius ^ 2 } ) ) digest(circle_class) ## [1] &quot;8f92f2c735b43de86ec66599369235a8&quot; circle &lt;- circle_class$new(radius = 5) digest(circle_class) # example_class changed ## [1] &quot;dee77b53266657e61a089d51607e1371&quot; rm(circle) Ordinarily, drake overreacts to this change and builds targets repeatedly. clean() plan &lt;- drake_plan( circle = circle_class$new(radius = 10), area = circle$area() ) make(plan) # `circle_class` changes because it is referenced. ## target circle ## target area make(plan) # Builds `circle` again because `circle_class` changed. ## target circle The solution is to define your R6 class inside a function. drake does the right thing when it comes to tracking changes to functions. clean() new_circle &lt;- function(radius){ circle_class &lt;- R6Class( &quot;circle_class&quot;, private = list(radius = NULL), public = list( initialize = function(radius){ private$radius &lt;- radius }, area = function(){ pi * private$radius ^ 2 } ) ) circle_class$new(radius = radius) } plan &lt;- drake_plan( circle = new_circle(radius = 10), area = circle$area() ) make(plan) ## target circle ## target area make(plan) ## All targets are already up to date. B.4.2 The magrittr dot (.) is always ignored. It is common practice to use a literal dot (.) to carry an object through a magrittr pipeline. Due to some tricky limitations in static code analysis, drake never treats the dot (.) as a dependency, even if you use it as an ordinary variable outside of a tidyverse context. deps_code(&quot;sqrt(x + y + .)&quot;) ## $globals ## [1] &quot;sqrt&quot; &quot;x&quot; &quot;y&quot; deps_code(&quot;dplyr::filter(complete.cases(.))&quot;) ## $namespaced ## [1] &quot;dplyr::filter&quot; ## ## $globals ## [1] &quot;complete.cases&quot; B.4.3 Dependencies are not tracked in some edge cases. You should explicitly learn the items in your workflow and the dependencies of your targets. ?deps ?tracked ?vis_drake_graph drake can be fooled into skipping objects that should be treated as dependencies. For example: f &lt;- function(){ b &lt;- get(&quot;x&quot;, envir = globalenv()) # x is incorrectly ignored digest::digest(file_dependency) } deps_code(f) ## $globals ## [1] &quot;get&quot; &quot;globalenv&quot; &quot;file_dependency&quot; ## ## $namespaced ## [1] &quot;digest::digest&quot; command &lt;- &quot;x &lt;- digest::digest(file_in(\\&quot;input_file.rds\\&quot;)); assign(\\&quot;x\\&quot;, 1); x&quot; # nolint deps_code(command) ## $globals ## [1] &quot;assign&quot; ## ## $namespaced ## [1] &quot;digest::digest&quot; ## ## $file_in ## [1] &quot;\\&quot;input_file.rds\\&quot;&quot; drake takes special precautions so that a target/import does not depend on itself. For example, deps_code(f) might return &quot;f&quot; if f() is a recursive function, but make() just ignores this conflict and runs as expected. In other words, make() automatically removes all self-referential loops in the dependency network. B.4.4 Dependencies of knitr reports If you have knitr reports, you can use knitr_report() in your commands so that your reports are refreshed every time one of their dependencies changes. See drake_example(&quot;mtcars&quot;) and the ?knitr_in() help file examples for demonstrations. Dependencies are detected if you call loadd() or readd() in your code chunks. But beware: an empty call to loadd() does not account for any dependencies even though it loads all the available targets into your R session. B.4.5 File outputs in imported functions. Do not call file_out() inside imported functions that you write. Only targets in your workflow plan data frame should have file outputs. ## toally_fine() will depend on the imported data.csv file. ## But make sure data.csv is an imported file and not a file target. totally_okay &lt;- function(x, y, z){ read.csv(file_in(&quot;data.csv&quot;)) } ## file_out() is for file targets, ## so `drake` will ignore it in imported functions. avoid_this &lt;- function(x, y, z){ read.csv(file_out(&quot;data.csv&quot;)) } B.4.6 Functions produced by Vectorize() With functions produced by Vectorize(), detecting dependencies is especially hard because the body of every such function is args &lt;- lapply(as.list(match.call())[-1L], eval, parent.frame()) names &lt;- if (is.null(names(args))) character(length(args)) else names(args) dovec &lt;- names %in% vectorize.args do.call(&quot;mapply&quot;, c(FUN = FUN, args[dovec], MoreArgs = list(args[!dovec]), SIMPLIFY = SIMPLIFY, USE.NAMES = USE.NAMES)) Thus, if f is constructed with Vectorize(g, ...), drake searches g() for dependencies, not f(). In fact, if drake sees that environment(f)[[&quot;FUN&quot;]] exists and is a function, then environment(f)[[&quot;FUN&quot;]] will be analyzed instead of f(). Furthermore, if f() is the output of Vectorize(), then drake reproducibly tracks environment(f)[[&quot;FUN&quot;]] rather than f() itself. Thus, if the configuration settings of vectorization change (such as which arguments are vectorized), but the core element-wise functionality remains the same, then make() will not react. Also, if you hover over the f node in vis_drake_graph(hover = TRUE), then you will see the body of environment(f)[[&quot;FUN&quot;]], not the body of f(). B.4.7 Compiled code is not reproducibly tracked. Some R functions use .Call() to run compiled code in the backend. The R code in these functions is tracked, but not the compiled object called with .Call(), nor its C/C++/Fortran source. B.4.8 Directories (folders) are not reproducibly tracked. In your workflow plan, you can use file_in(), file_out(), and knitr_in() to assert that some targets/imports are external files. However, entire directories (i.e. folders) cannot be reproducibly tracked this way. Please see issue 12 for a discussion. B.4.9 Packages are not tracked as dependencies. drake may import functions from packages, but the packages themselves are not tracked as dependencies. For this, you will need other tools that support reproducibility beyond the scope of drake. Packrat creates a tightly-controlled local library of packages to extend the shelf life of your project. And with Docker, you can execute your project on a virtual machine to ensure platform independence. Together, packrat and Docker can help others reproduce your work even if they have different software and hardware. B.5 High-performance computing B.5.1 The practical utility of parallel computing drake claims that it can Build and cache your targets in parallel (in stages). Build and cache your targets in the correct order, finishing dependencies before starting targets that depend on them. Deploy your targets to the parallel backend of your choice. However, the practical efficiency of the parallel computing functionality remains to be verified rigorously. Serious performance studies will be part of future work that has not yet been conducted at the time of writing. In addition, each project has its own best parallel computing set up, and the user needs to optimize it on a case-by-case basis. Some general considerations include the following. The high overhead and high scalability of distributed computing versus the low overhead and low scalability of local multicore computing. The high memory usage of local multicore computing, especially &quot;mclapply&quot; parallelism, as opposed to distributed computing, which can spread the memory demands over the available nodes on a cluster. The marginal gains of increasing the number of jobs indefinitely, especially in the case of local multicore computing if the number of cores is low. B.5.2 Maximum number of simultaneous jobs Be mindful of the maximum number of simultaneous parallel jobs you deploy. At best, too many jobs is poor etiquette on a system with many users and limited resources. At worst, too many jobs will crash a system. The jobs argument to make() sets the maximum number of simultaneous jobs in most cases, but not all. For most of drake’s parallel backends, jobs sets the maximum number of simultaneous parallel jobs. However, there are ways to break the pattern. For example, make(..., parallelism = &quot;Makefile&quot;, jobs = 2, args = &quot;--jobs=4&quot;) uses at most 2 jobs for the imports and at most 4 jobs for the targets. (In make(), args overrides jobs for the targets). For make(..., parallelism = &quot;future_lapply_staged&quot;), the jobs argument is ignored altogether. Instead, you should set the workers argument where it is available (for example, future::plan(mutlisession(workers = 2)) or future::plan(future.batchtools::batchtools_local(workers = 2))) in the preparations before make(parallelism = &quot;future_lapply_staged&quot;). Alternatively, you might limit the max number of jobs by setting options(mc.cores = 2) before calling make(parallelism = &quot;future_lapply_staged&quot;). Depending on the future backend you select with future::plan() or future::plan(), you might make use of one of the other environment variables listed in ?future::future.options. B.5.3 Parallel computing on Windows On Windows, do not use make(..., parallelism = &quot;mclapply&quot;, jobs = n) with n greater than 1. You could try, but jobs will just be demoted to 1. Instead, please replace &quot;mclapply&quot; with one of the other parallelism_choices() or let drake choose the parallelism backend for you. For make(..., parallelism = &quot;Makefile&quot;), Windows users need to download and install Rtools. B.5.4 Configuring future/batchtools parallelism for clusters The &quot;future_lapply&quot; backend unlocks a large array of distributed computing options on serious computing clusters. However, it is your responsibility to configure your workflow for your specific job scheduler. In particular, special batchtools *.tmpl configuration files are required, and the technique is described in the documentation of batchtools. You can find some examples of these files in the inst/templates folders of the batchtools and future.batchtools GitHub repositories. drake has some prepackaged example workflows. See drake_examples() to view your options, and then drake_example() to write the files for an example. drake_example(&quot;sge&quot;) # Sun/Univa Grid Engine workflow and supporting files drake_example(&quot;slurm&quot;) # SLURM drake_example(&quot;torque&quot;) # TORQUE To write just *.tmpl files from these examples, see the drake_batchtools_tmpl_file() function. Unfortunately, there is no one-size-fits-all *.tmpl configuration file for any job scheduler, so we cannot guarantee that the above examples will work for you out of the box. To learn how to configure the files to suit your needs, you should make sure you understand your job scheduler and batchtools. B.5.5 Proper Makefiles are not standalone. The Makefile generated by make(myplan, parallelism = &quot;Makefile&quot;) is not standalone. Do not run it outside of drake::make(). drake uses dummy timestamp files to tell the Makefile what to do, and running make in the terminal will most likely give incorrect results. B.5.6 Makefile-level parallelism for imported objects and files Makefile-level parallelism is only used for targets in your workflow plan data frame, not imports. To process imported objects and files, drake selects the best local parallel backend for your system and uses the jobs argument to make(). To use at most 2 jobs for imports and at most 4 jobs for targets, run make(..., parallelism = &quot;Makefile&quot;, jobs = 2, args = &quot;--jobs=4&quot;) B.5.7 Zombie processes Some parallel backends, particularly mclapply and future::multicore, may create zombie processes. Zombie children are not usually harmful, but you may wish to kill them yourself. The following function by Carl Boneri should work on Unix-like systems. For a discussion, see drake issue 116. fork_kill_zombies &lt;- function(){ require(inline) includes &lt;- &quot;#include &lt;sys/wait.h&gt;&quot; code &lt;- &quot;int wstat; while (waitpid(-1, &amp;wstat, WNOHANG) &gt; 0) {};&quot; wait &lt;- inline::cfunction( body = code, includes = includes, convention = &quot;.C&quot; ) invisible(wait()) } B.6 Storage B.6.1 Projects hosted on Dropbox and similar platforms If download a drake project from Dropbox, you may get an error like the one in issue 198: cache pathto/.drake connect 61 imports: ... connect 200 targets: ... Error in rawToChar(as.raw(x)) : embedded nul in string: 'initial_drake_version\\0\\0\\x9a\\x9d\\xdc\\0J\\xe9\\0\\0\\0(\\x9d\\xf9brם\\0\\xca)\\0\\0\\xb4\\xd7\\0\\0\\0\\0\\xb9' In addition: Warning message: In rawToChar(as.raw(x)) : out-of-range values treated as 0 in coercion to raw This is probably because Dropbox generates a bunch of “conflicted copy” files when file transfers do not go smoothly. This confuses storr, drake’s caching backend. keys/config/aG9vaw (Sandy Sum's conflicted copy 2018-01-31) keys/config/am9icw (Sandy Sum's conflicted copy 2018-01-31) keys/config/c2VlZA (Sandy Sum's conflicted copy 2018-01-31) Just remove these files using drake_gc() and proceed with your work. cache &lt;- get_cache() drake_gc(cache) B.6.2 Cache customization is limited The storage guide describes how storage works in drake. As explained near the end of that chapter, you can plug custom storr caches into make(). However, non-RDS caches such as storr_dbi() may not work with most forms of parallel computing. The storr::storr_dbi() cache and many others are not thread-safe. Either Set parallelism = &quot;clustermq_staged&quot; in make(), or Set parallelism = &quot;future&quot; with caching = &quot;master&quot; in make(), or Use no parallel computing at all. B.6.3 Runtime predictions In predict_runtime() and rate_limiting_times(), drake only accounts for the targets with logged build times. If some targets have not been timed, drake throws a warning and prints the untimed targets. "]
]
