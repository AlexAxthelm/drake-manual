---
title: "Time: logging, prediction, and strategy"
author: "Will Landau"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{timing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r suppression_timing, echo = F}
suppressMessages(suppressWarnings(library(drake)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE,
  fig.width = 6,
  fig.height = 6,
  fig.align = "center"
)
```

Thanks to [Jasper Clarkberg](https://github.com/dapperjapper), `drake` records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution.

```{r timing_intro}
library(drake)
load_mtcars_example() # Get the code with drake_example("mtcars").
make(my_plan, jobs = 2)

build_times(digits = 8) # From the cache.

# `dplyr`-style `tidyselect` commands
build_times(starts_with("coef"), digits = 8)

build_times(digits = 8, targets_only = TRUE)
```

For `drake` version 4.1.0 and earlier, `build_times()` just measures the elapsed runtime of each command in `my_plan$command`. For later versions, the build times also account for all the internal operations in `drake:::build()`, such as [storage and hashing](https://github.com/ropensci/drake/blob/master/vignettes/storage.Rmd).

# Predict total runtime

Drake uses these times to predict the runtime of the next `make()`. At this moment, everything is up to date in the current example, so the next `make()` should be fast. Here, we only factor in the times of the formal targets in the workflow plan, excluding any imports.

```{r predict_runtime}
config <- drake_config(my_plan, verbose = FALSE)
predict_runtime(config, targets_only = TRUE)
```

Suppose we change a dependency to make some targets out of date. Now, even though, the next `make()` should take a little longer.

```{r changedep_timing}
reg2 <- function(d){
  d$x3 <- d$x ^ 3
  lm(y ~ x3, data = d)
}

predict_runtime(config, targets_only = TRUE)
```

But what if you plan on starting from scratch next time, either after `clean()` or with `make(..., trigger = "always")`?

```{r predict_runtime_scratch}
predict_runtime(config, from_scratch = TRUE, targets_only = TRUE)
```

# Strategize your high-performance computing

Let's say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run `make()`, your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let's write down these known times in seconds.

```{r knowntimes}
known_times <- c(5, rep(7200, nrow(my_plan) - 1))
names(known_times) <- c(file_store("report.md"), my_plan$target[-1])
known_times
```

How many parallel jobs should you use in the next `make()`? The `predict_runtime()` function can help you decide. `predict_runtime(jobs = n)` simulates persistent parallel workers and reports the estimated total runtime of `make(jobs = n)`. (See also `predict_load_balancing()`.)

```{r predictjobs}
time <- c()
for (jobs in 1:12){
  time[jobs] <- predict_runtime(
    config,
    jobs = jobs,
    from_scratch = TRUE,
    known_times = known_times
  )
}
library(ggplot2)
ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) +
  geom_line(aes(x = jobs, y = time, group = group)) +
  scale_y_continuous(breaks = 0:10 * 4, limits = c(0, 29)) +
  theme_gray(16) +
  xlab("jobs argument of make()") +
  ylab("Predicted runtime of make() (hours)")
```

We see serious potential speed gains up to 4 jobs, but beyond that point, we have to double the jobs to shave off another 2 hours. Your choice of `jobs` for `make()` ultimately depends on the runtime you can tolerate and the computing resources at your disposal.

```{r endofline_timing, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```

A final note on predicting runtime: the output of `predict_runtime()` and `predict_load_balancing()` also depends the optional `workers` column of your `drake_plan()`. If you micromanage which workers are allowed to build which targets, you may minimize reads from disk, but you could also slow down your workflow if you are not careful. See the [high-performance computing guide](https://ropensci.github.io/drake/articles/parallelism.html) for more.
