# Time: logging, prediction, and strategy {#time}

```{r suppression_timing, echo = F}
suppressMessages(suppressWarnings(library(drake)))
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c(
    "main", "Makefile", "report.Rmd",
    "shell.sh", "STDIN.o*", "Thumbs.db"
  ),
  recursive = TRUE
)
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE,
  fig.width = 6,
  fig.height = 6,
  fig.align = "center"
)
```

Thanks to [Jasper Clarkberg](https://github.com/dapperjapper), `drake` records how long it takes to build each target. For large projects that take hours or days to run, this feature becomes important for planning and execution.

```{r timing_intro}
library(drake)
load_mtcars_example() # Get the code with drake_example("mtcars").
make(my_plan)

build_times(digits = 8) # From the cache.

## `dplyr`-style `tidyselect` commands
build_times(starts_with("coef"), digits = 8)
```

## Predict total runtime

`drake` uses these times to predict the runtime of the next `make()`. At this moment, everything is up to date in the current example, so the next `make()` should ideally take no time at all (except for preprocessing overhead).

```{r predict_runtime}
config <- drake_config(my_plan, verbose = FALSE)
predict_runtime(config)
```

Suppose we change a dependency to make some targets out of date. Now, the next `make()` should take longer since some targets are out of date.

```{r changedep_timing}
reg2 <- function(d){
  d$x3 <- d$x ^ 3
  lm(y ~ x3, data = d)
}

predict_runtime(config)
```

And what if you plan to delete the cache and build all the targets from scratch?

```{r predict_runtime_scratch}
predict_runtime(config, from_scratch = TRUE)
```

## Strategize your high-performance computing

Let's say you are scaling up your workflow. You just put bigger data and heavier computation in your custom code, and the next time you run `make()`, your targets will take much longer to build. In fact, you estimate that every target except for your R Markdown report will take two hours to complete. Let's write down these known times in seconds.

```{r knowntimes}
known_times <- rep(7200, nrow(my_plan))
names(known_times) <- my_plan$target
known_times["report"] <- 5
known_times
```

How many parallel jobs should you use in the next `make()`? The `predict_runtime()` function can help you decide. `predict_runtime(jobs = n)` simulates persistent parallel workers and reports the estimated total runtime of `make(jobs = n)`. (See also `predict_load_balancing()`.)

```{r predictjobs}
time <- c()
for (jobs in 1:12){
  time[jobs] <- predict_runtime(
    config,
    jobs = jobs,
    from_scratch = TRUE,
    known_times = known_times
  )
}
library(ggplot2)
ggplot(data.frame(time = time / 3600, jobs = ordered(1:12), group = 1)) +
  geom_line(aes(x = jobs, y = time, group = group)) +
  scale_y_continuous(breaks = 0:10 * 4, limits = c(0, 29)) +
  theme_gray(16) +
  xlab("jobs argument of make()") +
  ylab("Predicted runtime of make() (hours)")
```

We see serious potential speed gains up to 4 jobs, but beyond that point, we have to double the jobs to shave off another 2 hours. Your choice of `jobs` for `make()` ultimately depends on the runtime you can tolerate and the computing resources at your disposal.

A final note on predicting runtime: the output of `predict_runtime()` and `predict_load_balancing()` also depends the optional `workers` column of your `drake_plan()`. If you micromanage which workers are allowed to build which targets, you may minimize reads from disk, but you could also slow down your workflow if you are not careful. See the [high-performance computing guide](#hpc) for more.

```{r endofline_timing, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c(
    "main", "*.lock", "Makefile",
    "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"
  ),
  recursive = TRUE
)
```
