# Debugging and testing drake projects {#debug}

```{r debugstart, echo = F}
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(magrittr)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
```

This chapter is a guide to debugging and testing `drake` projects. Please also see the [compendium of cautionary notes](#caution), which addresses `drake`'s known edge cases, pitfalls, and weaknesses that may or may not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the [issue tracker](https://github.com/ropensci/drake/issues), where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version.

## The configuration list

Most of `drake`'s functions rely on a central `config` list. An understanding of `config` will help you grasp the internals. `make()` and `drake_config()` both return the `config` list. Unlike `make()`, `drake_config()`'s return value is visible, and its only purpose is to construct your `config`.

```{r debugconfig}
load_mtcars_example() # Get the code with drake_example("mtcars").
config <- drake_config(my_plan)

sort(names(config))
```

The fields of `config` mostly arguments to `make()` and are documented there. The rest of the fields are as follows.

- `graph`: An [igraph](https://github.com/igraph/rigraph) object with the directed acyclic graph (DAG) of the workflow.
- `inventory`: A running list of the cached objects in each `storr` namespace. Maintaining this list helps avoid repeated calls to `config$cache$list()`, which increases speed.
- `long_hash_algo`: Name of the long hash algorithm used throughout `make()`. Used to generate hash keys that *will not* become the names of files. See the [custom storage guide](#store) for details.
- `seed`: The random number generator seed taken from the user's R session. Each target is built reproducibly using a deterministic function of this seed, and the build does not change the seed outside the scope of the target's command.
- `short_hash_algo`: Name of the short hash algorithm used throughout `make()`. Used to generate hash keys that could become names of files. See the [custom storage guide](#store) for details.

Early in `make()`, the `config` list is stored in the cache. You can retrieve it with

```{r readconfig, eval = FALSE}
read_drake_config()
```

and you can access parts of it with some companion functions.

```{r readcompanions, eval = FALSE}
read_drake_graph()
read_drake_plan()
```

## Plan your work.

### Workflow plan data frames

The workflow plan data frame is your responsibility, and it takes effort and care. Fortunately, functions in `drake` can help. You can check the plan for formatting issues, missing input files, etc. with the `check_plan()` function.

```{r checkdebug}
load_mtcars_example() # Get the code with drake_example("mtcars").
my_plan

check_plan(my_plan) # No issues.
```

### Visualize your workflow.

After quality-checking your plan, you should check that you understand how the steps of your workflow are interconnected. The web of dependencies affects which targets are built and which ones are skipped during `make()`.

```{r demoplotgraphdebug}
# Hover, click, drag, zoom, and pan. See args 'from' and 'to'.
config <- drake_config(my_plan)
vis_drake_graph(config, width = "100%", height = "500px")
```

See the [visualization chapter](#vis) to learn more about how graphing can help (for example, how to visualize small subgraphs). If you want to take control of your own [visNetwork graph](http://datastorm-open.github.io/visNetwork/), use the `drake_graph_info()` function to get data frames of nodes, edges, and legend nodes.


### Check dependency relationships.

Programmatically, several functions can help you check immediate dependencies.

```{r checkdepsdebug}
deps_code(reg2)

# knitr_in() makes sure your target depends on `report.Rmd`
# and any dependencies loaded with loadd() and readd()
# in the report's active code chunks.
deps_code(my_plan$command[1])

deps_code(my_plan$command[nrow(my_plan)])
```

`drake` takes special precautions so that a target/import does not depend on itself. For example, `deps_code(f)` might return `"f"` if `f()` is a recursive function, but `make()` just ignores this conflict and runs as expected. In other words, `make()` automatically removes all self-referential loops in the dependency network.

List all the reproducibly-tracked objects and files, including imports and targets.

```{r trackeddebug}
config <- drake_config(my_plan)
tracked(config)
```

### Outdated, up to date, and missing items

`missed()` reports import dependencies missing from your environment

```{r misseddebug}
config <- drake_config(my_plan, verbose = FALSE)
missed(config) # Nothing is missing right now.
```

`outdated()` reports any targets that are outdated, plus any downstream targets that depend on them.

```{r outdateddebug}
outdated(config)
```

To get a rough idea of why a target is out of date, you can use `dependency_profile()`. It will tell you if any of the following changed since the last `make()`:

- The command in the workflow plan data frame.
- At least one non-file dependency. (For this, the imports have to be up to date and cached, either with `make()`, `make(skip_targets = TRUE)`, `outdated()`, or similar.)
- At least one input file declared with `file_in()` or `knitr_in()`.
- At least one output file declared with `file_out()`.

```{r depprofiledebug}
load_mtcars_example() # Get the code with drake_example("mtcars").
config <- make(my_plan, verbose = FALSE)
# regression2_small should be up to date.
dependency_profile(target = regression2_small, config = config)
# Change an in-memory dependency and update the cache.
reg2 <- function(d) {
  d$x3 <- d$x ^ 3
  lm(y ~ x3, data = d)
}
make(skip_targets = TRUE)
# The change is detected.
dependency_profile(target = regression2_small, config = config)
```

In `drake`, the "kernel" of a target or import is the piece of the output that is reproducibly tracked. For ordinary R objects, the kernel is just the object itself. For custom external files, it is a separate hash. But for functions, the kernel is the deparsed body of the function, together with the dependency hash if the function is imported (see `drake:::store_function()`).

The internal functions `drake:::meta()` and `drake:::meta_list()` compute the metadata on each target that `drake` uses to decide which targets to build and which to skip (via `drake:::should_build_target()`). Then, after the target/import is processed, `drake:::finish_meta()` updates the metadata (except for the `$missing` element) before it is cached. See `diagnose()` to read available metadata, along with any errors, warnings, and messages generated during the build.

```{r readdrakemeta}
str(diagnose(small))

str(diagnose("\"report.md\""))
```

If your target's last build succeeded, then `diagnose(your_target)` has the most current information from that build. But if your target failed, then only `diagnose(your_target)$error`, `diagnose(your_target)$warnings`, and `diagnose(your_target)$messages` correspond to the failure, and all the other metadata correspond to the last build that completed without an error.

## Skipping imports

It may be time-consuming to process all the imported objects and files, so for testing purposes, if you processed the imports once, you can choose to jump straight to the targets. This is not recommended for the production version

```{r skipimports}
clean(verbose = FALSE)

make(my_plan, skip_imports = TRUE)
```

## Impose timeouts and retries

See the `timeout`, `cpu`, `elapsed`, and `retries` argument to `make()`.

```{r timeoutretry}
clean(verbose = FALSE)
f <- function(...){
  Sys.sleep(1)
}
debug_plan <- drake_plan(x = 1, y = f(x))
debug_plan

withr::with_message_sink(
  stdout(),
  make(debug_plan, timeout = 1e-3, retries = 2)
)
```

To tailor these settings to each individual target, create new `timeout`, `cpu`, `elapsed`, or `retries` columns in your workflow plan. These columns override the analogous arguments to `make()`.

```{r timeoutretry2}
clean(verbose = FALSE)
debug_plan$timeout <- c(1e-3, 2e-3)
debug_plan$retries <- 1:2

debug_plan

withr::with_message_sink(
  new = stdout(),
  make(debug_plan, timeout = Inf, retries = 0)
)
```

## Diagnose failures.

`drake` records diagnostic metadata on all your targets, including the latest errors, warnings, messages, and other bits of context.

```{r diagnosedebug}
diagnose(verbose = FALSE) # Targets with available metadata.

f <- function(x){
  if (x < 0){
    stop("`x` cannot be negative.")
  }
  x
}
bad_plan <- drake_plan(
  a = 12,
  b = -a,
  my_target = f(b)
)

bad_plan

withr::with_message_sink(
  new = stdout(),
  make(bad_plan)
)

failed(verbose = FALSE) # from the last make() only

# See also warnings and messages.
error <- diagnose(my_target, verbose = FALSE)$error

error$message

error$call

error$calls # View the traceback.
```

To figure out what went wrong, you could try to build the failed target interactively. To do that, simply call `drake_build()`. This function first calls `loadd(deps = TRUE)` to load any missing dependencies (see the `replace` argument here) and then builds your target.

```{r loaddeps}
# Pretend we just opened a new R session.
library(drake)

# Unloads target `b`.
config <- drake_config(plan = bad_plan)

# my_target depends on b.
"b" %in% ls()

# Try to build my_target until the error is fixed.
# Skip all that pesky work checking dependencies.
drake_build(my_target, config = config)

# The target failed, but the dependency was loaded.
"b" %in% ls()

# What was `b` again?
b

# How was `b` used?
diagnose(my_target)$message

diagnose(my_target)$call

f

# Aha! The error was in f(). Let's fix it and try again.
f <- function(x){
  x <- abs(x)
  if (x < 0){
    stop("`x` cannot be negative.")
  }
  x
}

# Now it works!
# Since you called make() previously, `config` is read from the cache
# if you do not supply it.
drake_build(my_target)

readd(my_target)
```

### Tidy evaluation: a caveat to diagnosing interactively

Running commands in your R console is not always exactly like running them with `make()`. That's because `make()` uses tidy evaluation as implemented in the [`rlang` package](https://github.com/tidyverse/rlang).

```{r demotidyeval06}
# This workflow plan uses rlang's quasiquotation operator `!!`.
my_plan <- drake_plan(list = c(
  little_b = "\"b\"",
  letter = "!!little_b"
))
my_plan
make(my_plan)
readd(letter)
```


## Debrief a build session.

After your project is at least somewhat built, you can inspect and read your results from the cache.

```{r debriefdebug}
make(my_plan, verbose = FALSE)

# drake_session(verbose = FALSE) # Prints the sessionInfo() of the last make(). # nolint

cached(verbose = FALSE)

built(verbose = FALSE)

imported(verbose = FALSE)

loadd(little_b, verbose = FALSE)

little_b

readd(letter, verbose = FALSE)

progress(verbose = FALSE)

in_progress(verbose = FALSE) # Unfinished targets
```

There are functions to help you locate the project's cache.

```{r finddebug, eval = FALSE}
find_project()
find_cache()
```

For more information on the cache, see the [chapter on storage and caches](#store).

## Start tinkering.

The `load_mtcars_example()` function loads the [mtcars example](https://github.com/wlandau/drake-examples/tree/master/mtcars) from `drake_example("mtcars")` right into your workspace. The workflow plan data frame, workspace, and import files are set up for you. Only `make(my_plan)` is left to you.

`drake` has [many more built-in examples](https://github.com/wlandau/drake-examples). To see your choices, use

```{r examplesdrakedebug}
drake_examples()
```

To write the files for an example, use `drake_example()`.

```{r examplesdrake, eval = FALSE}
drake_example("main")
drake_example("packages")
drake_example("gsp")
```

```{r rmfiles_debug, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
```
