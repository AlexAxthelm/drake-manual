# Debugging and testing drake projects {#debug}

```{r debugstart, echo = F}
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(tidyverse)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
```

This chapter is a guide to debugging and testing `drake` projects.

## Dependencies

`drake` automatically detects dependency relationships among your targets and imports. While this is convenient most of the time, it can lead to some pitfalls. This section describes techniques to understand you project's dependency structure and diagnose and debug issues.

### Visualize your dependency graph.

To avoid frustration early on, please use [`drake`'s dependency graph visualizations](#visuals) to see how the steps of your workflow fit together. `drake` resolves the dependency relationships in the graph by analyzing the code in your commands and the functions in your environment.

```{r demoplotgraphdebug}
load_mtcars_example()
config <- drake_config(my_plan)
# Hover, click, drag, zoom, and pan. See args 'from' and 'to'.
vis_drake_graph(config, width = "100%", height = "500px")
```

### Check specific dependency information.

With the `deps_code()` function, you can see for yourself how `drake` detects first-order dependencies from code.

```{r checkdepsdebug}
print(simulate)

deps_code(simulate)

# knitr_in() makes sure your target depends on `report.Rmd`
# and any dependencies loaded with loadd() and readd()
# in the report's active code chunks.
cat(my_plan$command[1])

deps_code(my_plan$command[1])

cat(my_plan$command[nrow(my_plan)])

deps_code(my_plan$command[nrow(my_plan)])
```

With `deps_target()`, you can see the dependencies that `drake` has already detected for your targets and imports.

```{r checkdepsdebug2}
deps_target("simulate", config)

deps_target("small", config)

deps_target("report", config)
```

And with `tracked()`, you can list all the reproducibly tracked objects and files.

```{r trackeddebug}
tracked(config)
```

### Outdated targets and missing dependencies

`missed()` shows any imports missing from your environment

```{r misseddebug}
missed(config) # Nothing is missing right now.
```

`outdated()` reports any targets that are outdated.

```{r outdateddebug}
outdated(config)

make(my_plan)

outdated(config)
```

### But *why* are my targets out of date?

`drake` has the option to produce a cache log with the fingerprint of every target and import.

```{r cachelog}
head(drake_cache_log())
```

We highly recommend that you automatically produce a cache log file on every `make()` and put it under [version control](https://github.com) with the rest of your project.

```{r cachelog2}
make(my_plan, cache_log_file = "cache_log.txt")

read.table("cache_log.txt", nrows = 6, header = TRUE)
```

Suppose we go back and add input checking to one of our functions.

```{r changerandomrowsdebug}
print(random_rows)

random_rows <- function(data, n){
  stopifnot(n > 0)
  data[sample.int(n = nrow(data), size = n, replace = TRUE), ]
}
```

Then, we forget to run `make()` again, and we leave the the project for several months. When we come back, all our targets are suddenly out of date.

```{r whyoutdated}
outdated(config)
```

At first, we may not know why all our targets are outdated. But we can generate another cache log and check any hashes that changed. Our call to `outdated()` already re-cached the imports, so any changed imports will show up in the new log file.

```{r whyoutdated2}
drake_cache_log_file(file = "cache_log2.txt")

system2("diff", "cache_log.txt cache_log2.txt", stdout = TRUE) %>%
  cat(sep = "\n")
```

Now, we see that `random_rows()` has changed since last time, and we have a new dependency `stopifnot()`. `simulate()` shows up in the changes too, but only because `random_rows()` is nested in the body of `simulate()`. If we revert `random_rows()` to its original state, all our targets are up to date again.

```{r revertrandomrows}
random_rows <- function(data, n){
  data[sample.int(n = nrow(data), size = n, replace = TRUE), ]
}

outdated(config)

drake_cache_log_file(file = "cache_log3.txt")

system2("diff", "cache_log.txt cache_log3.txt", stdout = TRUE)
```


## Diagnose failures.

`drake` records diagnostic metadata on all your targets, including the latest errors, warnings, messages, and other bits of context.

```{r diagnosedebug}
f <- function(x){
  if (x < 0){
    stop("`x` cannot be negative.")
  }
  x
}
bad_plan <- drake_plan(
  a = 12,
  b = -a,
  my_target = f(b)
)

bad_plan

withr::with_message_sink(
  new = stdout(),
  make(bad_plan)
)

failed(verbose = FALSE) # from the last make() only

# See also warnings and messages.
error <- diagnose(my_target, verbose = FALSE)$error

error$message

error$call

str(error$calls) # View the traceback.
```

To figure out what went wrong, you could try to build the failed target interactively. To do that, simply call `drake_build()` or `drake_debug()`. These functions first call `loadd(deps = TRUE)` to load any missing dependencies (see the `replace` argument here) and then build your target. `drake_build()` simply runs the command, and `drake_debug()` runs the command in debug mode using `debugonce()`.

```{r loaddeps}
# Pretend we just opened a new R session.
library(drake)

# Unloads target `b`.
config <- drake_config(plan = bad_plan)

# my_target depends on b.
"b" %in% ls()

# Try to build my_target until the error is fixed.
# Skip all that pesky work checking dependencies.
drake_build(my_target, config = config) # See also drake_debug().

# The target failed, but the dependency was loaded.
"b" %in% ls()

# What was `b` again?
b

# How was `b` used?
diagnose(my_target)$message

diagnose(my_target)$call

f

# Aha! The error was in f(). Let's fix it and try again.
f <- function(x){
  x <- abs(x)
  if (x < 0){
    stop("`x` cannot be negative.")
  }
  x
}

# Now it works!
# Since you called make() previously, `config` is read from the cache
# if you do not supply it.
drake_build(my_target) # See also drake_debug().

readd(my_target)
```

## Timeouts and retries

See the `timeout`, `cpu`, `elapsed`, and `retries` argument to `make()`.

```{r timeoutretry}
clean(verbose = FALSE)
f <- function(...){
  Sys.sleep(1)
}
debug_plan <- drake_plan(x = 1, y = f(x))
debug_plan

withr::with_message_sink(
  stdout(),
  make(debug_plan, timeout = 1e-3, retries = 2)
)
```

To tailor these settings to each individual target, create new `timeout`, `cpu`, `elapsed`, or `retries` columns in your workflow plan. These columns override the analogous arguments to `make()`.

```{r timeoutretry2}
clean(verbose = FALSE)
debug_plan$timeout <- c(1e-3, 2e-3)
debug_plan$retries <- 1:2

debug_plan

withr::with_message_sink(
  new = stdout(),
  make(debug_plan, timeout = Inf, retries = 0)
)
```

## Blitz mode

In blitz mode, `drake` acts as a job scheduler without watching dependencies. In other words, `make(parallelism = "blitz")` always runs all the targets, and computational overhead is dramatically reduced. Because it helps deploy targets much faster, blitz mode may be a nice way to test a workflow before running it in production. Read more [here](https://ropenscilabs.github.io/drake-manual/hpc.html#blitz-mode).

## More help

Please also see the [compendium of cautionary notes](#caution), which addresses `drake`'s known edge cases, pitfalls, and weaknesses that may or may not be fixed in future releases. For the most up-to-date information on unhandled edge cases, please visit the [issue tracker](https://github.com/ropensci/drake/issues), where you can submit your own bug reports as well. Be sure to search the closed issues too, especially if you are not using the most up-to-date development version.

```{r endofline_debug, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c(
    "main", "Makefile", "report.Rmd", "raw_data.xlsx",
    "shell.sh", "STDIN.o*", "Thumbs.db", "*.txt"
  ),
  recursive = TRUE
)
```
