# Workflow data frames {#workflow-df}

This chapter provides a systematic introduction to workflow data frames, how 
you can create - modify and execute them. We'll therefore introduce a minimal 
example.

## What is a workflow plan data frame?

The workflow plan data frame is one of the main pillars of drake and the main 
input to `make()`. 
The purpose of a plan is to declare the data objects and files you are going to 
produce later on. We refer to these as *targets*. You can declare targets in 
any order and drake will figure out the order of execution. You can construct 
one conveniently with `drake_plan()`. Below, we create a plan with two targets:
```{r}
library(drake)
plan <- drake_plan(
  uniform = runif(10), 
  normal  = rnorm(50)
)
plan
```
In plain English, we want to obtain 10 values from the uniform distribution and
50 from the normal distribution. Let's do it:
```{r}
make(plan)
```

This is it. Now you might wonder where these two targets are stored. They
live in the drake cache. To make them available in your R session, you can 
either load them into the global environment with `loadd()` (which is called
for its side effects) or with `readd()` (which returns the target itself and has
no side effects.). That means that you can treat the return value of `readd()`
as any other R object.
```{r}
mean(readd(uniform))
```

Note that creating the plan does not run any of the code in the `command` 
column, that will only happen when you call `make()` on the plan.

A workflow plan data frame is what the name says, it is a data frame. 
```{r}
class(plan)
```
There is no magic. A workflow plan has two mandatory columns: `command`, which 
contains R code (as plain text) and `target` which stores the name of the 
targets. Hence, every row in the workflow plan corresponds to a target. 

Optionally, you can add a column `trigger` that determines the condition under 
which a target gets build (e.g. `"always"` to build the target with every 
`make()` call).  You can learn more about triggers with `?triggers`. 
You can also add arbitrary other columns (such as a grouping column to group 
commands) exactly the way you manipulate data frames.
```{r}
plan$my_col <- "my_custom_column"
plan
```

## Why making a plan?

Ok, fine, you might think. I can also run `runif(10)` in my console, why should
I use this complicated workflow? In brief because if your project grows in size 
and complexity, it's hard to keep in mind which parts of a project get outdated
when you modify an input or code. Let drake take care of it, so you can focus 
on the content of your project. Whenever you call `make()`, drake will only
re-build targets that got outdated since the last `make()`. 
```{r}
make(plan)
```

There are a whole lot of other advantages associated 
with using drake, you can check out [Visualization with drake] or 
[High-performance computing with drake] for more information.


## Why putting everything into a data frame?

Now that we've explained why it's a good idea to make a plan, you may wonder 
why you should use drake, as there are similar tools like GNU Make or the 
R package [remake](https://github.com/richfitz/remake)
that let you specify targets and their build commands and 
dependencies.^[In those tools, the equivalent of a plan is called `Makefile`.] 

drake is an R focussed implementation of Make - and 
the data frame was chosen as the data structure to represent
workflow plans. Data frames are fist-class citizen in R. They are R objects you 
can operate on using the power of R. 
[remake](https://github.com/richfitz/remake), which was a major 
source of inspiration for drake, uses a Makefile - just as GNU Make. This meant
that the workflow plan could not be manipulated programmatically and the whole 
plan had to be written by hand. For example, 
you could not generate many targets with `combn()` or `expand.grid()` (as it was 
done in the [GSP example](#example-gsp)), 
use wildcards (see [Wildcard templating: a tool for generating lots of targets]), filter for certain targets or 
triggers and the like. In the data frame format, you can easily split one
plan up into multiple, bind plans together with `bind_plans()` etc. 

In addition, drake uses static code analysis to detect dependencies among 
targets which GNU Make is not capable of. In GNU Make, 
all dependencies need to be delared manually, along with the build command.

## Wildcard templating: a tool for generating lots of targets

One advantage of using data frames to represent workflow plans becomes evident 
with the introduction of wildcards. Wildcards are essentially placeholders 
for *something yet to come*. 
This *something* can be multiple things - and 
drake will take care of creating all combinations of these *things* and your 
workflow plan. We'll look at the following example task:

* Sample values from two different distributions.
* Draw a large and a small sample from them.
* Calculate the mean and the standard deviation from these samples.

There are two ways to access the power of wildcards. We introduce both here 
but we encourage to use the first one because it is more generic and concise
in terms of syntax. A more detailed comparison follows after the aforementioned
example task.

### With the generic `evaluate_plan()`

- Set up a workflow plan that contains data you want to use for your task.
  This may contain more than one target, we decided to go with 
  two.^[This example is in fact only 
  interesting if there is more than one target in the `datasets` workflow plan.]
```{r}
datasets <- drake_plan(
  uniform = runif(50),
  normal  = rnorm(50)
)
```

- Set up another workflow plan that transforms the data. Use the placeholder
  (i.e. the wildcard) and think through the analysis as if there was just one 
  data input. I.e, we have two analysis tasks: sampling six and sampling ten
  observations from a data set. We denote data wildcards with `dataset__`.

```{r}
sampled <- drake_plan(
  small = sample(generic_wild_card__, 6),
  large = sample(generic_wild_card__, 10)
)
```

- Next, you take the plan with the wildcards, define the wildcard string to be 
  "generic_wild_card__" and set `values = datasets$target` to substitute the 
  wildcard with the targets `uniform` and `normal` from your data set workflow 
  data frame. In the output below we note that drake combines two data sets with
  two transformations, returning all four combinations of data and 
  transformations.
```{r}
sampled_evaluated <- evaluate_plan(
  sampled, wildcard = "generic_wild_card__", values = datasets$target
)

sampled_evaluated
```

- In a similar way, you can extend the plan just created using another 
  `evaluate_plan()`. You can use a different wildcard string but that's not 
  necessary. Let's assume that after sampling, you want to 
  find out more about the distribution of the data and decide to calculate some
  sample moments. Again, abstract from the number of samples you drew from the 
  data, and think in terms of one result of your sampling. For every result of 
  your sampling, you want to calculate the
  mean and the standard deviation, which means you have two goals in your 
  next step. First we create the plan for this step:

```{r}
moments <- drake_plan(
  mean = mean(generic_wild_card__),
  sd   = sd(generic_wild_card__)
)
```

- Then, we again replace our wildcard with the results from the previous step, 
  using `evaluate_plan()`.

```{r}
moments_evaluated <- evaluate_plan(
  moments, wildcard = "generic_wild_card__", values = sampled_evaluated$target
)
moments_evaluated
```

To have an executable plan, we need to bind the intermediate output with the
final plan:
```{r}
bind_plans(
  datasets,
  sampled_evaluated,
  moments_evaluated
)
```

`evaluate_plan()` offers a rather generic framework to work with wildcards. You 
can cascade multiple calls of `evaluate_plan()` as you want and you define 
a custom wildcard string. But be careful: `plan_evaluate()` 
will replace **all** occurrences of the wildcard in your workflow plan, so make sure
you choose a unique one. The convention is to use a string ending with two 
underscores to indicate that this is a reserved string.

### With `plan_analyses()` / `plan_summaries()`

For the sake of the argument, we do everything again.

- Set up a workflow plan that contains data you want to use for your analysis.
  This may contain more than one target.^[This example is in fact only 
  interesting if there is more than one target in the `datasets` workflow plan.]

```{r}
datasets <- drake_plan(
  uniform = runif(50),
  normal  = rnorm(50)
)
```

- Set up another workflow plan that transforms the data. Use the placeholder
  (i.e. the wildcard) and think through the analysis as if there was just one 
  data target. I.e, we have two analysis tasks: sampling six and sampling ten
  observations from a data set. We denote data wildcards with `dataset__`.
```{r}
methods <- drake_plan(
  small_sample = sample(dataset__, 6),
  large_sample = sample(dataset__, 10)
)
```

- Next, you combine the two workflow plans into one by substituting in your 
  data with `plan_analyses()`. Below, we note that drake combines two data sets 
  with two transformations, returning all four combinations of data and 
  transformations.
```{r}
(analysis <- plan_analyses(methods, datasets = datasets))
```

- In a similar way, you can extend the plan just created with `plan_summaries()`, 
  using the wildcard `analysis__`. Let's assume that after sampling, you want to 
  find out more about the distribution of the data and decide to calculate some
  sample moments. Again, abstract from the number of samples you drew from the 
  data, and think in terms of one result of your sampling. We call this result 
  analysis. For every analysis result, you want to calculate the
  mean and the standard deviation, which means you have two goals in your 
  next step. First we create the plan for this step:
```{r}
post_analysis <- drake_plan(
  mean = mean(analysis__),
  sd   = sd(analysis__)
)
```
- Then, we again replace our wildcard with the results from the previous step, 
  this time using `plan_analyses()`. Use the option `gather` to indicate whether
  or not a separate target with all the final targets (`mean_*` and `sd_*`) 
  should be added to the workflow data frame.
```{r}
plan_summaries(
  post_analysis, analysis, datasets, gather = NULL
)
```
Before you can call `make()` on this plan, you need to bind it with 
`datasets` and `analyses` using `bind_plans()` or `rbind`.


### Comparison of `evaluate_plan()` and `plan_analyses()` / `plan_summaries()`

As pointed out before, `evaluate_plan()` is more generic, in particular because
you can chain as many calls as you want. It does not require your task
to be of the form data-analysis-summary and you don't have to use 
the designated wildcards for the steps.

## Other tools to manipulate plans

There are a few more functions that are worth mentioning here: 

- `reduce_plan()` and `gather_plan()`: Reduce several targets down to one.
- `expand_plan()`: Create replicates of targets.