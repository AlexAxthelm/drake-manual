# General best practices for drake projects {#best-practices}

```{r bestpracticesstart, echo = F}
suppressMessages(suppressWarnings(library(drake)))
suppressMessages(suppressWarnings(library(magrittr)))
suppressMessages(suppressWarnings(library(curl)))
suppressMessages(suppressWarnings(library(httr)))
suppressMessages(suppressWarnings(library(R.utils)))
clean(destroy = TRUE, verbose = FALSE)
unlink(c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db"))
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  warning = TRUE
)
pkgconfig::set_config("drake::strings_in_dots" = "literals")
tmp <- file.create("data.csv")
```

This chapter describes general best practices for creating, configuring, and running `drake` projects. It answers frequently asked questions and clears up common misconceptions, and it will continuously develop in response to community feedback.

## How to organize your files

### Examples

For examples of how to structure your code files, see the beginner oriented example projects:

- [mtcars](https://github.com/ropensci/drake/tree/master/inst/examples/mtcars)
- [gsp](https://github.com/ropensci/drake/tree/master/inst/examples/gsp)
- [packages](https://github.com/ropensci/drake/tree/master/inst/examples/packages)

Write the code directly with the `drake_example()` function.

```{r exampledrakewritingbestpractices, eval = FALSE}
drake_example("mtcars")
drake_example("gsp")
drake_example("packages")
``` 

In practice, you do not need to organize your files the way the examples do, but it does happen to be a reasonable way of doing things.

### Where do you put your code?

It is best to write your code as a bunch of functions. You can save those functions in R scripts and then `source()` them before doing anything else.

```{r sourcefunctions, eval = FALSE}
## Load functions get_data(), analyze_data, and summarize_results()
source("my_functions.R")
```

```{r sourcefunctionsbk, echo = FALSE}
get_data <- analyze_data <- summarize_results <- function(){}
```

Then, set up your workflow plan data frame.

```{r storecode1}
good_plan <- drake_plan(
  my_data = get_data(file_in("data.csv")), # External files need to be in commands explicitly. # nolint
  my_analysis = analyze_data(my_data),
  my_summaries = summarize_results(my_data, my_analysis)
)

good_plan
```

`Drake` knows that `my_analysis` depends on `my_data` because `my_data` is an argument to `analyze_data()`, which is part of the command for `my_analysis`.

```{r visgood}
config <- drake_config(good_plan)
vis_drake_graph(config)
```

Now, you can call `make()` to build the targets.

```{r makestorecode, eval = FALSE}
make(good_plan)
```

If your commands are really long, just put them in larger functions. `Drake` analyzes imported functions for non-file dependencies.

### Remember: your commands are code chunks, not R scripts

Some people are accustomed to dividing their work into R scripts and then calling `source()` to run each step of the analysis. For example you might have the following files.

- `get_data.R`
- `analyze_data.R`
- `summarize_results.R`

If you migrate to `drake`, you may be tempted to set up a workflow plan like this.

```{r badsource}
bad_plan <- drake_plan(
  my_data = source(file_in("get_data.R")),
  my_analysis = source(file_in("analyze_data.R")),
  my_summaries = source(file_in("summarize_data.R"))
)

bad_plan
```

But now, the dependency structure of your work is broken. Your R script files are dependencies, but since `my_data` is not mentioned in a function or command, `drake` does not know that `my_analysis` depends on it.

```{r scripts, echo = FALSE}
files <- c("get_data.R", "analyze_data.R", "summarize_data.R")
lapply(files, file.create)
```

```{r visbad}
config <- drake_config(bad_plan)
vis_drake_graph(config)
```

```{r scripts2, echo = FALSE}
lapply(files, file.remove)
```

Dangers:

1. In the first `make(bad_plan, jobs = 2)`, `drake` will try to build `my_data` and `my_analysis` at the same time even though `my_data` must finish before `my_analysis` begins.
2. `Drake` is oblivious to `data.csv` since it is not explicitly mentioned in a workflow plan command. So when `data.csv` changes, `make(bad_plan)` will not rebuild `my_data`.
3. `my_analysis` will not update when `my_data` changes.
4. The return value of `source()` is formatted counter-intuitively. If `source(file_in("get_data.R"))` is the command for `my_data`, then `my_data` will always be a list with elements `"value"` and `"visible"`. In other words, `source(file_in("get_data.R"))$value` is really what you would want.

In addition, this `source()`-based approach is simply inconvenient. `Drake` rebuilds `my_data` every time `get_data.R` changes, even when those changes are just extra comments or blank lines. On the other hand, in the previous plan that uses `my_data = get_data()`, `drake` does not trigger rebuilds when comments or whitespace in `get_data()` are modified. `Drake` is R-focused, not file-focused. If you embrace this viewpoint, your work will be easier.

### File output targets

In your plan, the `file_out()` function tells `drake` that your target is an external file rather than an ordinary R object.

```{r fileplan}
plan <- drake_plan(
  writeLines(text = letters[1:6], con = file_out("file.txt"))
)
plan
```

Now, `make()` knows to expect a file called `file.txt`.

```{r fileplan2}
make(plan)
```

And if you manually mangle `file.txt` by accident, `make()` restores it to its reproducible state.

```{r fileplan3}
writeLines(text = "123", con = file_out("file.txt"))
make(plan)
make(plan)
```

But just because your command produces files does not mean you need to track them.

```{r fileplan5, eval = FALSE}
plan <- drake_plan(real_output = long_job())
make(plan)
list.files()
### [1] "date-time.log" "error.log" "console.log" 
```

These log files probably have nothing to do with the objectives of your research. If that is the case, you can safely ignore them with no loss of reproducibility.

Generally speaking, `drake` was designed to be as R-focused as possible, which means you should treat targets as R objects most of the time. External files are really an afterthought. This might be an uncomfortable notion. You may be accustomed to generating lots of files.

```{r fileplan6}
drake_plan(
  write.csv(tabulate_results(data), file_out("results.csv")),
  ggsave(my_ggplot(data), file = file_out("plot.pdf"))
)
```

But R object targets are much more convenient in the long run. If you really want to display them, consolidate them all in an R Markdown report at the end of the pipeline to reduce the number of output files.

```{r rmdready, echo = FALSE}
invisible(file.create("report.Rmd"))
```

```{r fileplan7}
drake_plan(
  tab_results = tabulate_results(data),
  data_plot = my_ggplot(data),
  rmarkdown::render(
    knitr_in("report.Rmd"), # References tab_results` and data_plot in active code chunks using loadd() or readd().
    output_file = file_out("report.html")
  )
)
```

But sometimes, you may unavoidably have multiple important files for each target. For example, maybe you work with spatial data and use the [`sf` package](https://github.com/r-spatial/sf). 

```{r sf, eval = FALSE}
st_write(spatial_data, "spatial_data.shp", driver = "ESRI Shapefile")

### Creates:
###   - "spatial_data.shp"
###   - "spatial_data.shx"
###   - "spatial_data.prj"
###   - "spatial_data.dbf"
```

Later targets may depend on many of these files, but there can only be one output file per target. So what do we do? Spoof `drake`: pick one file to be the real target, and let the other files be targets that depend on it.

```{r sf2}
library(drake)
library(magrittr)
drake_plan(
  st_write(spatial_data, file_out("spatial_data.shp"), driver = "ESRI Shapefile"),
  c(file_out("spatial_data.EXTN"), file_in("spatial_data.shp")),
  out = process_shx(file_in("spatial_data.EXTN"))
) %>%
  evaluate_plan(wildcard = "EXTN", values = c("shx", "prj", "dbj"))
```

But be warned: If you manually mangle `spatial_data.shx`, `spatial_data.prj` or `spatial_data.dbj` later on, `make()` will not restore them. Having lots of output files can also slow down the construction of workflow plan data frames (ref: [issue 366](https://github.com/ropensci/drake/issues/366)).

It may actually be safer to divide the workflow into two pipelines with separate caches and separate plans. That way, all the output files from the first pipeline, tracked or not tracked, become inputs to the second pipeline. An overarching R script can run both pipelines back to back.

```{r separate, eval = FALSE}
plan1 <- drake_plan(
  st_write(spatial_data, file_out("spatial_data.shp"), driver = "ESRI Shapefile")
)
plan2 <- drake_plan(out = process_shx(file_in("spatial_data.EXTN")))%>%
  evaluate_plan(wildcard = "EXTN", values = c("shx", "prj", "dbj"))
cache1 <- new_cache(path = "cache1")
cache2 <- new_cache(path = "cache2")
make(plan1, cache = cache1)
make(plan2, cache = cache2)
```

See the [storage guide](#store) for more on caching, particularly functions `get_cache()` and `this_cache()`.

### R Markdown and knitr reports

For a serious project, you should use `drake`'s `make()` function outside `knitr`. In other words, you should treat R Markdown reports and other `knitr` documents as targets and imports, not as a way to run `make()`. Viewed as targets, `drake` makes special exceptions for R Markdown reports and other [knitr](https://github.com/yihui/knitr) reports such as `*.Rmd` and `*.Rnw` files. Not every `drake` project needs them, but it is good practice to use them to summarize the final results of a project once all the other targets have already been built. The mtcars example, for instance, has an R Markdown report. `report.Rmd` is knitted to build `report.md`, which summarizes the final results.

To see where `report.md` will be built, look to the right of the dependency graph.

```{r revisitmtcarsgraph}
load_mtcars_example(overwrite = TRUE) # Get the code with drake_example("mtcars").
config <- drake_config(my_plan)
vis_drake_graph(config)
```

`Drake` treats [knitr](https://github.com/yihui/knitr) report as a special cases. Whenever `drake` sees `knit()` or `render()` ([rmarkdown](https://github.com/rstudio/rmarkdown)) mentioned in a command, it dives into the source file to look for dependencies. Consider `report.Rmd`, which you can view [here](https://github.com/ropensci/drake/blob/master/inst/examples/mtcars/report.Rmd). When `drake` sees `readd(small)` in an active code chunk, it knows [report.Rmd](https://github.com/ropensci/drake/blob/master/inst/examples/mtcars/report.Rmd) depends on the target called `small`, and it draws the appropriate arrow in the dependency graph above. And if `small` ever changes, `make(my_plan)` will re-process [report.Rmd](https://github.com/ropensci/drake/blob/master/inst/examples/mtcars/report.Rmd) to produce the target file `report.md`.

[knitr](https://github.com/yihui/knitr) reports are the only kind of file that `drake` analyzes for dependencies. It does not give R scripts the same special treatment.

### Workflows as R packages

The R package structure is a great way to organize the files of your project. Writing your own package to contain your data science workflow is a good idea, but you will need to

1. Use `expose_imports()` to properly account for all your nested function dependencies, and
2. If you load the package with `devtools::load_all()`, set the `prework` argument of `make()`: e.g. `make(prework = "devtools::load_all()")`.

Thanks to [Jasper Clarkberg](https://github.com/dapperjapper) for the workaround behind `expose_imports()`.

#### Advantages of putting workflows in R packages

- The file organization of R packages is a well-understood community standard. If you follow it, your work may be more readable and thus reproducible.
- R package installation is a standard process. The system makes it easier for others to obtain and run your code.
- You get development and quality control tools for free: [helpers for loading code and creating files](https://github.com/hadley/devtools), [unit testing](http://r-pkgs.had.co.nz/tests.html), [package checks](http://r-pkgs.had.co.nz/check.html), [code coverage](https://github.com/r-lib/covr), and [continuous integration](https://ipub.com/continuous-integration-for-r/).

#### The problem

For `drake`, there is one problem: nested functions. `Drake` always looks for imported functions nested in other imported functions, but only in your environment. When it sees a function from a package, it does not look in its body for other imports.

To see this, consider the `digest()` function from the [`digest` package](https://github.com/eddelbuettel/digest). [`Digest` package](https://github.com/eddelbuettel/digest) is a utility for computing hashes, not a data science workflow, but I will use it to demonstrate how `drake` treats imports from packages.


```{r nestingproblem}
library(digest)
g <- function(x){
  digest(x)
}
f <- function(x){
  g(x)
}
plan <- drake_plan(x = f(1))

## Here are the reproducibly tracked objects in the workflow.
config <- drake_config(plan)
tracked(config)

## But the `digest()` function has dependencies too.
## Because `drake` knows `digest()` is from a package,
## it ignores these dependencies by default.
head(deps_code(digest), 10)
```

#### The solution

To force `drake` to dive deeper into the nested functions in a package, you must use `expose_imports()`. Again, I demonstrate with the [`digest` package](https://github.com/eddelbuettel/digest) package, but you should really only do this with a package you write yourself to contain your workflow. For external packages, [packrat](https://rstudio.github.io/packrat/) is a much better solution for package reproducibility.

```{r nestingsolution}
expose_imports(digest)
config <- drake_config(plan)
new_objects <- tracked(config)
head(new_objects, 10)
length(new_objects)

## Now when you call `make()`, `drake` will dive into `digest`
## to import dependencies.

cache <- storr::storr_environment() # just for examples
make(plan, cache = cache)
head(cached(cache = cache), 10)
length(cached(cache = cache))
```

```{r rmfiles_caution, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
file.remove("report.Rmd")
unlink(
  c(
    "data.csv", "Makefile", "report.Rmd",
    "shell.sh", "STDIN.o*", "Thumbs.db"
  )
)
```

## Remote data sources

Some workflows rely on remote data from the internet, and the workflow needs to refresh when the datasets change. As an example, let us consider the download logs of [CRAN packages](https://cran.r-project.org/).

```{r logs1}
library(drake)
library(R.utils) # For unzipping the files we download.
library(curl)    # For downloading data.
library(httr)    # For querying websites.

url <- "http://cran-logs.rstudio.com/2018/2018-02-09-r.csv.gz"
```

How do we know when the data at the URL changed? We get the time that the file was last modified. (Alternatively, we could use an HTTP ETag.)

```{r logs2}
query <- HEAD(url)
timestamp <- query$headers[["last-modified"]]
timestamp
```

In our workflow plan, the timestamp is a target and a dependency. When the timestamp changes, so does everything downstream.

```{r logs3}
cranlogs_plan <- drake_plan(
  timestamp = HEAD(url)$headers[["last-modified"]],
  logs = get_logs(url, timestamp),
  strings_in_dots = "literals"
)
cranlogs_plan
```

To make sure we always have the latest timestamp, we use the `"always"` trigger. (For more on triggers, see the [guide to debugging and testing drake projects](#debug).)

```{r logs4}
cranlogs_plan$trigger <- c("always", "any")
cranlogs_plan
```

Lastly, we define the `get_logs()` function, which actually downloads the data.

```{r logs5}
## The ... is just so we can write dependencies as function arguments
## in the workflow plan.
get_logs <- function(url, ...){
  curl_download(url, "logs.csv.gz")       # Get a big file.
  gunzip("logs.csv.gz", overwrite = TRUE) # Unzip it.
  out <- read.csv("logs.csv", nrows = 4)  # Extract the data you need.
  unlink(c("logs.csv.gz", "logs.csv"))    # Remove the big files
  out                                     # Value of the target.
}
```

When we are ready, we run the workflow.

```{r logs6}
make(cranlogs_plan)

readd(logs)
```

```{r endofline_bestpractices, echo = F}
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c("Makefile", "report.Rmd", "shell.sh", "STDIN.o*", "Thumbs.db", "file.txt")
)
```
