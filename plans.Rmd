# Workflow plan data frames {#plans}

```{r loaddrake14, echo = FALSE}
unlink(
  c("main", "report.Rmd", "raw_data.xlsx"),
  recursive = TRUE
)
knitr::opts_chunk$set(collapse = TRUE)
suppressPackageStartupMessages(library(drake))
suppressPackageStartupMessages(library(glue))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(rlang))
suppressPackageStartupMessages(library(tidyverse))
pkgconfig::set_config("drake::strings_in_dots" = "literals")
invisible(drake_example("main", overwrite = TRUE))
invisible(file.copy("main/raw_data.xlsx", ".", overwrite = TRUE))
invisible(file.copy("main/report.Rmd", ".", overwrite = TRUE))
```

## What is a workflow plan data frame?

Your workflow plan data frame is the object where you declare all the objects and files you are going to produce when you run your project. It enumerates each output R object, or *target*, and the *command* that will produce it. Here is the workflow plan from our [previous example](#hpc).

```{r firstexampleplan}
plan <- drake_plan(
  raw_data = readxl::read_excel(file_in("raw_data.xlsx")),
  data = raw_data %>%
    mutate(Species = forcats::fct_inorder(Species)) %>%
    select(-X__1),
  hist = create_plot(data),
  fit = lm(Sepal.Width ~ Petal.Width + Species, data),
  report = rmarkdown::render(
    knitr_in("report.Rmd"),
    output_file = file_out("report.html"),
    quiet = TRUE
  )
)
plan
```

When you run `make(plan)`, `drake` will produce targets `raw_data`, `data`, `hist`, `fit`, and `report`. 

## Rationale

The workflow plan may seem like a burden to set up, and the use of data frames may seem counterintuitive at first, but the rewards are worth the effort.

### You can skip up-to-date work.

As we saw in our [previous example](#hpc), repeated `make()`s skip work that is already up to date. The reason `drake` can skip things is that you declared all the skippable steps as targets in the workflow plan. Thus, workflow plans are vital to the time savings `drake` brings to large projects.

This approach of declaring targets in advance has stood the test of time. The idea dates at least as far back as [GNU Make](https://www.gnu.org/software/make/), which uses  `Makefile`s to declare targets and dependencies. `drake`'s predecessor [`remake`](https://github.com/richfitz/remake) uses [`YAML`](http://yaml.org/) files in a similar way.

### Data frames scale well.

`Makefile`s are successful for [Make](https://www.gnu.org/software/make/) because they accommodate software written in multiple languages. However, such external configuration files are not the best solution for R. Maintaining a `Makefile` or a [`remake`](https://github.com/richfitz/remake) [`YAML`](http://yaml.org/) file requires a lot of manual typing. But with `drake` plans, you can use the usual data frame manipulation tools to expand, generate, and piece together large projets. The [gsp example](#gsp) shows how `expand.grid()` and `rbind()` to automatically create plans with hundreds of targets. In addition, `drake` has a wildcard templating mechanism to generate large plans.

### You do not need to worry about which targets run first.

When you call `make()` on the plan above, `drake` takes care of `"raw_data.xlsx"`, then `raw_data`, and then `data` in sequence. Once `data` completes, `fit` and `hist` can start in any order, and then `report` begins once everything else is done. The execution does not depend on the order of the rows in your plan. In other words, the following plan is equivalent.

```{r firstexampleplan2}
drake_plan(
  fit = lm(Sepal.Width ~ Petal.Width + Species, data),
  report = rmarkdown::render(
    knitr_in("report.Rmd"),
    output_file = file_out("report.html"),
    quiet = TRUE
  ),
  hist = create_plot(data),
  data = raw_data %>%
    mutate(Species = forcats::fct_inorder(Species)) %>%
    select(-X__1),
  raw_data = readxl::read_excel(file_in("raw_data.xlsx"))
)
```

## Automatic dependency detection

Why can you safely scramble the rows of a `drake` plan? Why is row order irrelevant to execution order? Because `drake` analyzes commands for dependencies, and `make()` processes those dependencies before moving on to downstream targets. To detect dependencies, `drake` walks through the [abstract syntax tree](http://adv-r.had.co.nz/Expressions.html#ast-funs) of every piece of code to find the objects and files relevant to the workflow pipeline.

```{r depscode_plans}
create_plot <- function(data) {
  ggplot(data, aes_string(x = "Petal.Width", fill = "Species")) +
    geom_histogram()
}

deps_code(create_plot)

deps_code(
  quote({
    some_function_i_wrote(data)
    rmarkdown::render(
      knitr_in("report.Rmd"),
      output_file = file_out("report.html"),
      quiet = TRUE
    )
  })
)
```

`drake` detects dependencies without actually running the command.

```{r depscode_plans2}
file.exists("report.html")
```

R objects and functions are detected implicitly, and you can specify multiple file inputs and outputs per command with the `file_in()`, `knitr_in()`, and `file_out()` functions. With `knitr_in()` R Markdown reports like `report.Rmd` ([online here](https://github.com/wlandau/drake-examples/blob/master/main/report.Rmd)), `drake` scans active code chunks for objects mentioned with `loadd()` and `readd()`. So when `fit` or `hist` change, `drake` rebuilds the `report` target to produce the file `report.html`.

Output files declared with `file_out()` do not appear in `vis_drake_graph()`, but targets can depend on one another through `file_in()`/`file_out()` connections.

```{r fileinfileout_plans}
saveRDS(1, "start.rds")

write_files <- function(){
  x <- readRDS(file_in("start.rds"))
  for (file in letters[1:3]){
    saveRDS(x, file)
  }
}

small_plan <- drake_plan(
  x = {
    write_files()
    file_out("a", "b", "c")
  },
  y = readRDS(file_in("a"))
)

config <- drake_config(small_plan)
vis_drake_graph(config)
```

So when target `x` changes the output for files `"a"`, `"b"`, or `"c"`, `drake` knows to rebuild target `y`.

And remember, `drake` also takes into account imported functions and imported files. If there are nontrivial changes to `start.rds`, `letters`, `readRDS()`, or `saveRDS()`, `drake` will rebuild targets `x` and/or `y` as appropriate.

## Automatically generating workflow plans

`drake` provides many more utilites that increase the flexibility of workflow plan generation beyond `expand.grid()`.

- `drake_plan()`
- `evaluate_plan()`
- `plan_analyses()`
- `plan_summaries()`
- `expand_plan()`
- `gather_plan()`
- `reduce_plan()`

### Wildcard templating

In `drake`, you can write plans with wildcards. These wilrdards are placeholders for text in commands. By iterating over the possible values of a wildcard, you can easily generate plans with thousands of targets. Let's say you are running a simulation study, and you need to generate sets of random numbers from different distributions.


```{r evaluteplan1}
plan <- drake_plan(
  t  = rt(1000, df = 5),
  normal = runif(1000, mean = 0, sd = 1)
)
```

If you need to generate many datasets with different means, you may wish to write out each target individually.

```{r evaluteplan2, eval = FALSE}
drake_plan(
  t  = rt(1000, df = 5),
  normal_0 = runif(1000, mean = 0, sd = 1),
  normal_1 = runif(1000, mean = 1, sd = 1),
  normal_2 = runif(1000, mean = 2, sd = 1),
  normal_3 = runif(1000, mean = 3, sd = 1),
  normal_4 = runif(1000, mean = 4, sd = 1),
  normal_5 = runif(1000, mean = 5, sd = 1),
  normal_6 = runif(1000, mean = 6, sd = 1),
  normal_7 = runif(1000, mean = 7, sd = 1),
  normal_8 = runif(1000, mean = 8, sd = 1),
  normal_9 = runif(1000, mean = 9, sd = 1)
)
```

But writing all that code manually is a pain and prone to human error. Instead, use `evaluate_plan()`

```{r evaluateplan3}
plan <- drake_plan(
  t  = rt(1000, df = 5),
  normal = runif(1000, mean = mean__, sd = 1)
)
evaluate_plan(plan, wildcard = "mean__", values = 0:9)
```

You can specify multiple wildcards at once. If multiple wildcards appear in the same command, you will get a new target for each unique combination of values.

```{r evaluateplan4}
plan <- drake_plan(
  t  = rt(1000, df = df__),
  normal = runif(1000, mean = mean__, sd = sd__)
)
evaluate_plan(
  plan,
  rules = list(
    mean__ = c(0, 1),
    sd__ = c(3, 4),
    df__ = 5:7
  )
)
```

Set `expand` to `FALSE` to disable expansion.

```{r noexpand}
plan <- drake_plan(
  t  = rpois(samples__, lambda = mean__),
  normal = runif(samples__, mean = mean__)
)
evaluate_plan(
  plan,
  rules = list(
    samples__ = c(50, 100),
    mean__ = c(1, 5)
  ),
  expand = FALSE
)
```

Wildcard templating can sometimes be tricky. For example, suppose your project is to analyze school data, and your workflow checks several metrics of several schools. The idea is to write a workflow plan with your metrics and let the wildcard templating expand over the available schools.

```{r schoolswildcards1}
hard_plan <- drake_plan(
  credits = check_credit_hours(school__),
  students = check_students(school__),
  grads = check_graduations(school__),
  public_funds = check_public_funding(school__)
)

evaluate_plan(
  hard_plan,
  rules = list(school__ = c("schoolA", "schoolB", "schoolC"))
)
```

But what if some metrics do not make sense? For example, what if `schoolC` is a completely privately-funded school? With no public funds, `check_public_funds(schoolC)` may quit in error if we are not careful. This is where setting up workflow plans requires a little creativity. In this case, we recommend that you use two wildcards: one for all the schools and another for just the public schools. The new plan has no twelfth row.

```{r schoolsplanfinal}
plan_template <- drake_plan(
  school = get_school_data("school__"),
  credits = check_credit_hours(all_schools__),
  students = check_students(all_schools__),
  grads = check_graduations(all_schools__),
  public_funds = check_public_funding(public_schools__)
)
evaluate_plan(
  plan = plan_template,
  rules = list(
    school__ = c("A", "B", "C"),
    all_schools__ =  c("school_A", "school_B", "school_C"),
    public_schools__ = c("school_A", "school_B")
  )
)
```

Thanks to [Alex Axthelm](https://github.com/AlexAxthelm) for this use case in [issue 235](https://github.com/ropensci/drake/issues/235).


### Wildcard clusters

With `evaluate_plan(trace = TRUE)`, you can generate columns that show how the targets were generated from the wildcards.

```{r trace1}
plan_template <- drake_plan(
  school = get_school_data("school__"),
  credits = check_credit_hours(all_schools__),
  students = check_students(all_schools__),
  grads = check_graduations(all_schools__),
  public_funds = check_public_funding(public_schools__)
)
plan <- evaluate_plan(
  plan = plan_template,
  rules = list(
    school__ = c("A", "B", "C"),
    all_schools__ =  c("school_A", "school_B", "school_C"),
    public_schools__ = c("school_A", "school_B")
  ),
  trace = TRUE
)
plan
```

And then when you visualize the dependency graph, you can cluster nodes based on the wildcard info.

```{r tracevis1hide, echo = FALSE}
check_credit_hours <- check_students <- check_graduations <-
  check_public_funding <- get_school_data <- function(){}
```

```{r tracevisplans1}
config <- drake_config(plan)
vis_drake_graph(
  config,
  group = "all_schools__",
  clusters = c("school_A", "school_B", "school_C")
)
```

See the [visualization guide](#vis) for more details.

### Specialized wildcard functionality

In the [`mtcars` example](#mtcars), we will analyze bootstrapped versions of the `mtcars` dataset to look for an association between the weight and the fuel efficiency of cars. This example uses `plan_analyses()` and `plan_summaries()`, two specialized applications of `evaluate_plan()`. First, we generate the plan for the bootstrapped datasets.

```{r datasets2}
my_datasets <- drake_plan(
  small = simulate(48),
  large = simulate(64))
my_datasets
```

We want to analyze each dataset with one of two regression models.

```{r methods2}
methods <- drake_plan(
  regression1 = reg1(dataset__),
  regression2 = reg2(dataset__))
methods
```

We evaluate the `dataset__` wildcard to generate all the regression commands we will need.

```{r analyses2}
my_analyses <- plan_analyses(methods, datasets = my_datasets)
my_analyses
```

Next, we summarize each analysis of each dataset. We calculate descriptive statistics on the residuals, and we collect the regression coefficients and their p-values.

```{r summaries2}
summary_types <- drake_plan(
  summ = suppressWarnings(summary(analysis__$residuals)),
  coef = suppressWarnings(summary(analysis__))$coefficients
)
summary_types

results <- plan_summaries(summary_types, analyses = my_analyses,
  datasets = my_datasets, gather = NULL) # Gathering is suppressed here.
results
```

Next, we bind all the rows together for a single plan that we can later supply to `make()`.

```{r wholeplan2}
my_plan <- rbind(my_datasets, my_analyses, results)
my_plan
```

### Non-wildcard functions

#### `expand_plan()`

Sometimes, you just want multiple replicates of the same targets.

```{r expandplan}
plan <- drake_plan(
  fake_data = simulate_from_model(),
  bootstrapped_data = bootstrap_from_real_data(real_data)
)
expand_plan(plan, values = 1:3)
```


#### `gather_plan()`

Other times, you want to combine multiple targets into one. 

```{r gather1}
plan <- drake_plan(
  small = data.frame(type = "small", x = rnorm(25), y = rnorm(25)),
  large = data.frame(type = "large", x = rnorm(1000), y = rnorm(1000))
)
gather_plan(plan, target = "combined")
```

In this case, `small` and `large` are data frames, so it may be more convenient to combine the rows together.

```{r gather2}
gather_plan(plan, target = "combined", gather = "rbind")
```

#### `reduce_plan()`

`reduce_plan()` is similar to `gather_plan()`, but it allows you to combine multiple targets together in pairs. This is useful if combining everything at once requires too much time or computer memory, or if you want to parallelize the aggregation.

```{r reduceplan}
plan <- drake_plan(
  a = 1,
  b = 2,
  c = 3,
  d = 4
)
reduce_plan(plan)
```

You can control how each pair of targets gets combined.

```{r reduceplan2}
reduce_plan(plan, begin = "c(", op = ", ", end = ")")
```

### Custom metaprogramming

The workflow plan is just a data frame. There is nothing magic about it, and you can create it any way you want. With your own custom metaprogramming, you don't even need the `drake_plan()` function.

Let's consider a file-based example workflow. Here, our targets execute Linux commands to process input files and create output files.

<pre><code>cat in1.txt > out1.txt
cat in2.txt > out2.txt
</code></pre>

The [`glue`](https://github.com/tidyverse/glue) package can automatically generate these Linux commands.

```{r systemcmdglue}
library(glue)
glue_data(
  list(
    inputs = c("in1.txt", "in2.txt"), 
    outputs = c("out1.txt", "out2.txt")
  ),
  "cat {inputs} > {outputs}"
)
```

Our `drake` commands will use `system()` to execute the Linux commands that [`glue`](https://github.com/tidyverse/glue) generates. Technically, we could use `drake_plan()` if we wanted.

```{r hypotheticaldrakeplan}
library(tidyverse)
drake_plan(
  glue_data(
    list(
      inputs = file_in(c("in1.txt", "in2.txt")), 
      outputs = file_out(c("out1.txt", "out2.txt"))
    ),
    "cat {inputs} > {outputs}"
  ) %>%
    lapply(FUN = system)
)
```

But what if we want to *generate* these [`glue`](https://github.com/tidyverse/glue) commands instead of writing them literally in our plan? This is a job for custom metaprogramming with [tidy evaluation](https://www.youtube.com/watch?v=nERXS3ssntw). First, we create a function to generate the `drake` command of an arbitrary target.

```{r tidyevalplan1}
library(rlang) # for tidy evaluation
write_command <- function(cmd, inputs = NULL , outputs = NULL){
  inputs <- enexpr(inputs)
  outputs <- enexpr(outputs)
  expr({
    glue_data(
      list(
        inputs = file_in(!!inputs),
        outputs = file_out(!!outputs)
      ),
      !!cmd
    ) %>%
      lapply(FUN = system)
  }) %>%
    expr_text
}

write_command(
  cmd = "cat {inputs} > {outputs}",
  inputs = c("in1.txt", "in2.txt"),
  outputs = c("out1.txt", "out2.txt")
) %>%
  cat
```

Then, we lay out all the arguments we will pass to `write_command()`. Here, each row corresponds to a separate target.

```{r tidyevalplan2}
meta_plan <- tribble(
  ~cmd, ~inputs, ~outputs,
  "cat {inputs} > {outputs}", c("in1.txt", "in2.txt"), c("out1.txt", "out2.txt"),
  "cat {inputs} {inputs} > {outputs}", c("out1.txt", "out2.txt"), c("out3.txt", "out4.txt")
) %>%
  print
```

Finally, we create our workflow plan without any built-in `drake` functions.

```{r tidyevalplan3}
plan <- tibble(
  target = paste0("target_", seq_len(nrow(meta_plan))),
  command = pmap_chr(meta_plan, write_command)
) %>%
  print
writeLines("in1", "in1.txt")
writeLines("in2", "in2.txt")
vis_drake_graph(drake_config(plan))
```

Alternatively, you could use `as.call()` instead of tidy evaluation to generate your plan. Use `as.call()` to construct calls to `file_in()`, `file_out()`, and custom functions in your commands.

```{r alternativenontidyplan4}
library(purrr) # pmap_chr() is particularly useful here.

# A function that will be called in your commands.
command_function <- function(cmd, inputs, outputs){
  glue_data(
    list(
      inputs = inputs,
      outputs = outputs
    ),
    cmd
  ) %>%
    purrr::walk(system)
}

# A function to generate quoted calls to command_function(),
# which in turn contain quoted calls to file_in() and file_out().
write_command <- function(...){
  args <- list(...)
  args$inputs <- as.call(list(quote(file_in), args$inputs))
  args$outputs <- as.call(list(quote(file_out), args$outputs))
  c(quote(command_function), args) %>%
    as.call() %>%
    rlang::expr_text()
}

plan <- tibble(
  target = paste0("target_", seq_len(nrow(meta_plan))),
  command = pmap_chr(meta_plan, write_command)
) %>%
  print
```

Metaprogramming gets much simpler if you do not need to construct literal calls to `file_in()`, `file_out()`, etc. in your commands. The construction of `model_plan` in the [gross state product exmaple](#gsp) is an example.

Thanks to [Chris Hammill](https://github.com/cfhammill) for [presenting this scenario and contributing to the solution](https://github.com/ropensci/drake/issues/451).

### Turn scripts and reports into plans

With the `code_to_plan()` function, you can turn your R scripts and `knitr` / R Markdown reports into `drake` workflow plan data frames. In your script or `knitr` / R Markdown code chunks, you can assign expressions to variables, and `code_to_plan()` will turn them into commands and targets, respectively. The `make.R` script demonstrates how this works for the script `script.R` and the report `report.Rmd`.

This feature is easy to break, so there are some rules for your source files:

1. Stick to ssigning a single expression to a single target at a time. For multi-line commands, please enclose the whole command in curly braces. Conversely, compound assignment is not supported (e.g. `target_1 <- target_2 <- target_3 <- get_data()`).
2. Once you assign an expression to a variable, do not modify the variable any more. The target/command [binding](https://cs.stackexchange.com/questions/39525/what-is-the-difference-between-assignment-valuation-and-name-binding) should be permanent.
3. Keep it simple. Please use the assignment operators rather than `assign()` and similar functions.

You can [find example code and extra documentation here](https://github.com/wlandau/drake-examples/tree/master/code_to_plan), and you can download the files with `drake_exmaple()`.

```{r code_to_plan}
library(drake)
drake_example("code_to_plan")
plan <- code_to_plan("code_to_plan/script.R")
plan2 <- code_to_plan("code_to_plan/report.Rmd")
identical(plan, plan2) # should be TRUE
print(plan)
make(plan)
readd(discrepancy)
```

## Optional columns in your plan.

Besides the usual columns `target` and `command`, there are other columns you can add.

- `cpu`, `elapsed`, and `timeout`: number of seconds to wait for the target to build before timing out (`timeout` for a general upper bound, `cpu` for CPU time, and `elapsed` for elapsed time).
- `priority`: for [paralllel computing](#hpc), optionally rank the targets according to priority. That way, when two targets become ready to build at the same time, `drake` will pick the one with the dominant priority first.
- `retries`: number of times to retry building a target in the event of an error.
- `trigger`: choose the criterion that `drake` uses to decide whether to build the target. See `?trigger` or read the [trigger chapter](#triggers) to learn more.
- `worker`: for [paralllel computing](#hpc), optionally name the preferred worker to assign to each target. 


```{r endofline_plans, echo = FALSE}
clean(destroy = TRUE, verbose = FALSE)
unlink(
  c("main", "code_to_plan", "start.rds", "report.Rmd", "raw_data.xlsx",
    "STDIN.o*", "Thumbs.db", "in1.txt", "in2.txt"),
  recursive = TRUE
)
```
